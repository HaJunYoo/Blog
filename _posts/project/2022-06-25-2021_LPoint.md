---
title: 2021 Lpoint 클릭스트림
description: 2021 국민대학교 빅데이터 경영학과 학회 D&A에서 주최한 Lpoint 클릭스트림 데이터를 이용한 성별 예측 대회
toc: true 
layout: post
badges: true
comments: false
categories: [datascience, project]
image:
---

# 2021 Lpoint 클릭스트림

## 개요
- 2021 국민대학교 빅데이터 경영학과 학회 D&A에서 주최한 Lpoint 클릭스트림 데이터를 이용한 성별 예측 대회 제출본입니다.


```python
%matplotlib inline
import random
import scipy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from datetime import date
```


```python
df = pd.read_csv('./L.POINT_train.csv')
df_test = pd.read_csv('./L.POINT_test.csv')
```

    /opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.
      exec(code_obj, self.user_global_ns, self.user_ns)



```python
y = pd.read_csv('./y_train.csv')
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CLNT_ID</th>
      <th>SESS_ID</th>
      <th>HITS_SEQ</th>
      <th>PD_C</th>
      <th>PD_ADD_NM</th>
      <th>PD_BRA_NM</th>
      <th>PD_BUY_AM</th>
      <th>PD_BUY_CT</th>
      <th>SESS_SEQ</th>
      <th>SESS_DT</th>
      <th>...</th>
      <th>TOT_SESS_HR_V</th>
      <th>DVC_CTG_NM</th>
      <th>ZON_NM</th>
      <th>CITY_NM</th>
      <th>KWD_NM</th>
      <th>SEARCH_CNT</th>
      <th>PD_NM</th>
      <th>CLAC1_NM</th>
      <th>CLAC2_NM</th>
      <th>CLAC3_NM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>6771240</td>
      <td>63</td>
      <td>578845</td>
      <td>1개</td>
      <td>필립스(PHILIPS)</td>
      <td>81,000</td>
      <td>1</td>
      <td>17</td>
      <td>20180609</td>
      <td>...</td>
      <td>922</td>
      <td>mobile</td>
      <td>Gyeonggi-do</td>
      <td>Bucheon-si</td>
      <td>에어컨 커버</td>
      <td>1</td>
      <td>아방세 프로믹스 핸드블렌더 HR1672/90</td>
      <td>생활/주방가전</td>
      <td>주방가전</td>
      <td>블랜더</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>6771240</td>
      <td>63</td>
      <td>788068</td>
      <td>선택:버닝 [베이지] / 1개</td>
      <td>쁘리엘르</td>
      <td>5,500</td>
      <td>1</td>
      <td>17</td>
      <td>20180609</td>
      <td>...</td>
      <td>922</td>
      <td>mobile</td>
      <td>Gyeonggi-do</td>
      <td>Bucheon-si</td>
      <td>에어컨 커버</td>
      <td>1</td>
      <td>스판 벽걸이 에어컨커버(트라이앵글_82x27x26) 모음 - 블루가든 [블루]</td>
      <td>침구/수예</td>
      <td>수예소품</td>
      <td>거실수예소품</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>5762174</td>
      <td>109</td>
      <td>180447</td>
      <td>사이즈:L(105) / 1개</td>
      <td>퀵실버</td>
      <td>59,000</td>
      <td>1</td>
      <td>12</td>
      <td>20180626</td>
      <td>...</td>
      <td>1,661</td>
      <td>mobile</td>
      <td>Seoul</td>
      <td>Seoul</td>
      <td>바비브라운</td>
      <td>1</td>
      <td>퀵실버 남성 루즈핏 래쉬가드 QS579KMT - M(100)</td>
      <td>시즌스포츠</td>
      <td>수영/물놀이</td>
      <td>남성수영복</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>5753875</td>
      <td>94</td>
      <td>731145</td>
      <td>1개</td>
      <td>키엘</td>
      <td>39,000</td>
      <td>1</td>
      <td>13</td>
      <td>20180626</td>
      <td>...</td>
      <td>620</td>
      <td>mobile</td>
      <td>Seoul</td>
      <td>Seoul</td>
      <td>키엘</td>
      <td>2</td>
      <td>칼렌듈라 딥 클렌징 포밍 페이스 워시 230ml</td>
      <td>화장품/뷰티케어</td>
      <td>스킨케어</td>
      <td>페이셜클렌저</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>7417570</td>
      <td>114</td>
      <td>216947</td>
      <td>1개</td>
      <td>키엘</td>
      <td>49,000</td>
      <td>1</td>
      <td>2</td>
      <td>20180529</td>
      <td>...</td>
      <td>860</td>
      <td>mobile</td>
      <td>Seoul</td>
      <td>Seoul</td>
      <td>키엘비타민</td>
      <td>2</td>
      <td>키엘 자외선 차단제 점보 세트</td>
      <td>화장품/뷰티케어</td>
      <td>선케어</td>
      <td>선크림류</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>




```python
len(df['KWD_NM'].unique())
```




    72089




```python
len(df['KWD_NM'].unique())
```




    72089




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1948686 entries, 0 to 1948685
    Data columns (total 21 columns):
     #   Column           Dtype  
    ---  ------           -----  
     0   CLNT_ID          int64  
     1   SESS_ID          int64  
     2   HITS_SEQ         int64  
     3   PD_C             int64  
     4   PD_ADD_NM        object 
     5   PD_BRA_NM        object 
     6   PD_BUY_AM        object 
     7   PD_BUY_CT        object 
     8   SESS_SEQ         int64  
     9   SESS_DT          int64  
     10  TOT_PAG_VIEW_CT  float64
     11  TOT_SESS_HR_V    object 
     12  DVC_CTG_NM       object 
     13  ZON_NM           object 
     14  CITY_NM          object 
     15  KWD_NM           object 
     16  SEARCH_CNT       int64  
     17  PD_NM            object 
     18  CLAC1_NM         object 
     19  CLAC2_NM         object 
     20  CLAC3_NM         object 
    dtypes: float64(1), int64(7), object(13)
    memory usage: 312.2+ MB



```python
y
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CLNT_ID</th>
      <th>LABEL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>F20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>F20</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>263094</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>263095</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>263096</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>263102</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>263103</td>
      <td>F30</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 2 columns</p>
</div>




```python
y['LABEL'].unique()
```




    array(['F20', 'F30', 'F40', 'M30', 'M40', 'M20'], dtype=object)




```python
y['LABEL'].value_counts()
```




    F30    59892
    F40    51936
    F20    17727
    M40     9904
    M30     7953
    M20     2588
    Name: LABEL, dtype: int64




```python
weight = np.array([59892/17727, 59892/59892, 59892/51936, 59892/2588, 59892/7953, 59892/9904])
weight
```




    array([ 3.37857506,  1.        ,  1.15318854, 23.14219474,  7.53074312,
            6.04725363])




```python
df = pd.merge(df, y, how='left', on='CLNT_ID')
```


```python
len(df['CLNT_ID'].unique())
```




    150000



1948686행, 150000 유저 -> 유저당 평균 13회 로그 정보


```python
len(df['PD_C'].unique())
```




    280867



280867개 고유 상품


```python
df.groupby(by='LABEL')['DVC_CTG_NM'].value_counts()
```




    LABEL  DVC_CTG_NM
    F20    mobile        150178
           tablet           519
           desktop           18
    F30    mobile        794940
           tablet          2417
           desktop           13
    F40    mobile        772145
           tablet          5190
           desktop           29
    M20    mobile         19259
           tablet           150
           desktop            9
    M30    mobile         78795
           tablet           109
           desktop           10
    M40    mobile        123945
           tablet           896
           desktop           64
    Name: DVC_CTG_NM, dtype: int64




```python
df['ZON_NM'].unique()
```




    array(['Gyeonggi-do', 'Seoul', 'Jeollabuk-do', 'Gwangju', 'Busan',
           'Incheon', 'Gyeongsangnam-do', 'Gyeongsangbuk-do',
           'Chungcheongnam-do', 'Jeollanam-do', 'Daegu', 'Gangwon-do',
           'Ulsan', 'Daejeon', 'Chungcheongbuk-do', 'Jeju-do'], dtype=object)




```python
df['CITY_NM'].unique()
```




    array(['Bucheon-si', 'Seoul', 'Wanju-gun', 'Gwangju', 'Namyangju-si',
           'Guri-si', 'Busan', 'Imsil-gun', 'Gimpo-si', 'Incheon',
           'Gimhae-si', 'Paju-si', 'Sangju-si', 'Yeongi-gun', 'Hwaseong-si',
           'Gongju-si', 'Seongnam-si', 'Andong', 'Jeongeup-si', 'Geoje-si',
           'Gangjin-gun', 'Gumi-si', 'Yeoju-gun', 'Pyeongtaek-si', 'Daegu',
           'Yongin-si', 'Yangsan-si', 'Chuncheon-si', 'Suwon-si', 'Ulsan',
           'Tongyeong-si', 'Haman-gun', 'Goyang-si', 'Daejeon', 'Cheonan-si',
           'Yeosu-si', 'Jeonju-si', 'Taean-gun', 'Chungju-si', 'Uijeongbu-si',
           'Gangneung-si', 'Gyeongju-si', 'Hanam-si', 'Sunchang-gun',
           'Mungyeong-si', 'Gunsan-si', 'Wonju-si', 'Uiwang-si', 'Anseong',
           'Anyang', 'Ansan-si', 'Gunpo-si', 'Sacheon-si', 'Cheongju-si',
           'Miryang-si', 'Gimcheon-si', 'Sokcho-si', 'Yeongju-si', 'Jeju-si',
           'Siheung-si', 'Pohang-si', 'Seosan-si', 'Hapcheon-gun',
           'Yangju-si', 'Taebaek-si', 'Muan-gun', 'Goesan-gun', 'Jindo-gun',
           'Gwangju-si', 'Yangpyeong-gun', 'Yeongyang-gun', 'Hongseong-gun',
           'Goryeong-gun', 'Naju-si', 'Icheon-si', 'Yeongdong-gun',
           'Gwangmyeong-si', 'Asan-si', 'Gyeongsan-si', 'Gochang-gun',
           'Hoengseong-gun', 'Yeongdeok-gun', 'Uljin-gun', 'Seongju-gun',
           'Suncheon-si', 'Cheorwon-gun', 'Boseong-gun', 'Yesan-gun', 'Iksan',
           'Geumsan-gun', 'Jincheon-gun', 'Dangjin-si', 'Jeungpyeong-gun',
           'Jinju-si', 'Gapyeong-gun', 'Mokpo-si', 'Seogwipo-si', 'Namwon-si',
           'Jangheung-gun', 'Geochang-gun', 'Gwangyang-si', 'Goseong-gun',
           'Chilgok-gun', 'Donghae-si', 'Yeongcheon-si', 'Cheongwon-gun',
           'Yangyang-gun', 'Osan-si', 'Haenam-gun', 'Gwacheon-si',
           'Yeonggwang-gun', 'Dongducheon-si', 'Yeongam-gun', 'Eumseong-gun',
           'Gokseong-gun', 'Cheongyang-gun', 'Yeongwol-gun', 'Nonsan-si',
           'Hongcheon-gun', 'Jecheon-si', 'Danyang-gun', 'Pyeongchang-gun',
           'Hamyang-gun', 'Hwasun-gun', 'Buyeo-gun', 'Samcheok-si',
           'Okcheon-gun', 'Cheongsong-gun', 'Yecheon-gun', 'Buan-gun',
           'Boeun-gun', 'Gyeryong-si', 'Yanggu-gun', 'Gurye-gun', 'Wando-gun',
           'Sancheong-gun', 'Namhae-gun', 'Hampyeong-gun', 'Hwacheon-gun',
           'Gimje-si', 'Cheongdo-gun', 'Jeongseon-gun', 'Pocheon-si',
           'Changnyeong-gun', 'Goheung-gun', 'Boryeong-si', 'Jinan-gun',
           'Uiseong-gun', 'Damyang-gun', 'Seocheon-gun', 'Uiryeong-gun',
           'Yeoncheon-gun', 'Jangsu-gun', 'Jangseong-gun', 'Sinan-gun',
           'Inje-gun', 'Gunwi-gun', 'Ulleung-gun', 'Bonghwa-gun',
           'Hadong-gun', 'Muju-gun', '(not set)'], dtype=object)




```python
df['PD_NM']
```




    0                             아방세 프로믹스 핸드블렌더 HR1672/90
    1          스판 벽걸이 에어컨커버(트라이앵글_82x27x26) 모음 - 블루가든 [블루]
    2                    퀵실버 남성 루즈핏 래쉬가드 QS579KMT - M(100)
    3                           칼렌듈라 딥 클렌징 포밍 페이스 워시 230ml
    4                                     키엘 자외선 차단제 점보 세트
                                  ...                     
    1948681         돌핀 배색 올인원 수영복[71SW77831] - 핑크(P) / 110
    1948682         돌핀 배색 올인원 수영복[71SW77831] - 핑크(P) / 110
    1948683         돌핀 배색 올인원 수영복[71SW77831] - 핑크(P) / 110
    1948684         돌핀 배색 올인원 수영복[71SW77831] - 핑크(P) / 110
    1948685        여아 돌고래 배색 래쉬가드[71SW50831] - 핑크(P) / 105
    Name: PD_NM, Length: 1948686, dtype: object




```python
len(df['PD_NM'].unique())
```




    276002



num(PD_C) != num(PD_NM)


```python
len(df['CLAC1_NM'].unique()), len(df['CLAC2_NM'].unique()), len(df['CLAC3_NM'].unique())
```




    (37, 128, 883)



대분류 37개, 중분류 128개, 소분류 883개


```python
df['KWD_NM']
```




    0           에어컨 커버
    1           에어컨 커버
    2            바비브라운
    3               키엘
    4            키엘비타민
                ...   
    1948681       키즈샌들
    1948682        갭키즈
    1948683      팁토이조이
    1948684    갭키즈 수영복
    1948685     갭키즈 여아
    Name: KWD_NM, Length: 1948686, dtype: object




```python
df['KWD_NM'].isna().sum()
```




    0




```python
df['SEARCH_CNT']
```




    0          1
    1          1
    2          1
    3          2
    4          2
              ..
    1948681    1
    1948682    1
    1948683    1
    1948684    1
    1948685    1
    Name: SEARCH_CNT, Length: 1948686, dtype: int64




```python
clnt_id = y['CLNT_ID'].values
```


```python
clnt_id
```




    array([     0,      1,      6, ..., 263096, 263102, 263103], dtype=int64)




```python
df['PD_BUY_CT'] = df['PD_BUY_CT'].astype('string')
df['TOT_SESS_HR_V'] = df['TOT_SESS_HR_V'].astype('string')

df['PD_BUY_AM'] = df['PD_BUY_AM'].map(lambda x: x.replace(',', ''))
df['PD_BUY_CT'] = df['PD_BUY_CT'].map(lambda x: x.replace(',', ''))
df['TOT_SESS_HR_V'] = df['TOT_SESS_HR_V'].map(lambda x: x.replace(',', ''))
```


```python
df['PD_BUY_AM'] = df['PD_BUY_AM'].astype('int')
df['PD_BUY_CT'] = df['PD_BUY_CT'].astype('int')
df['TOT_SESS_HR_V'] = df['TOT_SESS_HR_V'].astype('int')
```


```python
df['SESS_DT'] = df['SESS_DT'].map(lambda x: date.fromisoformat(str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))
```


```python
temp_df = df[df['CLNT_ID'] == clnt_id[4]]
temp_df = temp_df.sort_values(by=['SESS_DT', 'HITS_SEQ'])
temp_df = temp_df[~temp_df.duplicated(subset=['SESS_ID', 'HITS_SEQ'], keep='last')]
temp_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CLNT_ID</th>
      <th>SESS_ID</th>
      <th>HITS_SEQ</th>
      <th>PD_C</th>
      <th>PD_ADD_NM</th>
      <th>PD_BRA_NM</th>
      <th>PD_BUY_AM</th>
      <th>PD_BUY_CT</th>
      <th>SESS_SEQ</th>
      <th>SESS_DT</th>
      <th>...</th>
      <th>DVC_CTG_NM</th>
      <th>ZON_NM</th>
      <th>CITY_NM</th>
      <th>KWD_NM</th>
      <th>SEARCH_CNT</th>
      <th>PD_NM</th>
      <th>CLAC1_NM</th>
      <th>CLAC2_NM</th>
      <th>CLAC3_NM</th>
      <th>LABEL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>48</th>
      <td>12</td>
      <td>4738380</td>
      <td>47</td>
      <td>178471</td>
      <td>사이즈:18M / 1개</td>
      <td>갭 키즈</td>
      <td>10000</td>
      <td>1</td>
      <td>175</td>
      <td>2018-07-12</td>
      <td>...</td>
      <td>mobile</td>
      <td>Seoul</td>
      <td>Seoul</td>
      <td>베베드피노 오버롤</td>
      <td>1</td>
      <td>베이비 여아 화이트 블루머 5238234509001 - 24M</td>
      <td>속옷/양말/홈웨어</td>
      <td>유아동양말류</td>
      <td>유아동타이즈</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>44</th>
      <td>12</td>
      <td>4551726</td>
      <td>32</td>
      <td>233182</td>
      <td>색상:BLUE|사이즈:85 / 1개</td>
      <td>베베드피노</td>
      <td>39000</td>
      <td>1</td>
      <td>189</td>
      <td>2018-07-16</td>
      <td>...</td>
      <td>mobile</td>
      <td>Gyeonggi-do</td>
      <td>Namyangju-si</td>
      <td>뚜아후아 귀걸이</td>
      <td>4</td>
      <td>베이비 스트라이프 러플 바디수트BP8216222 - BLUE / 85</td>
      <td>유아동의류</td>
      <td>유아의류전신</td>
      <td>영유아점프수트/오버롤</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>57</th>
      <td>12</td>
      <td>4130706</td>
      <td>104</td>
      <td>141349</td>
      <td>모델명:01&gt;MS4388아이보리|사이즈:39 / 1개</td>
      <td>클립</td>
      <td>79000</td>
      <td>1</td>
      <td>219</td>
      <td>2018-07-22</td>
      <td>...</td>
      <td>mobile</td>
      <td>Gyeonggi-do</td>
      <td>Guri-si</td>
      <td>코치</td>
      <td>1</td>
      <td>베스트샌들 MS4388 Malou_Color line Webbing 아이보리 외 1...</td>
      <td>패션잡화</td>
      <td>여성화</td>
      <td>여성샌들</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>55</th>
      <td>12</td>
      <td>1776250</td>
      <td>36</td>
      <td>72185</td>
      <td>색상:코랄|사이즈:S(6~12m) / 1개</td>
      <td>해피프린스</td>
      <td>4560</td>
      <td>1</td>
      <td>313</td>
      <td>2018-09-01</td>
      <td>...</td>
      <td>mobile</td>
      <td>Gyeonggi-do</td>
      <td>Guri-si</td>
      <td>해피프린스</td>
      <td>4</td>
      <td>나리앙 니삭스 - 코랄 / M(12~24m)</td>
      <td>속옷/양말/홈웨어</td>
      <td>유아동양말류</td>
      <td>유아동일반양말</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>60</th>
      <td>12</td>
      <td>1011134</td>
      <td>50</td>
      <td>536257</td>
      <td>색상:베이지|사이즈:S / 1개</td>
      <td>해피프린스</td>
      <td>990</td>
      <td>1</td>
      <td>345</td>
      <td>2018-09-13</td>
      <td>...</td>
      <td>mobile</td>
      <td>Gyeonggi-do</td>
      <td>Namyangju-si</td>
      <td>해피프린스 양말</td>
      <td>1</td>
      <td>폴리지 삭스 - 블루 / S</td>
      <td>속옷/양말/홈웨어</td>
      <td>유아동양말류</td>
      <td>유아동일반양말</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>59</th>
      <td>12</td>
      <td>1011134</td>
      <td>58</td>
      <td>102618</td>
      <td>1개</td>
      <td>테팔</td>
      <td>15900</td>
      <td>1</td>
      <td>345</td>
      <td>2018-09-13</td>
      <td>...</td>
      <td>mobile</td>
      <td>Gyeonggi-do</td>
      <td>Namyangju-si</td>
      <td>해피프린스 양말</td>
      <td>1</td>
      <td>그래픽 패스포트 아이 러브 뉴욕 프라이팬 26cm</td>
      <td>식기/조리기구</td>
      <td>조리기구</td>
      <td>프라이팬</td>
      <td>F30</td>
    </tr>
  </tbody>
</table>
<p>6 rows × 22 columns</p>
</div>




```python
def calc_avg_shopping_interval(df):
    mean_shopping_interval = 0
    for i in range(len(df)-1):
        mean_shopping_interval += (df['SESS_DT'].iloc[i+1] - df['SESS_DT'].iloc[i]).days
    if len(df)-1 == 0:
        mean_shopping_interval = 183
    else:
        mean_shopping_interval = mean_shopping_interval / (len(df)-1)
    return mean_shopping_interval
```


```python
def calc_avg_total_price_ct(df):
    price = 0
    cnt = 0
    for i in range(len(df)):
        price += df['PD_BUY_AM'].iloc[i] * df['PD_BUY_CT'].iloc[i]
        cnt += df['PD_BUY_CT'].iloc[i]
    return price/cnt, price, cnt/len(df), cnt
```

#### 피처 생성 루프 코드


```python
num_shoppings = []
avg_prices = []
total_prices = []
avg_cts = []
total_cts = []
avg_sess_views = []
total_sess_views = []
avg_sess_hrs = []
total_sess_hrs = []
avg_shopping_intervals = []
main_devices = []
pd_cs = []
clac1_nms = []
clac2_nms = []
clac3_nms = []

for i in tqdm(range(len(clnt_id))):
    temp_df = df[df['CLNT_ID'] == clnt_id[i]]
    temp_df = temp_df.sort_values(by=['SESS_DT', 'HITS_SEQ', 'PD_C'])
    temp_df = temp_df[~temp_df.duplicated(subset=['SESS_ID', 'HITS_SEQ', 'PD_C'], keep='last')]
    
    num_shopping = len(temp_df)
    avg_price, total_price, avg_ct, total_ct = calc_avg_total_price_ct(temp_df)
    avg_sess_view = temp_df['TOT_PAG_VIEW_CT'].values.mean()
    total_sess_view = temp_df['TOT_PAG_VIEW_CT'].values.sum()
    avg_sess_hr = temp_df['TOT_SESS_HR_V'].values.mean()
    total_sess_hr = temp_df['TOT_SESS_HR_V'].values.sum()
    avg_shopping_interval = calc_avg_shopping_interval(temp_df)
    main_device = scipy.stats.mode(temp_df['DVC_CTG_NM'].values).mode[0]
    pd_c = temp_df['PD_C'].values
    clac1_nm = temp_df['CLAC1_NM'].values
    clac2_nm = temp_df['CLAC2_NM'].values
    clac3_nm = temp_df['CLAC3_NM'].values
    
    num_shoppings.append(num_shopping)
    avg_prices.append(avg_price)
    total_prices.append(total_price)
    avg_cts.append(avg_ct)
    total_cts.append(total_ct)
    avg_sess_views.append(avg_sess_view)
    total_sess_views.append(total_sess_view)
    avg_sess_hrs.append(avg_sess_hr)
    total_sess_hrs.append(total_sess_hr)
    avg_shopping_intervals.append(avg_shopping_interval)
    main_devices.append(main_device)
    pd_cs.append(pd_c)
    clac1_nms.append(clac1_nm)
    clac2_nms.append(clac2_nm)
    clac3_nms.append(clac3_nm)
```

    100%|█████████████████████████████████████████████████████████████████████████| 150000/150000 [15:52<00:00, 157.46it/s]



```python
data = pd.DataFrame([clnt_id, num_shoppings, avg_prices, total_prices, avg_cts, total_cts, avg_sess_views, 
                     total_sess_views, avg_sess_hrs, total_sess_hrs, avg_shopping_intervals, main_devices, 
                     pd_cs, clac1_nms, clac2_nms, clac3_nms, y['LABEL']]).T
```


```python
data.columns = ['clnt_id', 'num_shopping', 'avg_price', 'total_price', 'avg_ct', 'total_ct', 'avg_sess_view', 
                'total_sess_view', 'avg_sess_hr', 'total_sess_hr', 'avg_shopping_interval', 'main_device', 
                'pd_c', 'clac1_nm', 'clac2_nm', 'clac3_nm', 'label']
```


```python
data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clnt_id</th>
      <th>num_shopping</th>
      <th>avg_price</th>
      <th>total_price</th>
      <th>avg_ct</th>
      <th>total_ct</th>
      <th>avg_sess_view</th>
      <th>total_sess_view</th>
      <th>avg_sess_hr</th>
      <th>total_sess_hr</th>
      <th>avg_shopping_interval</th>
      <th>main_device</th>
      <th>pd_c</th>
      <th>clac1_nm</th>
      <th>clac2_nm</th>
      <th>clac3_nm</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>2</td>
      <td>43250</td>
      <td>86500</td>
      <td>1</td>
      <td>2</td>
      <td>59</td>
      <td>118</td>
      <td>922</td>
      <td>1844</td>
      <td>0</td>
      <td>mobile</td>
      <td>[578845, 788068]</td>
      <td>[생활/주방가전, 침구/수예]</td>
      <td>[주방가전, 수예소품]</td>
      <td>[블랜더, 거실수예소품]</td>
      <td>F20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>9</td>
      <td>77777.8</td>
      <td>700000</td>
      <td>1</td>
      <td>9</td>
      <td>132.333</td>
      <td>1191</td>
      <td>1311.11</td>
      <td>11800</td>
      <td>3.625</td>
      <td>mobile</td>
      <td>[216947, 236174, 1965, 190233, 731145, 180447,...</td>
      <td>[화장품/뷰티케어, 화장품/뷰티케어, 건강식품, 화장품/뷰티케어, 화장품/뷰티케어,...</td>
      <td>[선케어, 스킨케어, 홍삼/인삼가공식품, 스킨케어, 스킨케어, 수영/물놀이, 메이크...</td>
      <td>[선크림류, 에센스/세럼, 홍삼액, 에센스/세럼, 페이셜클렌저, 남성수영복, BB/...</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>4</td>
      <td>24225</td>
      <td>96900</td>
      <td>1</td>
      <td>4</td>
      <td>21.75</td>
      <td>87</td>
      <td>297.25</td>
      <td>1189</td>
      <td>5.66667</td>
      <td>mobile</td>
      <td>[554217, 248358, 506337, 506359]</td>
      <td>[스포츠패션, 속옷/양말/홈웨어, 속옷/양말/홈웨어, 속옷/양말/홈웨어]</td>
      <td>[여성스포츠화, 여성속옷, 여성속옷, 여성속옷]</td>
      <td>[여성스포츠샌들/슬리퍼, 여성속옷세트, 여성팬티, 브래지어]</td>
      <td>F20</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9</td>
      <td>2</td>
      <td>10550</td>
      <td>21100</td>
      <td>1</td>
      <td>2</td>
      <td>249</td>
      <td>498</td>
      <td>5049</td>
      <td>10098</td>
      <td>0</td>
      <td>mobile</td>
      <td>[436275, 578537]</td>
      <td>[속옷/양말/홈웨어, 속옷/양말/홈웨어]</td>
      <td>[유아동속옷, 유아동속옷]</td>
      <td>[유아동팬티, 유아동팬티]</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12</td>
      <td>16</td>
      <td>14325.6</td>
      <td>229210</td>
      <td>1</td>
      <td>16</td>
      <td>144.938</td>
      <td>2319</td>
      <td>4187.25</td>
      <td>66996</td>
      <td>4.2</td>
      <td>mobile</td>
      <td>[178471, 233182, 141349, 64916, 72185, 489942,...</td>
      <td>[속옷/양말/홈웨어, 유아동의류, 패션잡화, 패션잡화, 속옷/양말/홈웨어, 속옷/양...</td>
      <td>[유아동양말류, 유아의류전신, 여성화, 모자, 유아동양말류, 유아동양말류, 모자, ...</td>
      <td>[유아동타이즈, 영유아점프수트/오버롤, 여성샌들, 아동모, 유아동일반양말, 유아동일...</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>263094</td>
      <td>1</td>
      <td>10000</td>
      <td>10000</td>
      <td>1</td>
      <td>1</td>
      <td>66</td>
      <td>66</td>
      <td>513</td>
      <td>513</td>
      <td>183</td>
      <td>mobile</td>
      <td>[536890]</td>
      <td>[패션잡화]</td>
      <td>[여성화]</td>
      <td>[여성플랫]</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>263095</td>
      <td>2</td>
      <td>122000</td>
      <td>244000</td>
      <td>1</td>
      <td>2</td>
      <td>220.5</td>
      <td>441</td>
      <td>1828</td>
      <td>3656</td>
      <td>83</td>
      <td>mobile</td>
      <td>[411059, 741695]</td>
      <td>[유아동의류, 화장품/뷰티케어]</td>
      <td>[여아의류아우터, 스킨케어]</td>
      <td>[여아점퍼, 스킨케어세트]</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>263096</td>
      <td>3</td>
      <td>28500</td>
      <td>85500</td>
      <td>1</td>
      <td>3</td>
      <td>256</td>
      <td>768</td>
      <td>4237</td>
      <td>12711</td>
      <td>0</td>
      <td>mobile</td>
      <td>[406269, 592279, 592285]</td>
      <td>[스포츠패션, 스포츠패션, 스포츠패션]</td>
      <td>[여성스포츠화, 남성일반스포츠의류, 남성일반스포츠의류]</td>
      <td>[여성런닝/트레이닝화, 남성일반스포츠바지, 남성일반스포츠바지]</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>263102</td>
      <td>1</td>
      <td>1080</td>
      <td>1080</td>
      <td>1</td>
      <td>1</td>
      <td>188</td>
      <td>188</td>
      <td>1812</td>
      <td>1812</td>
      <td>183</td>
      <td>mobile</td>
      <td>[785281]</td>
      <td>[문구/사무용품]</td>
      <td>[필기도구]</td>
      <td>[볼펜]</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>263103</td>
      <td>7</td>
      <td>58628.6</td>
      <td>410400</td>
      <td>1</td>
      <td>7</td>
      <td>280</td>
      <td>1960</td>
      <td>3249.43</td>
      <td>22746</td>
      <td>4.66667</td>
      <td>mobile</td>
      <td>[381544, 576311, 171218, 144818, 307370, 30736...</td>
      <td>[패션잡화, 식기/조리기구, 유아동의류, 시즌스포츠, 시즌스포츠, 시즌스포츠, 화장...</td>
      <td>[유아동화, 그릇/식기, 여아의류상의, 수영/물놀이, 수영/물놀이, 수영/물놀이, ...</td>
      <td>[유아동샌들, 커피잔, 여아티셔츠/탑, 아동수영복, 아동수영복, 아동수영복, 크림/...</td>
      <td>F30</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 17 columns</p>
</div>




```python
pd_c_dic = {}
for i, nm in enumerate(pd.concat([df, df_test], axis=0)['PD_C'].unique()):
    pd_c_dic[nm] = i
```


```python
pd_c_dic
```




    {578845: 0,
     788068: 1,
     180447: 2,
     731145: 3,
     216947: 4,
     491876: 5,
     1965: 6,
     457181: 7,
     190233: 8,
     236174: 9,
     248358: 10,
     506359: 11,
     506337: 12,
     554217: 13,
     436275: 14,
     578537: 15,
     536257: 16,
     141349: 17,
     535684: 18,
     75955: 19,
     29069: 20,
     41283: 21,
     489942: 22,
     760527: 23,
     233182: 24,
     178471: 25,
     64916: 26,
     535442: 27,
     535601: 28,
     64933: 29,
     72185: 30,
     102618: 31,
     809170: 32,
     837392: 33,
     765673: 34,
     660999: 35,
     804: 36,
     375713: 37,
     133407: 38,
     742158: 39,
     742089: 40,
     484590: 41,
     641183: 42,
     68947: 43,
     842488: 44,
     225207: 45,
     682061: 46,
     569802: 47,
     832202: 48,
     221420: 49,
     826313: 50,
     276275: 51,
     140325: 52,
     694453: 53,
     811116: 54,
     585440: 55,
     700995: 56,
     7893: 57,
     733585: 58,
     185099: 59,
     722909: 60,
     611777: 61,
     425921: 62,
     499940: 63,
     549195: 64,
     220714: 65,
     218301: 66,
     626454: 67,
     473112: 68,
     739681: 69,
     746888: 70,
     657620: 71,
     478990: 72,
     676622: 73,
     685372: 74,
     186166: 75,
     455026: 76,
     454153: 77,
     839324: 78,
     523581: 79,
     397797: 80,
     594883: 81,
     836917: 82,
     689380: 83,
     714091: 84,
     554220: 85,
     255735: 86,
     255805: 87,
     841644: 88,
     530357: 89,
     550764: 90,
     217166: 91,
     207163: 92,
     783266: 93,
     733465: 94,
     310728: 95,
     841642: 96,
     634905: 97,
     111788: 98,
     798665: 99,
     740039: 100,
     155106: 101,
     793821: 102,
     498418: 103,
     265906: 104,
     717107: 105,
     789521: 106,
     209960: 107,
     369827: 108,
     412912: 109,
     352717: 110,
     772781: 111,
     664328: 112,
     520714: 113,
     448099: 114,
     315356: 115,
     621557: 116,
     242314: 117,
     577673: 118,
     332489: 119,
     221077: 120,
     339653: 121,
     360023: 122,
     164956: 123,
     217672: 124,
     825162: 125,
     210784: 126,
     657088: 127,
     807682: 128,
     812803: 129,
     782362: 130,
     624221: 131,
     750564: 132,
     427373: 133,
     159530: 134,
     600643: 135,
     659195: 136,
     5609: 137,
     809732: 138,
     744190: 139,
     314843: 140,
     117061: 141,
     72033: 142,
     6353: 143,
     500350: 144,
     651086: 145,
     777470: 146,
     261331: 147,
     406787: 148,
     151256: 149,
     323680: 150,
     696708: 151,
     818129: 152,
     377510: 153,
     736885: 154,
     107273: 155,
     274604: 156,
     149519: 157,
     119734: 158,
     814520: 159,
     418525: 160,
     742656: 161,
     176758: 162,
     193137: 163,
     448598: 164,
     665779: 165,
     222: 166,
     812291: 167,
     199507: 168,
     595905: 169,
     431974: 170,
     480855: 171,
     234515: 172,
     417612: 173,
     589529: 174,
     635789: 175,
     402421: 176,
     21386: 177,
     59919: 178,
     59915: 179,
     45864: 180,
     42760: 181,
     788073: 182,
     24286: 183,
     237980: 184,
     279738: 185,
     830988: 186,
     831995: 187,
     657509: 188,
     599806: 189,
     520208: 190,
     678491: 191,
     328975: 192,
     225011: 193,
     704761: 194,
     331305: 195,
     236745: 196,
     331313: 197,
     331315: 198,
     325462: 199,
     754008: 200,
     525734: 201,
     316297: 202,
     234321: 203,
     552345: 204,
     809795: 205,
     656134: 206,
     482240: 207,
     224538: 208,
     190919: 209,
     264638: 210,
     705025: 211,
     526416: 212,
     488965: 213,
     353082: 214,
     484753: 215,
     673029: 216,
     668377: 217,
     668381: 218,
     203849: 219,
     847487: 220,
     624244: 221,
     130018: 222,
     391390: 223,
     305827: 224,
     439597: 225,
     844427: 226,
     742356: 227,
     614387: 228,
     347882: 229,
     417009: 230,
     703034: 231,
     684955: 232,
     833340: 233,
     484185: 234,
     384940: 235,
     741257: 236,
     524284: 237,
     230466: 238,
     425801: 239,
     248687: 240,
     465628: 241,
     528909: 242,
     843603: 243,
     801330: 244,
     188144: 245,
     823839: 246,
     672920: 247,
     248047: 248,
     838123: 249,
     773995: 250,
     464972: 251,
     773992: 252,
     846851: 253,
     469740: 254,
     580208: 255,
     778734: 256,
     667897: 257,
     522054: 258,
     742108: 259,
     89333: 260,
     40972: 261,
     214597: 262,
     835321: 263,
     627769: 264,
     771557: 265,
     17101: 266,
     17095: 267,
     17100: 268,
     179067: 269,
     695935: 270,
     836099: 271,
     564962: 272,
     432361: 273,
     742269: 274,
     497067: 275,
     835143: 276,
     590408: 277,
     1967: 278,
     1974: 279,
     415723: 280,
     789560: 281,
     789582: 282,
     273953: 283,
     588278: 284,
     218470: 285,
     164930: 286,
     539083: 287,
     128079: 288,
     182425: 289,
     803165: 290,
     804957: 291,
     350387: 292,
     590047: 293,
     661770: 294,
     337522: 295,
     337517: 296,
     350336: 297,
     340910: 298,
     98359: 299,
     589994: 300,
     215829: 301,
     220490: 302,
     199326: 303,
     70943: 304,
     783193: 305,
     776237: 306,
     481452: 307,
     648696: 308,
     308641: 309,
     379286: 310,
     548654: 311,
     634550: 312,
     298844: 313,
     584080: 314,
     525226: 315,
     377512: 316,
     132566: 317,
     835090: 318,
     345871: 319,
     793807: 320,
     214124: 321,
     589276: 322,
     786274: 323,
     798040: 324,
     840782: 325,
     827708: 326,
     556576: 327,
     556736: 328,
     556575: 329,
     786573: 330,
     839528: 331,
     787263: 332,
     774268: 333,
     793173: 334,
     787900: 335,
     795003: 336,
     240984: 337,
     839518: 338,
     787901: 339,
     774271: 340,
     511275: 341,
     190234: 342,
     732222: 343,
     687109: 344,
     715428: 345,
     282505: 346,
     484979: 347,
     24736: 348,
     75417: 349,
     91765: 350,
     220251: 351,
     737516: 352,
     419017: 353,
     246686: 354,
     365338: 355,
     475039: 356,
     146717: 357,
     542723: 358,
     70388: 359,
     369894: 360,
     710881: 361,
     363761: 362,
     540267: 363,
     607801: 364,
     339928: 365,
     231953: 366,
     433158: 367,
     405531: 368,
     9622: 369,
     362816: 370,
     346349: 371,
     647567: 372,
     820596: 373,
     702687: 374,
     765348: 375,
     430918: 376,
     384838: 377,
     699485: 378,
     700083: 379,
     454775: 380,
     694510: 381,
     106165: 382,
     835536: 383,
     724248: 384,
     477852: 385,
     763182: 386,
     427529: 387,
     1555: 388,
     3627: 389,
     138127: 390,
     499565: 391,
     189112: 392,
     55505: 393,
     818150: 394,
     402455: 395,
     226541: 396,
     499571: 397,
     398276: 398,
     751647: 399,
     422559: 400,
     62544: 401,
     20778: 402,
     80953: 403,
     54442: 404,
     616222: 405,
     398777: 406,
     166466: 407,
     43168: 408,
     164222: 409,
     608512: 410,
     692709: 411,
     667520: 412,
     528952: 413,
     233088: 414,
     685637: 415,
     844451: 416,
     325784: 417,
     233093: 418,
     586706: 419,
     232364: 420,
     808125: 421,
     610038: 422,
     231869: 423,
     471376: 424,
     116151: 425,
     835793: 426,
     224479: 427,
     350816: 428,
     467676: 429,
     593242: 430,
     92054: 431,
     695295: 432,
     106313: 433,
     261184: 434,
     457338: 435,
     392975: 436,
     187406: 437,
     818235: 438,
     788184: 439,
     547987: 440,
     90726: 441,
     587688: 442,
     813725: 443,
     831581: 444,
     119440: 445,
     82412: 446,
     467056: 447,
     801428: 448,
     737381: 449,
     153479: 450,
     542721: 451,
     237334: 452,
     723410: 453,
     732165: 454,
     654287: 455,
     817724: 456,
     606643: 457,
     461583: 458,
     337679: 459,
     178427: 460,
     183546: 461,
     301060: 462,
     179311: 463,
     564444: 464,
     369907: 465,
     704429: 466,
     466095: 467,
     576935: 468,
     648484: 469,
     26786: 470,
     26781: 471,
     170172: 472,
     783315: 473,
     325027: 474,
     427578: 475,
     92959: 476,
     39798: 477,
     10617: 478,
     22503: 479,
     746106: 480,
     448458: 481,
     662207: 482,
     338225: 483,
     358308: 484,
     339641: 485,
     686956: 486,
     746637: 487,
     274520: 488,
     396880: 489,
     805342: 490,
     721493: 491,
     621954: 492,
     743591: 493,
     163986: 494,
     771158: 495,
     685591: 496,
     805287: 497,
     795994: 498,
     219950: 499,
     613327: 500,
     143001: 501,
     221502: 502,
     686598: 503,
     77784: 504,
     70824: 505,
     44083: 506,
     62448: 507,
     831368: 508,
     571977: 509,
     346353: 510,
     427398: 511,
     239291: 512,
     802617: 513,
     1991: 514,
     319558: 515,
     470374: 516,
     802533: 517,
     355423: 518,
     612430: 519,
     285225: 520,
     612192: 521,
     101236: 522,
     285226: 523,
     514632: 524,
     225375: 525,
     397026: 526,
     451206: 527,
     361266: 528,
     180640: 529,
     796170: 530,
     448112: 531,
     707723: 532,
     463914: 533,
     318514: 534,
     678593: 535,
     721593: 536,
     134109: 537,
     782256: 538,
     807086: 539,
     484066: 540,
     484021: 541,
     278269: 542,
     730248: 543,
     549163: 544,
     228969: 545,
     290962: 546,
     451855: 547,
     579504: 548,
     813584: 549,
     531745: 550,
     84088: 551,
     538938: 552,
     673948: 553,
     339654: 554,
     637249: 555,
     236205: 556,
     565534: 557,
     751437: 558,
     462370: 559,
     524623: 560,
     131621: 561,
     166295: 562,
     146757: 563,
     37591: 564,
     37589: 565,
     228819: 566,
     157438: 567,
     737833: 568,
     331291: 569,
     845920: 570,
     766109: 571,
     847340: 572,
     95376: 573,
     283474: 574,
     484910: 575,
     464142: 576,
     484914: 577,
     21527: 578,
     139318: 579,
     68262: 580,
     744813: 581,
     304719: 582,
     455203: 583,
     382491: 584,
     462218: 585,
     812355: 586,
     602936: 587,
     177621: 588,
     633165: 589,
     737465: 590,
     439241: 591,
     638834: 592,
     745992: 593,
     100581: 594,
     600886: 595,
     295538: 596,
     737019: 597,
     649639: 598,
     433922: 599,
     348994: 600,
     325866: 601,
     503810: 602,
     168943: 603,
     793984: 604,
     288716: 605,
     398993: 606,
     602742: 607,
     185590: 608,
     737456: 609,
     185451: 610,
     179794: 611,
     633205: 612,
     507445: 613,
     737108: 614,
     492100: 615,
     489573: 616,
     426460: 617,
     643719: 618,
     381421: 619,
     498898: 620,
     86035: 621,
     8044: 622,
     584546: 623,
     737770: 624,
     457825: 625,
     514057: 626,
     351544: 627,
     189859: 628,
     737744: 629,
     757621: 630,
     812389: 631,
     731144: 632,
     737753: 633,
     807229: 634,
     296651: 635,
     273453: 636,
     588188: 637,
     248898: 638,
     813189: 639,
     838160: 640,
     373139: 641,
     423749: 642,
     775255: 643,
     802474: 644,
     2574: 645,
     701460: 646,
     731231: 647,
     385727: 648,
     405952: 649,
     358419: 650,
     516983: 651,
     666218: 652,
     396146: 653,
     333489: 654,
     462824: 655,
     286237: 656,
     587449: 657,
     803232: 658,
     143555: 659,
     263449: 660,
     141498: 661,
     741797: 662,
     175170: 663,
     742014: 664,
     156769: 665,
     1959: 666,
     130354: 667,
     818172: 668,
     818170: 669,
     333607: 670,
     826962: 671,
     737200: 672,
     125314: 673,
     405643: 674,
     363078: 675,
     222557: 676,
     675999: 677,
     680819: 678,
     8781: 679,
     701610: 680,
     496043: 681,
     421603: 682,
     197538: 683,
     332857: 684,
     574687: 685,
     574728: 686,
     347557: 687,
     196508: 688,
     303129: 689,
     127472: 690,
     563385: 691,
     771558: 692,
     349862: 693,
     710755: 694,
     27493: 695,
     134810: 696,
     59137: 697,
     742702: 698,
     482531: 699,
     339054: 700,
     142758: 701,
     668157: 702,
     699665: 703,
     518765: 704,
     7010: 705,
     792983: 706,
     637455: 707,
     812091: 708,
     736142: 709,
     5671: 710,
     763472: 711,
     791368: 712,
     719827: 713,
     800276: 714,
     156818: 715,
     156821: 716,
     678668: 717,
     334785: 718,
     62002: 719,
     379743: 720,
     63194: 721,
     279649: 722,
     174418: 723,
     799721: 724,
     673660: 725,
     158936: 726,
     147650: 727,
     550249: 728,
     523074: 729,
     732025: 730,
     268104: 731,
     653293: 732,
     255244: 733,
     232737: 734,
     689564: 735,
     343289: 736,
     64438: 737,
     496201: 738,
     295663: 739,
     729276: 740,
     449614: 741,
     810725: 742,
     681618: 743,
     513766: 744,
     688070: 745,
     470469: 746,
     824246: 747,
     98085: 748,
     674014: 749,
     352123: 750,
     467361: 751,
     290426: 752,
     77621: 753,
     8143: 754,
     357620: 755,
     671211: 756,
     807545: 757,
     104938: 758,
     446901: 759,
     462910: 760,
     450004: 761,
     456907: 762,
     539982: 763,
     594892: 764,
     684241: 765,
     705211: 766,
     547370: 767,
     38078: 768,
     661752: 769,
     18096: 770,
     34901: 771,
     243436: 772,
     666769: 773,
     106113: 774,
     337126: 775,
     132486: 776,
     221243: 777,
     150551: 778,
     109616: 779,
     669572: 780,
     411683: 781,
     274759: 782,
     302954: 783,
     591447: 784,
     46814: 785,
     83872: 786,
     224211: 787,
     492664: 788,
     679395: 789,
     73461: 790,
     706836: 791,
     125551: 792,
     601039: 793,
     600923: 794,
     134168: 795,
     456811: 796,
     366070: 797,
     335108: 798,
     226114: 799,
     846665: 800,
     91249: 801,
     340190: 802,
     780899: 803,
     489294: 804,
     489616: 805,
     780898: 806,
     31803: 807,
     32543: 808,
     32539: 809,
     32546: 810,
     305699: 811,
     155435: 812,
     787614: 813,
     31880: 814,
     604488: 815,
     43292: 816,
     140109: 817,
     31871: 818,
     450598: 819,
     791897: 820,
     784275: 821,
     32541: 822,
     95079: 823,
     14957: 824,
     747812: 825,
     23085: 826,
     699048: 827,
     405748: 828,
     847121: 829,
     440300: 830,
     183258: 831,
     810085: 832,
     394172: 833,
     27852: 834,
     447552: 835,
     133610: 836,
     810083: 837,
     23082: 838,
     191422: 839,
     191427: 840,
     191424: 841,
     212054: 842,
     154999: 843,
     222951: 844,
     101612: 845,
     638673: 846,
     578512: 847,
     643030: 848,
     259725: 849,
     679889: 850,
     108585: 851,
     115889: 852,
     113999: 853,
     95077: 854,
     825163: 855,
     625310: 856,
     352237: 857,
     525279: 858,
     58764: 859,
     50185: 860,
     428396: 861,
     631492: 862,
     814295: 863,
     747182: 864,
     343507: 865,
     735510: 866,
     561804: 867,
     343675: 868,
     527221: 869,
     365009: 870,
     727523: 871,
     255857: 872,
     354836: 873,
     83748: 874,
     359218: 875,
     295051: 876,
     465400: 877,
     317252: 878,
     338161: 879,
     349436: 880,
     120008: 881,
     155128: 882,
     111253: 883,
     128018: 884,
     149651: 885,
     391579: 886,
     85339: 887,
     2310: 888,
     178122: 889,
     67884: 890,
     615582: 891,
     726154: 892,
     719174: 893,
     733764: 894,
     134830: 895,
     463058: 896,
     463062: 897,
     655621: 898,
     750374: 899,
     260824: 900,
     125584: 901,
     469647: 902,
     444736: 903,
     53376: 904,
     724836: 905,
     432644: 906,
     128837: 907,
     535774: 908,
     286948: 909,
     570870: 910,
     201175: 911,
     352958: 912,
     128829: 913,
     785097: 914,
     7514: 915,
     835836: 916,
     8460: 917,
     801491: 918,
     754998: 919,
     390491: 920,
     716500: 921,
     605825: 922,
     574707: 923,
     742651: 924,
     530833: 925,
     737665: 926,
     481765: 927,
     742654: 928,
     671328: 929,
     201595: 930,
     722733: 931,
     642112: 932,
     647750: 933,
     742311: 934,
     742625: 935,
     737808: 936,
     190230: 937,
     112365: 938,
     195072: 939,
     195071: 940,
     689788: 941,
     334695: 942,
     167536: 943,
     671828: 944,
     563784: 945,
     27708: 946,
     350247: 947,
     72324: 948,
     72394: 949,
     840047: 950,
     33795: 951,
     2376: 952,
     772365: 953,
     802906: 954,
     19558: 955,
     253332: 956,
     547977: 957,
     742747: 958,
     677214: 959,
     840541: 960,
     826939: 961,
     346356: 962,
     320963: 963,
     690246: 964,
     567651: 965,
     514547: 966,
     193356: 967,
     377673: 968,
     526080: 969,
     350581: 970,
     536810: 971,
     709293: 972,
     710074: 973,
     326193: 974,
     677217: 975,
     730706: 976,
     654997: 977,
     763454: 978,
     313387: 979,
     5197: 980,
     812280: 981,
     118621: 982,
     835: 983,
     538088: 984,
     48019: 985,
     677215: 986,
     212710: 987,
     203160: 988,
     481045: 989,
     458353: 990,
     632139: 991,
     295137: 992,
     844453: 993,
     302856: 994,
     244483: 995,
     538085: 996,
     9991: 997,
     465895: 998,
     599577: 999,
     ...}




```python
data['pd_c'] = data['pd_c'].map(lambda x: [pd_c_dic[nm] for nm in x])
```

- 클라이언트 별 각 대 중 소 분류 검색 누적 숫자 => matrix 변환


```python
clac1_nm_dic = {}
for i, nm in enumerate(df['CLAC1_NM'].unique()):
    clac1_nm_dic[nm] = i
```


```python
clac1_nm_dic
```




    {'생활/주방가전': 0,
     '침구/수예': 1,
     '시즌스포츠': 2,
     '화장품/뷰티케어': 3,
     '건강식품': 4,
     '속옷/양말/홈웨어': 5,
     '스포츠패션': 6,
     '패션잡화': 7,
     '주방잡화': 8,
     '유아동의류': 9,
     '식기/조리기구': 10,
     '세제/위생': 11,
     '청소/세탁/욕실용품': 12,
     '출산/육아용품': 13,
     '냉장/세탁가전': 14,
     '퍼스널케어': 15,
     '구기/필드스포츠': 16,
     '원예/애완': 17,
     '음료': 18,
     '여성의류': 19,
     '남성의류': 20,
     '계절가전': 21,
     '냉장식품': 22,
     '아웃도어/레저': 23,
     '냉동식품': 24,
     '가구': 25,
     '완구': 26,
     '헬스/피트니스': 27,
     '축산물': 28,
     '컴퓨터': 29,
     '모바일': 30,
     '문구/사무용품': 31,
     '인테리어/조명': 32,
     '상품권': 33,
     '과일': 34,
     '자동차용품': 35,
     '영상/음향가전': 36}




```python
data['clac1_nm'] = data['clac1_nm'].map(lambda x: [clac1_nm_dic[nm] for nm in x])
```


```python
clac1_matrix = np.zeros((len(data), len(clac1_nm_dic)))
for i in range(len(data)):
    for j in data['clac1_nm'][i]:
        clac1_matrix[i, j] += 1
```

- 분류별 구매 횟수 계산 행렬


```python
clac1_matrix
```




    array([[1., 1., 0., ..., 0., 0., 0.],
           [0., 0., 2., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 3., ..., 0., 0., 0.]])




```python
cols = ['clac1_nm_' + str(i) for i in range(len(clac1_nm_dic))]
```


```python
clac1_df = pd.DataFrame(clac1_matrix)
clac1_df.columns = cols
```


```python
clac1_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clac1_nm_0</th>
      <th>clac1_nm_1</th>
      <th>clac1_nm_2</th>
      <th>clac1_nm_3</th>
      <th>clac1_nm_4</th>
      <th>clac1_nm_5</th>
      <th>clac1_nm_6</th>
      <th>clac1_nm_7</th>
      <th>clac1_nm_8</th>
      <th>clac1_nm_9</th>
      <th>...</th>
      <th>clac1_nm_27</th>
      <th>clac1_nm_28</th>
      <th>clac1_nm_29</th>
      <th>clac1_nm_30</th>
      <th>clac1_nm_31</th>
      <th>clac1_nm_32</th>
      <th>clac1_nm_33</th>
      <th>clac1_nm_34</th>
      <th>clac1_nm_35</th>
      <th>clac1_nm_36</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 37 columns</p>
</div>




```python
clac2_nm_dic = {}
for i, nm in enumerate(df['CLAC2_NM'].unique()):
    clac2_nm_dic[nm] = i
```


```python
clac2_nm_dic
```




    {'주방가전': 0,
     '수예소품': 1,
     '수영/물놀이': 2,
     '스킨케어': 3,
     '선케어': 4,
     '홍삼/인삼가공식품': 5,
     '메이크업': 6,
     '여성속옷': 7,
     '여성스포츠화': 8,
     '유아동속옷': 9,
     '유아동양말류': 10,
     '여성화': 11,
     '조리도구': 12,
     '유아의류전신': 13,
     '모자': 14,
     '조리기구': 15,
     '화장지/티슈': 16,
     '정리용품': 17,
     '유아스킨/바디케어': 18,
     '냉장/냉동고': 19,
     '핸드/풋케어': 20,
     '남성일반스포츠의류': 21,
     '골프': 22,
     '남성지갑': 23,
     '남성스포츠화': 24,
     '애견용품': 25,
     '생수': 26,
     '여성일반스포츠의류': 27,
     '건강진액': 28,
     '남성케어': 29,
     '우산/양산류': 30,
     '스포츠잡화': 31,
     '남성속옷': 32,
     '여성의류상의': 33,
     '여성의류전신': 34,
     '유아동침구': 35,
     '남성의류상의': 36,
     '남성의류하의': 37,
     '여성의류아우터': 38,
     '냉방가전': 39,
     '유아의류상의': 40,
     '여아의류상의': 41,
     '여성위생용품': 42,
     '유아위생용품': 43,
     '영양제': 44,
     '포장반찬': 45,
     '남성등산/아웃도어의류': 46,
     '유아동스포츠화': 47,
     '헤어케어': 48,
     '캠핑': 49,
     '등산': 50,
     '여성가방': 51,
     '유아의류하의': 52,
     '고양이용품': 53,
     '구강케어': 54,
     '남성의류아우터': 55,
     '냉동간편식': 56,
     '수납가구': 57,
     '사무용/학생용가구': 58,
     '수유/이유용품': 59,
     '유아발육용품': 60,
     '유아동화': 61,
     '여성의류하의': 62,
     '교육완구': 63,
     '피트니스': 64,
     '그릇/식기': 65,
     '남성골프의류': 66,
     '여성골프의류': 67,
     '거실가구': 68,
     '닭고기류': 69,
     '남아의류상의': 70,
     '홈웨어': 71,
     '주방가구': 72,
     '밀폐/보관용기': 73,
     '시계': 74,
     '바디케어': 75,
     '여행용가방류': 76,
     '기능성음료': 77,
     '캐쥬얼가방': 78,
     '컴퓨터/노트북': 79,
     '침실가구': 80,
     '모바일액세서리': 81,
     '성인침구': 82,
     '일반문구/사무용품': 83,
     '필기도구': 84,
     '안경/선글라스': 85,
     '여아의류하의': 86,
     '남아의류하의': 87,
     '커튼/블라인드류': 88,
     '여성지갑': 89,
     '욕실용품': 90,
     '남아완구': 91,
     '세탁세제': 92,
     '여성등산/아웃도어의류': 93,
     '패션액세서리': 94,
     '컴퓨터주변기기': 95,
     '남아의류세트': 96,
     '청소기': 97,
     '여성양말류': 98,
     '미용소품': 99,
     '여아완구': 100,
     '유아안전용품': 101,
     '모바일상품권': 102,
     '남성화': 103,
     '이미용가전': 104,
     '향수': 105,
     '주방정리용품/소모품': 106,
     '보석': 107,
     '남성가방': 108,
     '남성양말류': 109,
     '공기청정/가습/제습': 110,
     '여아의류아우터': 111,
     '건강보조식품': 112,
     '유아동가구': 113,
     '인라인/스케이트보드/킥보드': 114,
     '두유': 115,
     '견과류': 116,
     '자동차음향/가전기기': 117,
     '유아동일반스포츠의류': 118,
     '국산과일': 119,
     '세탁기': 120,
     '모바일기기': 121,
     '유아의류아우터': 122,
     '남성의류세트': 123,
     '카메라/캠코더': 124,
     'TV': 125,
     '축산선물세트': 126,
     '시공/DIY가구': 127}




```python
data['clac2_nm'] = data['clac2_nm'].map(lambda x: [clac2_nm_dic[nm] for nm in x])
```


```python
clac2_matrix = np.zeros((len(data), len(clac2_nm_dic)))
for i in range(len(data)):
    for j in data['clac2_nm'][i]:
        clac2_matrix[i, j] += 1
```


```python
clac2_matrix
```




    array([[1., 1., 0., ..., 0., 0., 0.],
           [0., 0., 2., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 3., ..., 0., 0., 0.]])




```python
cols = ['clac2_nm_' + str(i) for i in range(len(clac2_nm_dic))]
```


```python
clac2_df = pd.DataFrame(clac2_matrix)
clac2_df.columns = cols
```


```python
clac2_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clac2_nm_0</th>
      <th>clac2_nm_1</th>
      <th>clac2_nm_2</th>
      <th>clac2_nm_3</th>
      <th>clac2_nm_4</th>
      <th>clac2_nm_5</th>
      <th>clac2_nm_6</th>
      <th>clac2_nm_7</th>
      <th>clac2_nm_8</th>
      <th>clac2_nm_9</th>
      <th>...</th>
      <th>clac2_nm_118</th>
      <th>clac2_nm_119</th>
      <th>clac2_nm_120</th>
      <th>clac2_nm_121</th>
      <th>clac2_nm_122</th>
      <th>clac2_nm_123</th>
      <th>clac2_nm_124</th>
      <th>clac2_nm_125</th>
      <th>clac2_nm_126</th>
      <th>clac2_nm_127</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 128 columns</p>
</div>




```python
clac3_nm_dic = {}
for i, nm in enumerate(df['CLAC3_NM'].unique()):
    clac3_nm_dic[nm] = i
```


```python
clac3_nm_dic
```




    {'블랜더': 0,
     '거실수예소품': 1,
     '남성수영복': 2,
     '페이셜클렌저': 3,
     '선크림류': 4,
     '여성비치웨어': 5,
     '홍삼액': 6,
     'BB/파운데이션/컴팩트류': 7,
     '에센스/세럼': 8,
     '여성속옷세트': 9,
     '브래지어': 10,
     '여성팬티': 11,
     '여성스포츠샌들/슬리퍼': 12,
     '유아동팬티': 13,
     '유아동일반양말': 14,
     '여성샌들': 15,
     '유아동타이즈': 16,
     '주방칼/가위': 17,
     '영유아점프수트/오버롤': 18,
     '아동모': 19,
     '프라이팬': 20,
     '롤티슈': 21,
     '플라스틱서랍장': 22,
     '유아용화장품': 23,
     '일반형냉장고': 24,
     '핸드로션/크림': 25,
     '남성스포츠티셔츠': 26,
     '크림/밤/오일': 27,
     '골프공': 28,
     '골프연습장비': 29,
     '립글로즈/틴트': 30,
     '남성일반지갑': 31,
     '남성런닝/트레이닝화': 32,
     '애견주거/실내용품': 33,
     '생수': 34,
     '여성트레이닝복': 35,
     '여성런닝셔츠/캐미솔': 36,
     '채소즙': 37,
     '남성용스킨케어류': 38,
     '3단우산': 39,
     '스포츠가방': 40,
     '남성팬티': 41,
     '여성남방셔츠': 42,
     '여성원피스': 43,
     '유아동이불/이불커버': 44,
     '남성티셔츠': 45,
     '남성캐주얼바지': 46,
     '여성코트': 47,
     '남성청바지': 48,
     '기타여성속옷': 49,
     '기타냉방가전': 50,
     '영유아티셔츠/탑': 51,
     '여아티셔츠/탑': 52,
     '생리대': 53,
     '유아용기저귀': 54,
     '유산균/프로바이오틱스': 55,
     '김치류': 56,
     '남성등산바지': 57,
     '유아동스포츠샌들/슬리퍼': 58,
     '골프패션잡화': 59,
     '염모제': 60,
     '장우산': 61,
     '골프필드용품': 62,
     '텐트': 63,
     '기타에어컨': 64,
     '여성스니커즈': 65,
     '남성정장셔츠': 66,
     '토스터/제빵기': 67,
     '선풍기': 68,
     '유아동런닝셔츠': 69,
     '배낭': 70,
     '여성크로스백': 71,
     '남성런닝셔츠': 72,
     '남성남방셔츠': 73,
     '여성스웨터/풀오버': 74,
     '영유아스커트': 75,
     '여아가디건': 76,
     '아동수영복': 77,
     '미스트': 78,
     '스킨/토너': 79,
     '애견장난감/훈련': 80,
     '고양이캣타워/실내용품': 81,
     '기타구강관리용품': 82,
     '남성등산티셔츠': 83,
     '남성점퍼': 84,
     '팬티라이너': 85,
     '여성로퍼': 86,
     '여성티셔츠/탑': 87,
     '냉동국탕류': 88,
     '서랍장/수납장': 89,
     '책상의자': 90,
     '스킨케어세트': 91,
     '페이셜팩류': 92,
     '출산/신생아용품세트': 93,
     '아기띠/캐리어': 94,
     '여성스포츠티셔츠/탑': 95,
     '유아동샌들': 96,
     '여성바지': 97,
     '영유아원피스': 98,
     '남성정장바지': 99,
     '남성일반스포츠바지': 100,
     '미술/창작완구': 101,
     '기타요가/필라테스소품': 102,
     '숟가락/젓가락': 103,
     '아쿠아슈즈': 104,
     '여성재킷': 105,
     '영유아바지': 106,
     '남성골프바지': 107,
     '유아동침구세트': 108,
     '유아/아동용치약': 109,
     '스포츠모자': 110,
     '유아동슬리퍼': 111,
     '여성골프패딩': 112,
     '여성숄더백': 113,
     '야구모자': 114,
     '유아동스니커즈': 115,
     '헤어에센스': 116,
     '탁자': 117,
     '닭가슴살': 118,
     '유아동내의': 119,
     '남아티셔츠/탑': 120,
     '오리발/스노클링': 121,
     '남아잠옷': 122,
     '식탁의자': 123,
     '유아용물티슈': 124,
     '애견간식': 125,
     '여성플랫': 126,
     '애견사료': 127,
     '반찬통/밀폐용기': 128,
     '어린이홍삼': 129,
     '스포츠시계': 130,
     '고양이모래/배변용품': 131,
     '남성스포츠샌들/슬리퍼': 132,
     '혼합즙': 133,
     '아이브로우': 134,
     '아이케어': 135,
     '바디워시': 136,
     '유아용샴푸/바디워시': 137,
     '샴푸': 138,
     '남성가디건': 139,
     '여성가디건': 140,
     '캐리어': 141,
     '한방음료': 142,
     '아동용가방': 143,
     '여성일반스포츠바지': 144,
     '노트북': 145,
     '메이크업세트': 146,
     '여성임부속옷': 147,
     '접시': 148,
     '반상기세트/홈세트': 149,
     '장롱': 150,
     '매트리스': 151,
     '여성점퍼': 152,
     '홈웨어세트': 153,
     '남성골프티셔츠': 154,
     '여성스웨트셔츠/후드/집업': 155,
     '남성비치웨어': 156,
     '기타모바일액세서리': 157,
     '성인침구속통/솜': 158,
     '바구니': 159,
     '테이프': 160,
     '칼/가위': 161,
     '수정용품': 162,
     '유아동선글라스': 163,
     '여성클러치백': 164,
     '영유아청바지': 165,
     '캠핑테이블/의자': 166,
     '여아바지': 167,
     '남아청바지': 168,
     '고양이사료': 169,
     '커튼': 170,
     '기타물놀이용품': 171,
     '여성신발부속품': 172,
     '노트북가방': 173,
     '여성카드/명함지갑': 174,
     '책상': 175,
     '여성스커트': 176,
     '욕실발판': 177,
     '마스카라': 178,
     '남성시계': 179,
     '남성패딩': 180,
     '식탁세트': 181,
     '피규어': 182,
     '욕실소품': 183,
     '롤플레잉완구': 184,
     '기타캠핑용품': 185,
     '분말표백제': 186,
     '풀': 187,
     '종합영양제': 188,
     '피트니스용품': 189,
     '스툴/리빙의자': 190,
     '책장': 191,
     '칫솔': 192,
     '수건': 193,
     '액상세탁세제': 194,
     '영유아블라우스': 195,
     '남아바지': 196,
     '헤어케어선물세트': 197,
     '여성등산티셔츠/탑': 198,
     '남성베스트': 199,
     '썬캡': 200,
     '스카프': 201,
     '블러셔/쉐이딩/하이라이터': 202,
     '여아레깅스': 203,
     '애견목욕/위생용품': 204,
     '기타일반문구/사무용품': 205,
     '린스/컨디셔너': 206,
     '샴푸/린스세트': 207,
     '미용비누': 208,
     '성인매트리스커버': 209,
     '스냅백': 210,
     '기타컴퓨터액세서리': 211,
     '유아동플랫': 212,
     '기타영양제': 213,
     '여아남방셔츠': 214,
     '남아의류세트': 215,
     '탄산수': 216,
     '일반청소기': 217,
     '손싸개/발싸개': 218,
     '유아동런닝/트레이닝화': 219,
     '여성토트백': 220,
     '여성일반양말': 221,
     '여성런닝/트레이닝화': 222,
     '치약': 223,
     '샤워/목욕도구/목욕헤어밴드': 224,
     '봉제인형': 225,
     '유아동베개/베개커버': 226,
     '놀이방매트': 227,
     '식음료모바일상품권': 228,
     '여아베스트': 229,
     '성인베개/베개커버': 230,
     '이유식용품': 231,
     '아이섀도우': 232,
     '남성샌들': 233,
     '기타이미용가전': 234,
     '요가/필라테스복': 235,
     '여성향수': 236,
     '기타정리용품': 237,
     '트리트먼트/팩': 238,
     '욕실청소용품': 239,
     '기타주방정리용품/소모품': 240,
     '여아청바지': 241,
     '기름종이': 242,
     '아이라이너': 243,
     '홍삼/인삼혼합세트': 244,
     '여성시계': 245,
     '손수건': 246,
     '팔찌': 247,
     '남성숄더/크로스백': 248,
     '지퍼백/비닐백': 249,
     '성인패드/스프레드': 250,
     '여성펌프스': 251,
     '구강청정제': 252,
     '남성일반양말': 253,
     '여성오픈토': 254,
     '공기청정기': 255,
     '여성등산바지': 256,
     '커튼링/커튼봉/부속품': 257,
     '메이크업베이스/프라이머': 258,
     '여아점퍼': 259,
     '남성등산점퍼/재킷': 260,
     '유아동슬립온': 261,
     '인삼가공식품': 262,
     '여성덧신류': 263,
     '냉동핫도그': 264,
     '여성블라우스': 265,
     '식기건조대/수저통': 266,
     '국자/뒤지개/주걱': 267,
     '여성부츠': 268,
     '여성베스트': 269,
     '에멀젼/로션': 270,
     '기타조리도구': 271,
     '바디보습': 272,
     '여성패딩': 273,
     '성인이불/이불커버': 274,
     '솥': 275,
     '이불/옷압축팩': 276,
     '섬유유연제/향기지속제': 277,
     '양산': 278,
     '여성수영복': 279,
     '남성내의': 280,
     '기타등산용품': 281,
     '유아용세척용품': 282,
     '도마': 283,
     '생활모바일상품권': 284,
     '젖병/젖꼭지': 285,
     '아동우산': 286,
     '케이스/보호필름': 287,
     '남성스킨케어세트': 288,
     '여성백팩': 289,
     '커피머신': 290,
     '방석/방석커버': 291,
     '장식장/진열장': 292,
     '다이어트보조식품': 293,
     '남성스니커즈': 294,
     '남성용클렌저': 295,
     '소품가방': 296,
     '변기시트/커버': 297,
     '헤드웨어': 298,
     '여행용소품': 299,
     '목걸이': 300,
     '미용보조식품': 301,
     '제빵용품': 302,
     'PC부품': 303,
     '등산화': 304,
     '젤네일/케어류': 305,
     '핸디형청소기': 306,
     '주방선반/걸이대': 307,
     '옷걸이': 308,
     '유아동침대': 309,
     '로봇청소기': 310,
     '헤어드라이어': 311,
     '레저모바일상품권': 312,
     '스폰지/퍼프': 313,
     '커피용품': 314,
     '커피잔': 315,
     '저장장치': 316,
     '우주복': 317,
     '립스틱/립라이너': 318,
     '조립/프라모델': 319,
     '스케이트보드/킥보드': 320,
     '남성로퍼': 321,
     '여성쪼리': 322,
     '여성트렌치코트': 323,
     '남성정장재킷': 324,
     '골프화': 325,
     '남성트레이닝복': 326,
     '전기찜기': 327,
     '남성향수': 328,
     '일반비타민': 329,
     '여성슬링백': 330,
     '촉각놀이/오뚝이': 331,
     '기타패션잡화': 332,
     '데오도란트': 333,
     '수영모자': 334,
     '여성슬리퍼': 335,
     '고무장갑': 336,
     '일반두유': 337,
     '속눈썹/쌍꺼풀': 338,
     '물안경': 339,
     '기타여행용가방': 340,
     '남성트렌치코트': 341,
     '발찌': 342,
     '남성스웨터/풀오버': 343,
     '호두': 344,
     '캠핑침구': 345,
     '아동비치웨어': 346,
     '애견의류/악세서리': 347,
     '성인침구세트': 348,
     '남성등산패딩': 349,
     '냉동만두': 350,
     '냄비': 351,
     '남성캐주얼재킷': 352,
     '승마운동기': 353,
     '바디케어세트': 354,
     '거들': 355,
     '성인요/요커버': 356,
     '여성내의': 357,
     '루테인': 358,
     '여성청바지': 359,
     '여성잠옷': 360,
     '인덕션/가스레인지': 361,
     '캠핑취사': 362,
     '마우스': 363,
     '블랙박스': 364,
     '포크/나이프': 365,
     '남성속옷세트': 366,
     '여성슬립온': 367,
     '오메가3/기타추출오일': 368,
     '헤어세팅기': 369,
     '키보드': 370,
     '모바일배터리/충전기': 371,
     '채반/바구니/쟁반': 372,
     '선반장/행거': 373,
     '안경테': 374,
     '여성골프바지': 375,
     '커피메이커/포트': 376,
     '유아동스포츠티셔츠/탑': 377,
     '유아동스포츠스웨트셔츠/후드/집업': 378,
     '고양이간식': 379,
     '사무용/학생용가구세트': 380,
     '벽걸이형에어컨': 381,
     '여성등산점퍼/재킷': 382,
     '홍삼정/분말/환': 383,
     '토마토': 384,
     '스킨케어디바이스': 385,
     '입욕제/스파제품': 386,
     '과일즙': 387,
     '전기튀김기': 388,
     '성인담요': 389,
     '귀걸이': 390,
     '소파': 391,
     '행주': 392,
     '여성골프남방셔츠': 393,
     '영유아남방셔츠': 394,
     '스텝퍼/트위스트': 395,
     '여성등산패딩': 396,
     '식탁': 397,
     '전기밥솥': 398,
     '대접/볼': 399,
     '밥공기': 400,
     '찬기/종지': 401,
     '여성발가락양말': 402,
     '역할놀이': 403,
     '유아용카시트/매트': 404,
     '젖병소독/건조용품': 405,
     '유아/아동용칫솔': 406,
     '기타냉동간편식': 407,
     '유아목욕용품': 408,
     '유아동트레이닝복': 409,
     '여아잠옷': 410,
     '선반/걸이': 411,
     '여성양말선물세트': 412,
     '물티슈': 413,
     '남성코트': 414,
     '분말세탁세제': 415,
     '수유패드/보조용품': 416,
     '유아동의자': 417,
     '학생용가방': 418,
     '남성스포츠점퍼/재킷': 419,
     '유아공부상/디딤대': 420,
     '잉크/토너': 421,
     '여성향수세트': 422,
     '펜던트': 423,
     '여성일반지갑': 424,
     '보드게임': 425,
     '레고': 426,
     '유아동스포츠점퍼/재킷': 427,
     '치약/칫솔세트': 428,
     '수영가방': 429,
     '패션인형': 430,
     '도시락/찬합': 431,
     '발효원액': 432,
     '남성등산베스트': 433,
     '여성선글라스': 434,
     '여성점프수트/오버롤': 435,
     '치아발육기/딸랑이': 436,
     '남성카드/명함지갑': 437,
     '남성용선크림/메이크업류': 438,
     '남성골프점퍼/재킷': 439,
     '여성골프티셔츠/탑': 440,
     '기타국산과일류': 441,
     '여아스커트': 442,
     '건조기': 443,
     '기타기능성음료': 444,
     '스피커': 445,
     '얼음/빙수용품': 446,
     '스타킹': 447,
     '보온병/텀블러': 448,
     '전동칫솔/칫솔모': 449,
     '여아스웨트셔츠/후드/집업': 450,
     '혼합견과': 451,
     '볼펜': 452,
     '필통': 453,
     '샤프/샤프심': 454,
     '필기구세트': 455,
     '엽산/철분': 456,
     '유아패션잡화': 457,
     '휴대폰': 458,
     '각티슈/미용티슈': 459,
     '골프가방': 460,
     '여성타이즈': 461,
     '요가/스포츠매트': 462,
     '여성골프스커트': 463,
     '골프장갑': 464,
     '여성골프니트/가디건': 465,
     '여성골프베스트': 466,
     '영유아점퍼': 467,
     '영유아가디건': 468,
     '남성스웨트셔츠/후드/집업': 469,
     '사인펜': 470,
     '남성선글라스': 471,
     '반지': 472,
     '이어폰/헤드폰': 473,
     '조리도구세트': 474,
     '키친타올': 475,
     '남녀공용향수': 476,
     '유아동레인부츠/슈즈': 477,
     '유아건강보조제': 478,
     '여성가운': 479,
     '남성잠옷': 480,
     '유아동스포츠패딩': 481,
     '여아패딩': 482,
     '전통/종교장신구': 483,
     '복근/벨트마사지기구': 484,
     '벙거지': 485,
     '슬립': 486,
     '만년필': 487,
     '공병/모델링팩전용도구': 488,
     '화장대': 489,
     '핸드카트': 490,
     '여성레인부츠/슈즈': 491,
     '퍼즐': 492,
     '캐쥬얼크로스백': 493,
     '음악/악기완구': 494,
     '아기체육관/러닝홈': 495,
     '면봉/화장솜': 496,
     '남성골프남방셔츠': 497,
     '수예소품속통/솜': 498,
     '쿠션/쿠션커버': 499,
     '붙박이장': 500,
     '기타견과류': 501,
     '아몬드': 502,
     '여성등산베스트': 503,
     '영유아레깅스': 504,
     '여성컴포트화': 505,
     '남성정장화': 506,
     '블라인드/버티컬': 507,
     '스포츠두건/머플러/마스크': 508,
     '남성골프패딩': 509,
     '스포츠양말': 510,
     '남성정장세트': 511,
     '음료용컵': 512,
     '인라인/스케이트보드/킥보드안전용품': 513,
     '애견식기/물병': 514,
     '복숭아': 515,
     '글루코사민': 516,
     '헤어브러쉬/롤': 517,
     '거실화/실내화': 518,
     '유아동담요': 519,
     '고데기': 520,
     '칼슘/미네랄': 521,
     '주방수예소품': 522,
     '무선조종': 523,
     '수세미/솔': 524,
     '유모차': 525,
     '남성덧신류': 526,
     '참외': 527,
     '사과': 528,
     '제습기': 529,
     '양문형냉장고': 530,
     '메이크업브러쉬': 531,
     '가습기': 532,
     '유아동요/요커버': 533,
     '구명조끼/안전용품': 534,
     '네일케어도구': 535,
     '애견건강용품': 536,
     '오븐/전자레인지': 537,
     '압력솥': 538,
     '기타유아동화': 539,
     '유아동일반스포츠바지': 540,
     '여성세정제': 541,
     '비닐장갑': 542,
     '스포츠선글라스': 543,
     '미니자동차': 544,
     '전자교육완구': 545,
     '수박': 546,
     '바디슬리밍/리프팅': 547,
     '남아셔츠': 548,
     '헤어무스/젤': 549,
     '남아실내복': 550,
     '풋케어': 551,
     '여아재킷': 552,
     '제기': 553,
     '일반교육완구': 554,
     '올인원': 555,
     '유축기': 556,
     '욕실화': 557,
     '필기도구소모품': 558,
     '남성머니클립': 559,
     '헤어스프레이': 560,
     '스팀청소기': 561,
     '여성스포츠점퍼/재킷': 562,
     '기타보석류': 563,
     '자두': 564,
     '메론': 565,
     '멀티형에어컨': 566,
     '기타모자': 567,
     '유아동패드/스프레드': 568,
     '카메라액세서리': 569,
     '전기면도기': 570,
     '유아동방한화': 571,
     '남성서류가방': 572,
     '책상정리용품': 573,
     '비니': 574,
     '남성슬립온': 575,
     '디저트포크/스푼': 576,
     '목욕용장난감': 577,
     '우비': 578,
     '주방수납장': 579,
     '유아동수납장': 580,
     '냉동떡볶이': 581,
     '건강보조식품세트': 582,
     '특수용세탁세제': 583,
     '여성레깅스': 584,
     '집게/클립': 585,
     '캐노피': 586,
     '핸드워시/손세정제': 587,
     '유아용욕조': 588,
     '모유보관용품': 589,
     '바운서/쏘서/보행기': 590,
     '발포비타민': 591,
     '여성스포츠베스트': 592,
     '드럼세탁기': 593,
     '고양이건강용품': 594,
     '스탠드형에어컨': 595,
     '남성신발부속품': 596,
     '유아동속옷세트': 597,
     '자연유래영양제': 598,
     '조리기구세트': 599,
     '남성골프스웨트셔츠/후드/집업': 600,
     '여성방한화': 601,
     'LED': 602,
     'UHD': 603,
     '냉동튀김': 604,
     '캐쥬얼백팩': 605,
     '욕실수납용품': 606,
     '연필깎이': 607,
     '연필': 608,
     '탐폰': 609,
     '운동보조식품': 610,
     '남성백팩': 611,
     '유아동로퍼': 612,
     '헤어왁스': 613,
     '배냇저고리': 614,
     '영유아베스트': 615,
     '고양이장난감': 616,
     '고양이목욕/위생용품': 617,
     '여성스포츠스웨트셔츠/후드/집업': 618,
     '국그릇': 619,
     '남성힙색': 620,
     '남성스포츠화부속품': 621,
     '여아블라우스': 622,
     '기타주방가전': 623,
     '남성스포츠스웨트셔츠/후드/집업': 624,
     '서류정리용품': 625,
     '숙취해소음료': 626,
     '배': 627,
     '영화/문화모바일상품권': 628,
     '남성스포츠속옷': 629,
     '보온도시락': 630,
     '남아스웨트셔츠/후드/집업': 631,
     '홍삼절편': 632,
     '침실가구세트': 633,
     '일반네일/케어류': 634,
     '남성슬리퍼': 635,
     '시계세트': 636,
     '한우선물세트': 637,
     '남성클러치백': 638,
     '붕붕카/스프링카/흔들말': 639,
     '이발기': 640,
     '삼계탕용닭': 641,
     '남아레깅스': 642,
     '블록': 643,
     '기타청소기': 644,
     '남성양말선물세트': 645,
     '남성캐쥬얼스포츠양말': 646,
     '목욕타올': 647,
     '여성골프스웨트셔츠/후드/집업': 648,
     '호일/랩/기름종이': 649,
     '파일/바인더': 650,
     '기타피트니스기구': 651,
     '영양제세트': 652,
     '튜브/보트': 653,
     '에어로빅복': 654,
     '여성사파리': 655,
     '러닝/워킹머신': 656,
     '롤스크린': 657,
     '애견이동장': 658,
     '액상표백제': 659,
     '야외용돗자리': 660,
     '침대': 661,
     '여성등산전신/원피스': 662,
     '메탈미용소도구': 663,
     '모빌': 664,
     '주전자': 665,
     '주류잔': 666,
     '오븐팬/피자팬': 667,
     '홍삼근': 668,
     '남성사파리': 669,
     '여성골프점퍼/재킷': 670,
     '형광펜': 671,
     '독서대': 672,
     '보석세트': 673,
     '열쇠고리': 674,
     '기타여성의류아우터': 675,
     '영유아코트': 676,
     '군모': 677,
     '헬스바이크': 678,
     '남성등산/아웃도어세트': 679,
     '남성등산전신': 680,
     '유아동부츠': 681,
     '남성컴포트화': 682,
     '영유아스웨터/풀오버': 683,
     '스탠드형김치냉장고': 684,
     '부분세탁제': 685,
     '닭윗날개(봉)': 686,
     '샤워커튼': 687,
     '골프채': 688,
     '미러리스': 689,
     '기타유아안전용품': 690,
     '영유아재킷': 691,
     '등산지팡이/스틱': 692,
     '땅콩': 693,
     '냉동밥': 694,
     '스포츠아대/헤어밴드': 695,
     '순금/순은/장식품': 696,
     '가발/부분가발': 697,
     '여성등산/아웃도어세트': 698,
     '전기그릴': 699,
     '그릴/구이불판': 700,
     '컵/행주살균기': 701,
     '뚝배기': 702,
     '기타유아동양말류': 703,
     '유아변기/배변훈련기': 704,
     '여성골프전신/원피스': 705,
     '무릎담요': 706,
     '태블릿PC': 707,
     '캐슈넛': 708,
     '남성스포츠패딩': 709,
     '싱크대/배수구용품': 710,
     '여성스포츠속옷': 711,
     '교자상/다용도상': 712,
     '캐쥬얼힙색': 713,
     '매직/보드마카': 714,
     '영유아스웨트셔츠/후드/집업': 715,
     '남성골프베스트': 716,
     '여성스포츠스커트': 717,
     '자/제도용품': 718,
     '문구세트': 719,
     '남성수면양말': 720,
     '남아스웨터/풀오버': 721,
     '일반세탁기': 722,
     '캐쥬얼숄더백': 723,
     '이불/옷커버류': 724,
     '전기냄비/뚝배기': 725,
     '피스타치오': 726,
     '돼지고기선물세트': 727,
     '전자계산기': 728,
     '힙색/사이드백': 729,
     '머플러': 730,
     '넥워머': 731,
     '젓갈': 732,
     '세탁비누': 733,
     '목욕가운': 734,
     '남성발가락양말': 735,
     '공유기': 736,
     '협탁': 737,
     '성인침대커버/스커트': 738,
     '명함정리용품': 739,
     '절임반찬': 740,
     '과실주병': 741,
     '립밤/립스크럽': 742,
     '제모용품': 743,
     '제모기': 744,
     '계량도구': 745,
     '보드류': 746,
     '프린터/복합기/스캐너': 747,
     '양념통': 748,
     '방울토마토': 749,
     '밤': 750,
     '스포츠목걸이/팔찌': 751,
     '고양이식기/급수': 752,
     '그늘막/타프': 753,
     '기타모바일기기': 754,
     '유아동옷장': 755,
     '남아가디건': 756,
     '테이블데코': 757,
     '여성스포츠전신/원피스': 758,
     '냅킨': 759,
     '바란스': 760,
     '뚜껑형김치냉장고': 761,
     '남성골프니트/가디건': 762,
     '여아코트': 763,
     '유아동매트리스커버': 764,
     '여성스포츠패딩': 765,
     '다기류': 766,
     '컴팩트': 767,
     '2단우산': 768,
     '기타냉장고': 769,
     '물병': 770,
     '데스크탑/올인원PC': 771,
     '기타카메라': 772,
     '마카다미아': 773,
     '냉동부침': 774,
     '브로치': 775,
     '남아베스트': 776,
     '여행용세트': 777,
     '귤류': 778,
     '하이앤드': 779,
     '항아리/쌀독류': 780,
     '헤어롤': 781,
     '여아스웨터/풀오버': 782,
     '냉동고': 783,
     '하이브리드': 784,
     '기타여성양말류': 785,
     '패션액세서리세트': 786,
     '기타영유아아우터': 787,
     '잣': 788,
     '스포츠음료': 789,
     '스테이플러': 790,
     '영유아패딩': 791,
     '기타배낭소품': 792,
     '시공가구': 793,
     'DIY가구': 794,
     '살구': 795,
     '여성수면양말': 796,
     '유아동침구매트': 797,
     '기타남성양말류': 798,
     '정수기': 799,
     '여아실내복': 800,
     '모니터': 801,
     '유아동침구속통/솜': 802,
     '전동보드/전동킥보드': 803,
     '공간박스': 804,
     '하이패스': 805,
     '신발장': 806,
     'DSLR': 807,
     '남성실내복': 808,
     '기타남성화': 809,
     '네비게이션': 810,
     '전기프라이팬': 811,
     '여성실내복': 812,
     '오프너/와인스크류': 813,
     '유아동시계': 814,
     '환풍기': 815,
     '볶음반찬': 816,
     '파티션': 817,
     '냉온풍기': 818,
     '펀치류': 819,
     '냉동피자': 820,
     '기차/레일완구': 821,
     '인라인/롤러스케이트': 822,
     '매실': 823,
     '주방용탈수기': 824,
     '유아동침대커버/스커트': 825,
     '기타자동차가전기기': 826,
     '여성캐쥬얼스포츠양말': 827,
     '네일세트': 828,
     '남성스포츠베스트': 829,
     '채칼/강판/절구': 830,
     '캐쥬얼시계': 831,
     '니삭스/오버니삭스': 832,
     '싸인물/자석/압핀': 833,
     '고양이의류/악세서리': 834,
     '비타민/에너지음료': 835,
     '에어워셔': 836,
     '냉장/냉동가전소모품': 837,
     '남성부츠': 838,
     '물걸레청소기': 839,
     '음식물건조기': 840,
     '요구르트/청국장제조기': 841,
     '골프채세트': 842,
     '고양이이동장': 843,
     '냉동면': 844,
     '소프트웨어': 845,
     '캠코더': 846,
     '무화과': 847,
     'OLED': 848,
     '탈수기': 849,
     '육가공품선물세트': 850,
     '딸기': 851,
     '식기세척기': 852,
     '차량용충전기': 853,
     '유아두유': 854,
     '닭근위': 855,
     '닭아랫날개(윙)': 856,
     '안경소품': 857,
     '여성가방액세서리': 858,
     '태닝/애프터선케어': 859,
     '유아동스포츠스커트': 860,
     '카메라렌즈': 861,
     '닭안심': 862,
     '포도': 863,
     '볶음탕용닭': 864,
     '콜렉션인형': 865,
     'LCD': 866,
     '리모컨/액세서리': 867,
     '반죽기/제면기': 868,
     '식기건조기': 869,
     '단무지': 870,
     '닭다리': 871,
     '감': 872,
     '오리고기': 873,
     '침구청소기': 874,
     '싱크대': 875,
     '미용거울': 876,
     '남녀공용향수세트': 877,
     '인라인/스케이트보드/킥보드기타액세서리': 878,
     '남성등산스웨트셔츠/후드/집업': 879,
     '커튼류세트': 880,
     '오토캠핑용품세트': 881,
     '수도용품': 882}




```python
data['clac3_nm'] = data['clac3_nm'].map(lambda x: [clac3_nm_dic[nm] for nm in x])
```


```python
clac3_matrix = np.zeros((len(data), len(clac3_nm_dic)))
for i in range(len(data)):
    for j in data['clac3_nm'][i]:
        clac3_matrix[i, j] += 1
```


```python
clac3_matrix
```




    array([[1., 1., 0., ..., 0., 0., 0.],
           [0., 0., 1., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.]])




```python
cols = ['clac3_nm_' + str(i) for i in range(len(clac3_nm_dic))]
```


```python
clac3_df = pd.DataFrame(clac3_matrix)
clac3_df.columns = cols
```


```python
clac3_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clac3_nm_0</th>
      <th>clac3_nm_1</th>
      <th>clac3_nm_2</th>
      <th>clac3_nm_3</th>
      <th>clac3_nm_4</th>
      <th>clac3_nm_5</th>
      <th>clac3_nm_6</th>
      <th>clac3_nm_7</th>
      <th>clac3_nm_8</th>
      <th>clac3_nm_9</th>
      <th>...</th>
      <th>clac3_nm_873</th>
      <th>clac3_nm_874</th>
      <th>clac3_nm_875</th>
      <th>clac3_nm_876</th>
      <th>clac3_nm_877</th>
      <th>clac3_nm_878</th>
      <th>clac3_nm_879</th>
      <th>clac3_nm_880</th>
      <th>clac3_nm_881</th>
      <th>clac3_nm_882</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 883 columns</p>
</div>




```python
data_concat = pd.concat([data.iloc[:, :-6], clac1_df, clac2_df, clac3_df, data.iloc[:, -1]], axis=1)
```


```python
data_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clnt_id</th>
      <th>num_shopping</th>
      <th>avg_price</th>
      <th>total_price</th>
      <th>avg_ct</th>
      <th>total_ct</th>
      <th>avg_sess_view</th>
      <th>total_sess_view</th>
      <th>avg_sess_hr</th>
      <th>total_sess_hr</th>
      <th>...</th>
      <th>clac3_nm_874</th>
      <th>clac3_nm_875</th>
      <th>clac3_nm_876</th>
      <th>clac3_nm_877</th>
      <th>clac3_nm_878</th>
      <th>clac3_nm_879</th>
      <th>clac3_nm_880</th>
      <th>clac3_nm_881</th>
      <th>clac3_nm_882</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>2</td>
      <td>43250</td>
      <td>86500</td>
      <td>1</td>
      <td>2</td>
      <td>59</td>
      <td>118</td>
      <td>922</td>
      <td>1844</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>9</td>
      <td>77777.8</td>
      <td>700000</td>
      <td>1</td>
      <td>9</td>
      <td>132.333</td>
      <td>1191</td>
      <td>1311.11</td>
      <td>11800</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>4</td>
      <td>24225</td>
      <td>96900</td>
      <td>1</td>
      <td>4</td>
      <td>21.75</td>
      <td>87</td>
      <td>297.25</td>
      <td>1189</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F20</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9</td>
      <td>2</td>
      <td>10550</td>
      <td>21100</td>
      <td>1</td>
      <td>2</td>
      <td>249</td>
      <td>498</td>
      <td>5049</td>
      <td>10098</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12</td>
      <td>16</td>
      <td>14325.6</td>
      <td>229210</td>
      <td>1</td>
      <td>16</td>
      <td>144.938</td>
      <td>2319</td>
      <td>4187.25</td>
      <td>66996</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>263094</td>
      <td>1</td>
      <td>10000</td>
      <td>10000</td>
      <td>1</td>
      <td>1</td>
      <td>66</td>
      <td>66</td>
      <td>513</td>
      <td>513</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>263095</td>
      <td>2</td>
      <td>122000</td>
      <td>244000</td>
      <td>1</td>
      <td>2</td>
      <td>220.5</td>
      <td>441</td>
      <td>1828</td>
      <td>3656</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>263096</td>
      <td>3</td>
      <td>28500</td>
      <td>85500</td>
      <td>1</td>
      <td>3</td>
      <td>256</td>
      <td>768</td>
      <td>4237</td>
      <td>12711</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>263102</td>
      <td>1</td>
      <td>1080</td>
      <td>1080</td>
      <td>1</td>
      <td>1</td>
      <td>188</td>
      <td>188</td>
      <td>1812</td>
      <td>1812</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>263103</td>
      <td>7</td>
      <td>58628.6</td>
      <td>410400</td>
      <td>1</td>
      <td>7</td>
      <td>280</td>
      <td>1960</td>
      <td>3249.43</td>
      <td>22746</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 1060 columns</p>
</div>




```python
data_concat.to_csv('./train_preprocessed.csv', index=False)
data_concat = pd.read_csv('./train_preprocessed.csv')
```


```python
lookup_table_clac1 = []
for i in range(37):
    # clac1_nm_number 이 1 이상인 것들(즉 검색 횟수가 1번 이상인 것들) 라벨 별(6차원) value count => 라벨별 분류 별 개수 파악
    prob = data_concat[data_concat['clac1_nm_' + str(i)] >= 1].groupby('label')['label'].value_counts().values
    if len(prob) != 6:
        print(i, prob)
    lookup_table_clac1.append(prob)
```


```python
lookup_table_clac1 = np.stack(lookup_table_clac1)
```

- 가중치 스케일링 진행 


```python
weight = np.array([59892/17727, 59892/59892, 59892/51936, 59892/2588, 59892/7953, 59892/9904])
weight
```




    array([ 3.37857506,  1.        ,  1.15318854, 23.14219474,  7.53074312,
            6.04725363])




```python
lookup_table_clac1 = lookup_table_clac1 * weight
```

- 가중치를 곱해준 후 열별 합으로 나누어 확률을 계산


```python
lookup_table_clac1 = lookup_table_clac1 / lookup_table_clac1.sum(1).reshape(-1, 1)
```


```python
lookup_table_clac1
```




    array([[0.1196019 , 0.15351727, 0.20977359, 0.12166872, 0.17288697,
            0.22255155],
           [0.10090549, 0.22714818, 0.27848079, 0.07186492, 0.14375322,
            0.1778474 ],
           [0.10631361, 0.2387599 , 0.22268107, 0.0900885 , 0.15848887,
            0.18366804],
           [0.29039753, 0.19554586, 0.18492875, 0.11494716, 0.10232984,
            0.11185087],
           [0.12042409, 0.17029651, 0.21638294, 0.08393391, 0.18585456,
            0.223108  ],
           [0.13488386, 0.23263313, 0.22147735, 0.09935707, 0.14373666,
            0.16791193],
           [0.11902816, 0.13211609, 0.16521248, 0.21401495, 0.17611567,
            0.19351265],
           [0.16240187, 0.19808438, 0.20762432, 0.1376104 , 0.14092544,
            0.1533536 ],
           [0.07940145, 0.22461032, 0.27383925, 0.05948637, 0.1576259 ,
            0.2050367 ],
           [0.03879911, 0.36162852, 0.26285742, 0.01136482, 0.1382575 ,
            0.18709264],
           [0.08528045, 0.2160113 , 0.33747026, 0.03723632, 0.11662744,
            0.20737422],
           [0.09358846, 0.18638675, 0.24438927, 0.09532372, 0.16983156,
            0.21048024],
           [0.09206891, 0.20384676, 0.25386732, 0.06491919, 0.1679978 ,
            0.21730001],
           [0.10342799, 0.33434196, 0.14533046, 0.04690085, 0.23776732,
            0.13223142],
           [0.08571612, 0.13047683, 0.22820432, 0.03355022, 0.18014104,
            0.34191147],
           [0.17141221, 0.1971363 , 0.22892891, 0.09959467, 0.12696112,
            0.17596679],
           [0.06222505, 0.08604227, 0.23216874, 0.12038342, 0.13869779,
            0.36048272],
           [0.22324304, 0.15132509, 0.21897612, 0.11280584, 0.12399265,
            0.16965727],
           [0.09232229, 0.15829274, 0.22398933, 0.08735568, 0.20083988,
            0.23720008],
           [0.15518083, 0.23827301, 0.35983805, 0.03133611, 0.06798093,
            0.14739108],
           [0.07570349, 0.11247887, 0.15227928, 0.22193144, 0.22689   ,
            0.21071693],
           [0.08260026, 0.11442791, 0.23484849, 0.12832265, 0.18221544,
            0.25758525],
           [0.06496974, 0.11974998, 0.31650811, 0.05057075, 0.11519424,
            0.33300719],
           [0.05506505, 0.13947588, 0.20192598, 0.08704121, 0.21007141,
            0.30642047],
           [0.1021726 , 0.16848741, 0.26515277, 0.05554371, 0.17351595,
            0.23512756],
           [0.1075171 , 0.16963273, 0.19924108, 0.13591302, 0.18616776,
            0.20152831],
           [0.05438879, 0.28590838, 0.15388338, 0.03694675, 0.26049635,
            0.20837634],
           [0.17495297, 0.19306772, 0.23370193, 0.10849474, 0.12998832,
            0.15979432],
           [0.15856251, 0.12967989, 0.15168173, 0.17149013, 0.19066678,
            0.19791896],
           [0.08792574, 0.08498125, 0.12449074, 0.2230608 , 0.22501851,
            0.25452295],
           [0.14436981, 0.09515721, 0.10600739, 0.26868961, 0.19830566,
            0.18747033],
           [0.07729805, 0.18127121, 0.22229937, 0.11675443, 0.1661101 ,
            0.23626684],
           [0.08819849, 0.1800361 , 0.29377603, 0.08332861, 0.14235959,
            0.21230117],
           [0.15508448, 0.12116081, 0.10063624, 0.3026265 , 0.17685887,
            0.1436331 ],
           [0.07460569, 0.13838052, 0.27962674, 0.08760444, 0.14253759,
            0.27724502],
           [0.05972663, 0.06089107, 0.07248401, 0.18182619, 0.3993866 ,
            0.22568551],
           [0.10097066, 0.11264563, 0.11576269, 0.10640274, 0.30008079,
            0.26413749]])



- 룩업 테이블 중 6차원이 되지 못하는 것들 결측치 보완해 6차원 만들어주기


```python
lookup_table_clac2 = []
for i in range(128):
    prob = data_concat[data_concat['clac2_nm_' + str(i)] >= 1].groupby('label')['label'].value_counts().values
    if len(prob) != 6:
        print(i, prob)
    lookup_table_clac2.append(prob)
```

    42 [133 607 646  27  64]
    52 [  42 1780 1083   87  134]
    86 [  18 1216 1003   58  147]
    111 [  7 605 370  34  39]
    120 [ 21  90 124  21  35]
    122 [ 20 759 471  47  63]
    127 [ 1  9 10  4  4]



```python
data_concat[data_concat['clac2_nm_' + str(86)] >= 1].groupby('label')['label'].value_counts()
```




    label  label
    F20    F20        18
    F30    F30      1216
    F40    F40      1003
    M30    M30        58
    M40    M40       147
    Name: label, dtype: int64




```python
lookup_table_clac2[13] = np.array([56, 1238, 504, 0, 58, 71])
lookup_table_clac2[42] = np.array([109, 519, 550, 0, 21, 52])
lookup_table_clac2[52] = np.array([23, 1036, 685, 0, 52, 75])
lookup_table_clac2[86] = np.array([9, 724, 644, 0, 39, 90])
lookup_table_clac2[111] = np.array([5, 420, 261, 0, 28, 29])
lookup_table_clac2[120] = np.array([18, 88, 121, 0, 20, 35])
lookup_table_clac2[122] = np.array([18, 535, 349, 0, 37, 44])
lookup_table_clac2[127] = np.array([1, 9, 10, 0, 4, 4])
```


```python
lookup_table_clac2 = np.stack(lookup_table_clac2)
```


```python
lookup_table_clac2
```




    array([[ 167,  791, 1075,   19,  100,  193],
           [  71,  454,  706,    7,   45,   86],
           [ 776, 5888, 4762,   96,  519,  749],
           [2576, 8291, 7464,  175,  595,  777],
           [ 347, 1245, 1276,   18,   86,  152],
           [ 324, 1687, 1673,   36,  257,  343],
           [5173, 9270, 6888,  160,  339,  613],
           [1158, 4240, 4284,   28,  160,  401],
           [ 741, 2125, 2624,  134,  373,  439],
           [  75, 3239, 1694,    3,  149,  233],
           [  59, 1537,  578,    2,   64,   92],
           [1046, 4946, 5185,   47,  276,  482],
           [  96,  887, 1001,   10,   78,  143],
           [  56, 1238,  504,    0,   58,   71],
           [ 181, 1942, 1187,   25,  169,  182],
           [  72,  697, 1094,    4,   55,  134],
           [  89,  535,  702,   15,   80,  124],
           [ 103,  800,  845,    8,   78,  125],
           [ 131, 1686,  846,    4,  131,  122],
           [  18,   97,  162,    2,   16,   47],
           [ 158,  654,  554,   11,   54,   69],
           [ 755, 2113, 2934,  400,  772,  842],
           [ 131,  612, 1432,   37,  131,  424],
           [ 201,  251,  301,   49,   74,   67],
           [ 916, 2915, 3431,  280,  636,  756],
           [ 368,  931, 1218,   27,  112,  194],
           [ 176, 1073, 1191,   32,  210,  298],
           [ 911, 2243, 2646,   68,  156,  355],
           [ 110,  482,  658,    8,   53,  123],
           [ 175,  708,  719,   72,  210,  221],
           [ 168, 1059, 1204,   15,   78,  149],
           [ 366, 1225, 1554,   78,  255,  298],
           [ 243, 1047, 1350,  120,  344,  378],
           [1158, 6383, 9230,   33,  236,  723],
           [ 849, 4607, 5286,   22,  140,  366],
           [  74,  865,  282,    7,   65,   37],
           [1409, 6914, 7984,  580, 1790, 1975],
           [ 441, 2995, 3678,  284,  873, 1065],
           [ 321, 1590, 2651,    5,   46,  180],
           [  97,  454,  808,   22,   96,  169],
           [ 111, 3567, 1951,    5,  172,  266],
           [  55, 2762, 1891,    2,  131,  256],
           [ 109,  519,  550,    0,   21,   52],
           [ 178, 2684, 1138,    9,  255,  218],
           [ 364, 1916, 2233,   38,  280,  433],
           [  44,  274,  628,    5,   35,  126],
           [ 292, 1630, 1980,  112,  459,  821],
           [ 142, 3073, 1538,   13,  230,  242],
           [ 411, 1656, 2061,   48,  143,  316],
           [  35,  400,  377,   11,   97,  113],
           [ 114,  779, 1238,   24,  151,  362],
           [ 694, 1750, 2069,   50,   97,  181],
           [  23, 1036,  685,    0,   52,   75],
           [ 272,  558,  652,   19,   51,   73],
           [ 113,  860,  953,   25,  104,  189],
           [  88,  533, 1047,   41,  170,  330],
           [  63,  351,  479,    5,   48,   81],
           [  65,  268,  345,   11,   31,   52],
           [  41,  211,  255,   12,   36,   58],
           [ 242, 2458,  559,    9,  197,   84],
           [  94,  743,  162,   11,   93,   44],
           [ 159, 4015, 2016,    7,  249,  309],
           [ 735, 3917, 5331,   19,  146,  415],
           [  41,  891,  421,    4,   88,  109],
           [ 243,  906,  951,   22,   81,  124],
           [ 144,  956, 1367,    8,   50,  133],
           [  47,  352,  766,   19,  118,  336],
           [  40,  325, 1111,    9,   14,  102],
           [  38,  177,  247,   12,   38,   45],
           [  59,  166,  144,   10,   27,   25],
           [  54, 1965, 1470,    2,  103,  201],
           [  85,  963, 1099,    8,   63,  129],
           [  16,  114,  171,    2,   10,   14],
           [  60,  698,  801,    4,   59,  106],
           [ 163,  325,  412,   31,   35,   58],
           [ 615, 2143, 1953,   38,  170,  242],
           [ 141,  411,  560,    9,   50,   89],
           [  46,  499,  776,   10,   71,  143],
           [  65,  746,  664,   13,   70,  111],
           [  15,   35,   62,    6,   24,   21],
           [  75,  262,  376,   10,   46,   70],
           [ 347,  754,  724,   92,  199,  238],
           [ 201, 1383, 1930,   21,  109,  228],
           [ 160, 1410, 1481,   38,  167,  294],
           [  43,  244,  282,    6,   28,   65],
           [  66,  363,  419,    5,   44,   66],
           [   9,  724,  644,    0,   39,   90],
           [  32, 2532, 1868,    1,  123,  247],
           [  29,  200,  283,    4,   21,   39],
           [ 310,  515,  605,   38,   43,   71],
           [ 109,  829,  951,   15,   96,  168],
           [  19,  793,  461,    3,  104,  129],
           [ 195, 1401, 1539,   28,  154,  234],
           [ 128,  843, 1487,   17,   79,  201],
           [ 289, 1027, 1218,   40,  105,  164],
           [ 153,  496,  618,   56,  166,  244],
           [  28, 1128,  780,    5,   77,  102],
           [  55,  333,  370,    8,   55,   73],
           [ 244, 1383, 1493,   19,  122,  221],
           [ 505, 1467, 1173,    8,   79,  147],
           [  69,  734,  269,    6,   89,   53],
           [  48,  580,  152,   10,   74,   29],
           [ 172,  454,  327,   49,   88,   89],
           [ 103,  558,  765,   69,  232,  300],
           [ 142,  430,  516,   26,   70,  108],
           [ 549, 1127, 1011,   71,  136,  160],
           [ 112, 1159, 1165,   11,  107,  163],
           [ 501, 1016, 1116,   50,   78,  132],
           [ 178,  303,  447,   43,   84,   95],
           [  39,  249,  331,   19,   71,   77],
           [  59,  281,  228,   11,   50,   68],
           [   5,  420,  261,    0,   28,   29],
           [ 125,  429,  592,   14,   50,   85],
           [  25,  370,   82,    1,   40,   31],
           [   9,  206,  115,    1,   27,   25],
           [ 133,  455,  568,    6,   59,   68],
           [  79,  454,  739,   11,   50,  131],
           [   9,   31,   32,    4,   27,   19],
           [  22, 1116,  821,    1,   65,  122],
           [  27,  213,  463,    8,   41,   96],
           [  18,   88,  121,    0,   20,   35],
           [  11,   45,   49,    6,   23,   23],
           [  18,  535,  349,    0,   37,   44],
           [  11,   69,   94,   25,   83,   99],
           [  30,  107,   71,    5,   37,   34],
           [   9,   40,   60,    2,   15,   23],
           [  17,   44,   70,    2,   14,   28],
           [   1,    9,   10,    0,    4,    4]], dtype=int64)




```python
lookup_table_clac2 = lookup_table_clac2 * weight
```


```python
lookup_table_clac2 = lookup_table_clac2 / lookup_table_clac2.sum(1).reshape(-1, 1)
```


```python
lookup_table_clac2
```




    array([[0.11387393, 0.15964331, 0.25019754, 0.08874265, 0.15198897,
            0.2355536 ],
           [0.09485229, 0.17951954, 0.32192959, 0.0640558 , 0.13400044,
            0.20564233],
           [0.10631361, 0.2387599 , 0.22268107, 0.0900885 , 0.15848887,
            0.18366804],
           [0.22413044, 0.21351497, 0.22166308, 0.10429512, 0.11539214,
            0.12100425],
           [0.19964605, 0.21201521, 0.25058131, 0.07093731, 0.11028945,
            0.15653067],
           [0.11457987, 0.17658134, 0.20194169, 0.08720407, 0.20258192,
            0.2171111 ],
           [0.39140263, 0.20760004, 0.17788575, 0.08292247, 0.05717224,
            0.08301687],
           [0.22523187, 0.24409201, 0.28440517, 0.03730356, 0.06936582,
            0.13960156],
           [0.15435505, 0.1310171 , 0.1865663 , 0.19119582, 0.17318717,
            0.16367855],
           [0.03149144, 0.40253967, 0.24277919, 0.00862827, 0.13945107,
            0.17511036],
           [0.05715761, 0.4407196 , 0.19112463, 0.01327159, 0.13819945,
            0.15952712],
           [0.1720522 , 0.2407959 , 0.29110124, 0.05295383, 0.101191  ,
            0.14190582],
           [0.08009934, 0.21905226, 0.28507459, 0.05715163, 0.14506296,
            0.21355922],
           [0.06581918, 0.4306768 , 0.20219094, 0.        , 0.15194859,
            0.14936449],
           [0.08895892, 0.28250529, 0.1991262 , 0.08416314, 0.1851407 ,
            0.16010575],
           [0.06912809, 0.19807116, 0.35851399, 0.02630589, 0.1177034 ,
            0.23027747],
           [0.08990184, 0.15995535, 0.24203737, 0.10378648, 0.18012451,
            0.22419445],
           [0.09531764, 0.21912527, 0.26690671, 0.0507104 , 0.16089217,
            0.20704782],
           [0.08993877, 0.34260968, 0.19824979, 0.01881077, 0.20047083,
            0.14992017],
           [0.07643565, 0.12191626, 0.23480385, 0.0581734 , 0.15144248,
            0.35722836],
           [0.18374676, 0.22511622, 0.21990703, 0.08762464, 0.13997827,
            0.14362707],
           [0.09042372, 0.07490337, 0.11993952, 0.32814546, 0.20609004,
            0.18049789],
           [0.06222505, 0.08604227, 0.23216874, 0.12038342, 0.13869779,
            0.36048272],
           [0.20129571, 0.07440097, 0.10288965, 0.33612863, 0.16518646,
            0.12009858],
           [0.11991786, 0.11295186, 0.15331189, 0.25108305, 0.18558794,
            0.1771474 ],
           [0.19987876, 0.14967006, 0.22580464, 0.10045084, 0.1355942 ,
            0.18860151],
           [0.08298891, 0.14975232, 0.19168402, 0.10335425, 0.22071455,
            0.25150596],
           [0.23198729, 0.16906026, 0.22998653, 0.1186112 , 0.08854717,
            0.16180755],
           [0.1263869 , 0.16391656, 0.2580489 , 0.06296081, 0.13573427,
            0.25295257],
           [0.08808163, 0.10547438, 0.12352161, 0.24822801, 0.2355976 ,
            0.19909676],
           [0.1170163 , 0.218323  , 0.28624001, 0.07156478, 0.12109772,
            0.18575819],
           [0.12642292, 0.12524121, 0.18321562, 0.18454841, 0.19633113,
            0.18424071],
           [0.07410828, 0.09450909, 0.1405274 , 0.25067597, 0.23384236,
            0.2063369 ],
           [0.14046852, 0.22917209, 0.38215442, 0.02741924, 0.0638097 ,
            0.15697603],
           [0.16534633, 0.26556541, 0.35138302, 0.02934814, 0.06077419,
            0.12758293],
           [0.10797639, 0.37357656, 0.14044715, 0.06996262, 0.21140473,
            0.09663255],
           [0.07970245, 0.11575946, 0.15415158, 0.22472928, 0.22569295,
            0.19996427],
           [0.0526235 , 0.1057802 , 0.14980269, 0.23212956, 0.23219863,
            0.22746541],
           [0.14892673, 0.21833893, 0.4198016 , 0.01588944, 0.04756962,
            0.14947368],
           [0.08260026, 0.11442791, 0.23484849, 0.12832265, 0.18221544,
            0.25758525],
           [0.04071252, 0.38723499, 0.24424691, 0.01256163, 0.14061698,
            0.17462697],
           [0.02410322, 0.35826348, 0.28285946, 0.00600362, 0.12796406,
            0.20080615],
           [0.18467518, 0.26026503, 0.31806176, 0.        , 0.07930592,
            0.15769212],
           [0.0747562 , 0.33363849, 0.16313089, 0.02589051, 0.23871057,
            0.16387334],
           [0.10856926, 0.16914822, 0.22733221, 0.07763545, 0.18615204,
            0.23116283],
           [0.06496974, 0.11974998, 0.31650811, 0.05057075, 0.11519424,
            0.33300719],
           [0.06199536, 0.10243075, 0.14348559, 0.16287909, 0.21721674,
            0.31199247],
           [0.05437754, 0.34830542, 0.20102697, 0.03409931, 0.19631946,
            0.16587129],
           [0.14586122, 0.17395013, 0.24965642, 0.11668371, 0.11311972,
            0.20072881],
           [0.04510974, 0.15259091, 0.16584804, 0.09711044, 0.27866232,
            0.26067856],
           [0.05949789, 0.1203374 , 0.22053835, 0.08579835, 0.17566204,
            0.33816597],
           [0.24778347, 0.18493424, 0.25213904, 0.12227955, 0.07719494,
            0.11566875],
           [0.02826967, 0.37689392, 0.28737585, 0.        , 0.1424625 ,
            0.16499806],
           [0.26300913, 0.15969913, 0.21518711, 0.12584225, 0.10991991,
            0.12634248],
           [0.07879122, 0.17748607, 0.22680835, 0.11940166, 0.16163559,
            0.23587711],
           [0.0474765 , 0.08511178, 0.19280108, 0.15151333, 0.20443216,
            0.31866516],
           [0.1021726 , 0.16848741, 0.26515277, 0.05554371, 0.17351595,
            0.23512756],
           [0.13010442, 0.15877419, 0.23570268, 0.15081424, 0.13830715,
            0.18629733],
           [0.08976614, 0.13673433, 0.19056169, 0.17996204, 0.17568531,
            0.2272905 ],
           [0.13359609, 0.40163051, 0.10533118, 0.03403234, 0.24240908,
            0.0830008 ],
           [0.12866043, 0.30100409, 0.0756831 , 0.103129  , 0.28372942,
            0.10779396],
           [0.0498196 , 0.37235319, 0.21560577, 0.01502353, 0.17390285,
            0.17329506],
           [0.14962327, 0.23601076, 0.37041386, 0.02649332, 0.06624741,
            0.15121139],
           [0.04728605, 0.30415382, 0.1657288 , 0.03159949, 0.22622265,
            0.22500919],
           [0.17495297, 0.19306772, 0.23370193, 0.10849474, 0.12998832,
            0.15979432],
           [0.11095275, 0.21802178, 0.35950987, 0.04222178, 0.08587165,
            0.18342217],
           [0.03339958, 0.07403758, 0.18579699, 0.09248424, 0.18690866,
            0.42737296],
           [0.05058002, 0.12163786, 0.47951234, 0.07795293, 0.03945947,
            0.23085738],
           [0.09001799, 0.12410389, 0.19971441, 0.19471433, 0.20064741,
            0.19080196],
           [0.17840406, 0.14856867, 0.14862161, 0.20712079, 0.18197878,
            0.13530609],
           [0.03102731, 0.33417918, 0.28829326, 0.00787139, 0.1319143 ,
            0.20671456],
           [0.07257117, 0.24335369, 0.32026513, 0.04678495, 0.11989195,
            0.1971331 ],
           [0.09458732, 0.19947304, 0.34504503, 0.08098673, 0.13177019,
            0.14813769],
           [0.06751951, 0.23248765, 0.30766444, 0.03083252, 0.14799066,
            0.21350523],
           [0.20529292, 0.12115355, 0.17711295, 0.26743549, 0.09825591,
            0.13074918],
           [0.20580528, 0.21226089, 0.22307473, 0.08710357, 0.12680447,
            0.14495105],
           [0.17934696, 0.15473307, 0.24312503, 0.07841306, 0.14175852,
            0.20262336],
           [0.04887015, 0.1569108 , 0.28139368, 0.07277075, 0.16813126,
            0.27192336],
           [0.0679779 , 0.23091898, 0.2370223 , 0.09312552, 0.16317615,
            0.20777915],
           [0.08393841, 0.05797009, 0.11842078, 0.22998087, 0.29935395,
            0.2103359 ],
           [0.12993614, 0.13434961, 0.22234291, 0.11866965, 0.17763592,
            0.21706577],
           [0.14976144, 0.09631819, 0.10665368, 0.27197522, 0.19143788,
            0.18385359],
           [0.09738401, 0.19832625, 0.31916528, 0.06969183, 0.11771244,
            0.1977202 ],
           [0.07137796, 0.18617857, 0.22551008, 0.11611778, 0.16605994,
            0.23475567],
           [0.09969284, 0.16743713, 0.22315745, 0.09528351, 0.14469643,
            0.26973264],
           [0.11642021, 0.18952107, 0.25226977, 0.06041231, 0.17299812,
            0.20837854],
           [0.01302228, 0.31006259, 0.31805117, 0.        , 0.12578048,
            0.23308348],
           [0.01493836, 0.34985104, 0.29764368, 0.0031976 , 0.12798599,
            0.20638333],
           [0.08819849, 0.1800361 , 0.29377603, 0.08332861, 0.14235959,
            0.21230117],
           [0.26906271, 0.13230172, 0.17923134, 0.22591569, 0.08318874,
            0.11029981],
           [0.08407927, 0.18927071, 0.25038581, 0.07925464, 0.16505852,
            0.23195105],
           [0.02124516, 0.26244961, 0.17594381, 0.02297728, 0.25920532,
            0.25817883],
           [0.09335259, 0.19851637, 0.25147634, 0.09181651, 0.16432996,
            0.20050824],
           [0.0832595 , 0.16229975, 0.3301426 , 0.07574322, 0.11453948,
            0.23401545],
           [0.15964406, 0.16791589, 0.22965133, 0.15135121, 0.1292851 ,
            0.1621524 ],
           [0.08994345, 0.08630307, 0.12400333, 0.2254951 , 0.21751563,
            0.25673942],
           [0.02754419, 0.3284335 , 0.26189865, 0.03369092, 0.16883672,
            0.17959602],
           [0.09355262, 0.16765014, 0.21481357, 0.09320822, 0.20852599,
            0.22224946],
           [0.12445275, 0.20878692, 0.25992092, 0.06638031, 0.13870074,
            0.20175837],
           [0.27541769, 0.23680834, 0.21835604, 0.02988556, 0.0960355 ,
            0.14349686],
           [0.09685464, 0.30495365, 0.12888144, 0.05768907, 0.27846179,
            0.1331594 ],
           [0.08619164, 0.30826082, 0.09316102, 0.1229971 , 0.29618284,
            0.09320657],
           [0.15508448, 0.12116081, 0.10063624, 0.3026265 , 0.17685887,
            0.1436331 ],
           [0.05009762, 0.08033051, 0.12700127, 0.22987934, 0.25151977,
            0.26117148],
           [0.14596695, 0.13082811, 0.18104337, 0.18306719, 0.16038675,
            0.19870763],
           [0.23833294, 0.14481117, 0.14980614, 0.21112567, 0.1315997 ,
            0.12432438],
           [0.07680261, 0.23523819, 0.27267834, 0.051668  , 0.16354829,
            0.20006457],
           [0.25888198, 0.15539042, 0.1968317 , 0.17697221, 0.0898386 ,
            0.12208509],
           [0.16603492, 0.08365434, 0.14231599, 0.27473808, 0.17464774,
            0.15860893],
           [0.05982513, 0.11305372, 0.17330608, 0.1996382 , 0.24276254,
            0.21141432],
           [0.11163667, 0.15737206, 0.1472504 , 0.14256684, 0.21087697,
            0.23029706],
           [0.01502783, 0.37363016, 0.26775246, 0.        , 0.18758085,
            0.1560087 ],
           [0.15365239, 0.15608207, 0.24838065, 0.11787679, 0.13699464,
            0.18701345],
           [0.07961858, 0.34877278, 0.08913639, 0.02181451, 0.28394791,
            0.17670984],
           [0.04072331, 0.27588887, 0.17760906, 0.03099356, 0.27231311,
            0.20247208],
           [0.17595768, 0.17816993, 0.25649072, 0.05437244, 0.17398542,
            0.16102381],
           [0.08907588, 0.15151488, 0.28440957, 0.08495651, 0.12566296,
            0.2643802 ],
           [0.05972663, 0.06089107, 0.07248401, 0.18182619, 0.3993866 ,
            0.22568551],
           [0.02194203, 0.32944631, 0.27948849, 0.00683164, 0.14450126,
            0.21779027],
           [0.04769548, 0.11136776, 0.27916515, 0.09679979, 0.16143644,
            0.30353537],
           [0.09347154, 0.13525583, 0.21446628, 0.        , 0.23149475,
            0.3253116 ],
           [0.06300986, 0.07629477, 0.09580289, 0.23541711, 0.2936621 ,
            0.23581328],
           [0.03941322, 0.34672855, 0.26083241, 0.        , 0.18058238,
            0.17244344],
           [0.01842693, 0.03421179, 0.05374708, 0.28686082, 0.30991501,
            0.29683838],
           [0.11386041, 0.12019923, 0.09197644, 0.12998477, 0.31300946,
            0.2309697 ],
           [0.06943374, 0.09133862, 0.15799597, 0.10568881, 0.25794288,
            0.31759998],
           [0.11414176, 0.08744093, 0.16042071, 0.09198068, 0.20952118,
            0.33649474],
           [0.04319189, 0.11505649, 0.14742425, 0.        , 0.38509371,
            0.30923367]])




```python
lookup_table_clac3 = np.zeros((883, 6))
# 888*6의 zero nd array를 생성 initialize

for i in range(883):
    try:
        prob = data_concat[data_concat['clac3_nm_' + str(i)] >= 1].groupby('label')['label'].value_counts()
        for j in range(len(prob.keys())):
            if prob.keys()[j][0] == 'F20':
                lookup_table_clac3[i, 0] = prob.values[j]
            elif prob.keys()[j][0] == 'F30':
                lookup_table_clac3[i, 1] = prob.values[j]
            elif prob.keys()[j][0] == 'F40':
                lookup_table_clac3[i, 2] = prob.values[j]
            elif prob.keys()[j][0] == 'M20':
                lookup_table_clac3[i, 3] = prob.values[j]
            elif prob.keys()[j][0] == 'M30':
                lookup_table_clac3[i, 4] = prob.values[j]
            elif prob.keys()[j][0] == 'M40':
                lookup_table_clac3[i, 5] = prob.values[j]
    except:
        lookup_table_clac3[i] = 1/weight
```
lookup_table_clac3

```python

```


```python
lookup_table_clac3 = lookup_table_clac3 * weight
```


```python
lookup_table_clac3 = lookup_table_clac3 / lookup_table_clac3.sum(1).reshape(-1, 1)
```


```python
lookup_table_clac3
```




    array([[0.13835664, 0.14548446, 0.28831758, 0.07481844, 0.13796521,
            0.21505768],
           [0.07972002, 0.21727755, 0.33786185, 0.        , 0.13327016,
            0.23187043],
           [0.09684827, 0.1934031 , 0.2083384 , 0.1637977 , 0.13858443,
            0.1990281 ],
           ...,
           [0.        , 0.30244584, 0.69755416, 0.        , 0.        ,
            0.        ],
           [0.        , 0.        , 0.        , 0.        , 1.        ,
            0.        ],
           [0.        , 0.        , 1.        , 0.        , 0.        ,
            0.        ]])




```python
mat_clac1 = np.array(data_concat.iloc[:, 11:48]).dot(lookup_table_clac1)
mat_clac1 = mat_clac1 / mat_clac1.sum(1).reshape(-1, 1)
```


```python
mat_clac1 = pd.DataFrame(mat_clac1)
mat_clac1 = mat_clac1.rename(columns={0: 'given_clac1_F20_prob', 1:'given_clac1_F30_prob', 2:'given_clac1_F40_prob', 3:'given_clac1_M20_prob', 4:'given_clac1_M30_prob', 5:'given_clac1_M40_prob'})
```

- 본격 룩업 테이블에서 벡터 뽑아내는 코드


```python
mat_buy_clac1 = np.where(np.array(data_concat.iloc[:, 11:48]) >= 1, 1, 0).dot(lookup_table_clac1)
# np.where(조건, True return value, False return value)
# 원 핫 매트릭스와 확률이 담긴 룩업 테이블을 내적

mat_buy_clac1 = mat_buy_clac1 / mat_buy_clac1.sum(1).reshape(-1, 1)
# 열 별로 합치고 reshape 한 것과 나눈다
```


```python
mat_buy_clac1 = pd.DataFrame(mat_buy_clac1)
mat_buy_clac1 = mat_buy_clac1.rename(columns={0: 'buy_clac1_F20_prob', 1:'buy_clac1_F30_prob', 2:'buy_clac1_F40_prob', 3:'buy_clac1_M20_prob', 4:'buy_clac1_M30_prob', 5:'buy_clac1_M40_prob'})
```


```python
mat_clac2 = np.array(data_concat.iloc[:, 48:-884]).dot(lookup_table_clac2)
mat_clac2 = mat_clac2 / mat_clac2.sum(1).reshape(-1, 1)
```


```python
mat_clac2 = pd.DataFrame(mat_clac2)
mat_clac2 = mat_clac2.rename(columns={0: 'given_clac2_F20_prob', 1:'given_clac2_F30_prob', 2:'given_clac2_F40_prob', 3:'given_clac2_M20_prob', 4:'given_clac2_M30_prob', 5:'given_clac2_M40_prob'})
```


```python
mat_buy_clac2 = np.where(np.array(data_concat.iloc[:, 48:-884]) >= 1, 1, 0).dot(lookup_table_clac2)
mat_buy_clac2 = mat_buy_clac2 / mat_buy_clac2.sum(1).reshape(-1, 1)
```


```python
mat_buy_clac2 = pd.DataFrame(mat_buy_clac2)
mat_buy_clac2 = mat_buy_clac2.rename(columns={0: 'buy_clac2_F20_prob', 1:'buy_clac2_F30_prob', 2:'buy_clac2_F40_prob', 3:'buy_clac2_M20_prob', 4:'buy_clac2_M30_prob', 5:'buy_clac2_M40_prob'})
```


```python
mat_clac3 = np.array(data_concat.iloc[:, -884:-1]).dot(lookup_table_clac3)
mat_clac3 = mat_clac3 / mat_clac3.sum(1).reshape(-1, 1)
```


```python
mat_clac3 = pd.DataFrame(mat_clac3)
mat_clac3 = mat_clac3.rename(columns={0: 'given_clac3_F20_prob', 1:'given_clac3_F30_prob', 2:'given_clac3_F40_prob', 3:'given_clac3_M20_prob', 4:'given_clac3_M30_prob', 5:'given_clac3_M40_prob'})
```


```python
mat_buy_clac3 = np.where(np.array(data_concat.iloc[:, -884:-1]) >= 1, 1, 0).dot(lookup_table_clac3)
mat_buy_clac3 = mat_buy_clac3 / mat_buy_clac3.sum(1).reshape(-1, 1)
```


```python
mat_buy_clac3 = pd.DataFrame(mat_buy_clac3)
mat_buy_clac3 = mat_buy_clac3.rename(columns={0: 'buy_clac3_F20_prob', 1:'buy_clac3_F30_prob', 2:'buy_clac3_F40_prob', 3:'buy_clac3_M20_prob', 4:'buy_clac3_M30_prob', 5:'buy_clac3_M40_prob'})
```

clac1_embed = np.load('C:/Users/user/Desktop/DHCN-main/datasets/clac1_nm/embedding.npy')

train_clac1_mean = np.array(data_concat.iloc[:, 11:48]).dot(clac1_embed)
train_clac1_mean = train_clac1_mean / train_clac1_mean.sum(1).reshape(-1, 1)

train_clac1_weight = np.where(np.array(data_concat.iloc[:, 11:48]) >= 1, 1, 0).dot(clac1_embed)
train_clac1_weight = train_clac1_weight / train_clac1_weight.sum(1).reshape(-1, 1)

train_clac1_mean = pd.DataFrame(train_clac1_mean)
train_clac1_weight = pd.DataFrame(train_clac1_weight)
train_clac1_mean.columns = ['clac1_mean_'+str(i) for i in range(5)]
train_clac1_weight.columns = ['clac1_weight_'+str(i) for i in range(5)]

clac2_embed = np.load('C:/Users/user/Desktop/DHCN-main/datasets/clac2_nm/embedding.npy')

train_clac2_mean = np.array(data_concat.iloc[:, 48:-884]).dot(clac2_embed)
train_clac2_mean = train_clac2_mean / train_clac2_mean.sum(1).reshape(-1, 1)

train_clac2_weight = np.where(np.array(data_concat.iloc[:, 48:-884]) >= 1, 1, 0).dot(clac2_embed)
train_clac2_weight = train_clac2_weight / train_clac2_weight.sum(1).reshape(-1, 1)

train_clac2_mean = pd.DataFrame(train_clac2_mean)
train_clac2_weight = pd.DataFrame(train_clac2_weight)
train_clac2_mean.columns = ['clac2_mean_'+str(i) for i in range(10)]
train_clac2_weight.columns = ['clac2_weight_'+str(i) for i in range(10)]

clac3_embed = np.load('C:/Users/user/Desktop/DHCN-main/datasets/clac3_nm/embedding.npy')

train_clac3_mean = np.array(data_concat.iloc[:, -884:-1]).dot(clac3_embed[:883])
train_clac3_mean = train_clac3_mean / train_clac3_mean.sum(1).reshape(-1, 1)

train_clac3_weight = np.where(np.array(data_concat.iloc[:, -884:-1]) >= 1, 1, 0).dot(clac3_embed[:883])
train_clac3_weight = train_clac3_weight / train_clac3_weight.sum(1).reshape(-1, 1)

train_clac3_mean = pd.DataFrame(train_clac3_mean)
train_clac3_weight = pd.DataFrame(train_clac3_weight)
train_clac3_mean.columns = ['clac3_mean_'+str(i) for i in range(30)]
train_clac3_weight.columns = ['clac3_weight_'+str(i) for i in range(30)]


```python
data_concat['gender'] = data_concat['label'].map(lambda x: x[:1])
data_concat['age'] = data_concat['label'].map(lambda x: x[1:])
```


```python
data_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clnt_id</th>
      <th>num_shopping</th>
      <th>avg_price</th>
      <th>total_price</th>
      <th>avg_ct</th>
      <th>total_ct</th>
      <th>avg_sess_view</th>
      <th>total_sess_view</th>
      <th>avg_sess_hr</th>
      <th>total_sess_hr</th>
      <th>...</th>
      <th>clac3_nm_876</th>
      <th>clac3_nm_877</th>
      <th>clac3_nm_878</th>
      <th>clac3_nm_879</th>
      <th>clac3_nm_880</th>
      <th>clac3_nm_881</th>
      <th>clac3_nm_882</th>
      <th>label</th>
      <th>gender</th>
      <th>age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>2</td>
      <td>43250.000000</td>
      <td>86500</td>
      <td>1.0</td>
      <td>2</td>
      <td>59.000000</td>
      <td>118.0</td>
      <td>922.000000</td>
      <td>1844</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F20</td>
      <td>F</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>9</td>
      <td>77777.777778</td>
      <td>700000</td>
      <td>1.0</td>
      <td>9</td>
      <td>132.333333</td>
      <td>1191.0</td>
      <td>1311.111111</td>
      <td>11800</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
      <td>F</td>
      <td>30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>4</td>
      <td>24225.000000</td>
      <td>96900</td>
      <td>1.0</td>
      <td>4</td>
      <td>21.750000</td>
      <td>87.0</td>
      <td>297.250000</td>
      <td>1189</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F20</td>
      <td>F</td>
      <td>20</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9</td>
      <td>2</td>
      <td>10550.000000</td>
      <td>21100</td>
      <td>1.0</td>
      <td>2</td>
      <td>249.000000</td>
      <td>498.0</td>
      <td>5049.000000</td>
      <td>10098</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
      <td>F</td>
      <td>30</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12</td>
      <td>16</td>
      <td>14325.625000</td>
      <td>229210</td>
      <td>1.0</td>
      <td>16</td>
      <td>144.937500</td>
      <td>2319.0</td>
      <td>4187.250000</td>
      <td>66996</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
      <td>F</td>
      <td>30</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>263094</td>
      <td>1</td>
      <td>10000.000000</td>
      <td>10000</td>
      <td>1.0</td>
      <td>1</td>
      <td>66.000000</td>
      <td>66.0</td>
      <td>513.000000</td>
      <td>513</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
      <td>F</td>
      <td>30</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>263095</td>
      <td>2</td>
      <td>122000.000000</td>
      <td>244000</td>
      <td>1.0</td>
      <td>2</td>
      <td>220.500000</td>
      <td>441.0</td>
      <td>1828.000000</td>
      <td>3656</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
      <td>F</td>
      <td>30</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>263096</td>
      <td>3</td>
      <td>28500.000000</td>
      <td>85500</td>
      <td>1.0</td>
      <td>3</td>
      <td>256.000000</td>
      <td>768.0</td>
      <td>4237.000000</td>
      <td>12711</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
      <td>F</td>
      <td>30</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>263102</td>
      <td>1</td>
      <td>1080.000000</td>
      <td>1080</td>
      <td>1.0</td>
      <td>1</td>
      <td>188.000000</td>
      <td>188.0</td>
      <td>1812.000000</td>
      <td>1812</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
      <td>F</td>
      <td>30</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>263103</td>
      <td>7</td>
      <td>58628.571429</td>
      <td>410400</td>
      <td>1.0</td>
      <td>7</td>
      <td>280.000000</td>
      <td>1960.0</td>
      <td>3249.428571</td>
      <td>22746</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>F30</td>
      <td>F</td>
      <td>30</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 1062 columns</p>
</div>




```python
label_dic = {}
for i, label in enumerate(['F20', 'F30', 'F40', 'M20', 'M30', 'M40']):
    label_dic[label] = i
```


```python
label_dic
```




    {'F20': 0, 'F30': 1, 'F40': 2, 'M20': 3, 'M30': 4, 'M40': 5}




```python
gender_dic = {}
for i, gender in enumerate(['F', 'M']):
    gender_dic[gender] = i
```


```python
gender_dic
```




    {'F': 0, 'M': 1}




```python
age_dic = {}
for i, age in enumerate(['20', '30', '40']):
    age_dic[age] = i
```


```python
age_dic
```




    {'20': 0, '30': 1, '40': 2}




```python
data_concat['label'] = data_concat['label'].map(lambda x: label_dic[x])
data_concat['gender'] = data_concat['gender'].map(lambda x: gender_dic[x])
data_concat['age'] = data_concat['age'].map(lambda x: age_dic[x])
```


```python
(np.array(mat_clac1).argmax(1) == data_concat['label'].values).sum()  # 50456
```




    52069




```python
(np.array(mat_buy_clac1).argmax(1) == data_concat['label'].values).sum()  # 51031
```




    52940




```python
(np.array(mat_clac2).argmax(1) == data_concat['label'].values).sum()  # 55530
```




    57561




```python
(np.array(mat_buy_clac2).argmax(1) == data_concat['label'].values).sum()  # 55932
```




    58263




```python
(np.array(mat_clac3).argmax(1) == data_concat['label'].values).sum()  # 56845
```




    58976




```python
(np.array(mat_buy_clac3).argmax(1) == data_concat['label'].values).sum()  # 57160
```




    59451



### word2vec


```python
from gensim.models import word2vec
```


```python
def oversample(x, n):
    lst = []
    for i in x:
        tmp = []
        for j in range(n):
            random.shuffle(i)
            tmp += list(i)
        lst.append(tmp)
    return lst
```

#### Product code Word2vec


```python
df['PD_C'] = df['PD_C'].astype('string')
df_test['PD_C'] = df_test['PD_C'].astype('string')
```


```python
train_pd_w2v = list(df.groupby('CLNT_ID')['PD_C'].unique()[y['CLNT_ID'].values])
test_pd_w2v = list(df_test.groupby('CLNT_ID')['PD_C'].unique()[df_test['CLNT_ID'].unique()])
```


```python
tt = df.groupby('CLNT_ID')['PD_C'].value_counts()[y['CLNT_ID'].values]
kk = y['CLNT_ID'].values
ww = df.groupby(['CLNT_ID', 'PD_C'])['KWD_NM'].count()
```


```python
train_pd_w2v.extend(test_pd_w2v)
```


```python
pd_w2v_input = oversample(train_pd_w2v, 10)
```


```python
# product code word2vec
pd_w2v = word2vec.Word2Vec(sentences=pd_w2v_input,
                           size=256,
                           window=3,
                           min_count=1,
                           sg=1).wv
```


```python
pd_mean_vector = []

for words in tqdm(train_pd_w2v[:-len(test_pd_w2v)]):
        tmp = np.zeros(256) 
        cnt = 0
        for word in words:
            try:
                tmp += pd_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        pd_mean_vector.append(tmp)
        
pd_mean_vector = np.array(pd_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:05<00:00, 28873.49it/s]



```python
pd_unweight_vector = []

for i, words in tqdm(enumerate(train_pd_w2v[:-len(test_pd_w2v)])):
        tmp = np.zeros(256) 
        cnt = 0
        for word in words:
            try:
                tmp += pd_w2v[word] * tt[kk[i]][word]
                cnt += tt[kk[i]][word]
            except:
                pass
        #tmp /= cnt 
        pd_unweight_vector.append(tmp)
        
pd_unweight_vector = np.array(pd_unweight_vector)
```

    150000it [04:30, 553.55it/s]



```python
pd_weight_vector = []

for i, words in tqdm(enumerate(train_pd_w2v[:-len(test_pd_w2v)])):
        tmp = np.zeros(256) 
        cnt = 0
        for word in words:
            try:
                tmp += pd_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        pd_weight_vector.append(tmp)
        
pd_weight_vector = np.array(pd_weight_vector)
```

    150000it [12:04, 207.08it/s]



```python
train_pd_mean = pd_mean_vector
train_pd_unweight = pd_unweight_vector
train_pd_weight = pd_weight_vector

train_pd_mean = pd.DataFrame(train_pd_mean)
train_pd_unweight = pd.DataFrame(train_pd_unweight)
train_pd_weight = pd.DataFrame(train_pd_weight)

train_pd_mean.columns = ['pd_mean_'+str(i) for i in range(256)]
train_pd_unweight.columns = ['pd_unweight_'+str(i) for i in range(256)]
train_pd_weight.columns = ['pd_weight_'+str(i) for i in range(256)]
```


```python
df['PD_BRA_NM'] = df['PD_BRA_NM'].astype('string')
df_test['PD_BRA_NM'] = df_test['PD_BRA_NM'].astype('string')
```

#### brand name word2vec


```python
train_bra_w2v = list(df.groupby('CLNT_ID')['PD_BRA_NM'].unique()[y['CLNT_ID'].values])
test_bra_w2v = list(df_test.groupby('CLNT_ID')['PD_BRA_NM'].unique()[df_test['CLNT_ID'].unique()])
```


```python
tt = df.groupby('CLNT_ID')['PD_BRA_NM'].value_counts()[y['CLNT_ID'].values]
## tt : kk에 해당하는 product name count 값
kk = y['CLNT_ID'].values
## kk : client id list
ww = df.groupby(['CLNT_ID', 'PD_BRA_NM'])['KWD_NM'].count()
## client id, brand name로 그룹핑한 keyword들마다의 개수
```


```python
train_bra_w2v.extend(test_bra_w2v)
```


```python
bra_w2v_input = oversample(train_bra_w2v, 10)
```


```python
bra_w2v = word2vec.Word2Vec(sentences=bra_w2v_input,
                           size=128,
                           window=3,
                           min_count=1,
                           sg=1).wv
```


```python
bra_mean_vector = []

for words in tqdm(train_bra_w2v[:-len(test_bra_w2v)]):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += bra_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        bra_mean_vector.append(tmp)
        
bra_mean_vector = np.array(bra_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:03<00:00, 40477.85it/s]



```python
bra_unweight_vector = [] 

for i, words in tqdm(enumerate(train_bra_w2v[:-len(test_bra_w2v)])):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += bra_w2v[word] * tt[kk[i]][word]
                cnt += tt[kk[i]][word]
            except:
                pass
        #tmp /= cnt 
        bra_unweight_vector.append(tmp)
        
bra_unweight_vector = np.array(bra_unweight_vector)
```

    150000it [03:17, 758.58it/s]



```python
bra_weight_vector = [] 

for i, words in tqdm(enumerate(train_bra_w2v[:-len(test_bra_w2v)])):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += bra_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        bra_weight_vector.append(tmp)
        
bra_weight_vector = np.array(bra_weight_vector)
```

    150000it [08:57, 278.88it/s]



```python
train_bra_mean = bra_mean_vector
train_bra_unweight = bra_unweight_vector
train_bra_weight = bra_weight_vector

train_bra_mean = pd.DataFrame(train_bra_mean)
train_bra_unweight = pd.DataFrame(train_bra_unweight)
train_bra_weight = pd.DataFrame(train_bra_weight)

train_bra_mean.columns = ['bra_mean_'+str(i) for i in range(128)]
train_bra_unweight.columns = ['bra_unweight_'+str(i) for i in range(128)]
train_bra_weight.columns = ['bra_weight_'+str(i) for i in range(128)]
```

#### keyword_name word2vec


```python
df['KWD_NM'] = df['KWD_NM'].astype('string')
df_test['KWD_NM'] = df_test['KWD_NM'].astype('string')
```


```python
train_kwd_w2v = list(df.groupby('CLNT_ID')['KWD_NM'].unique()[y['CLNT_ID'].values])
test_kwd_w2v = list(df_test.groupby('CLNT_ID')['KWD_NM'].unique()[df_test['CLNT_ID'].unique()])
```


```python
tt = df.groupby('CLNT_ID')['KWD_NM'].value_counts()[y['CLNT_ID'].values]
kk = y['CLNT_ID'].values
ww = df.groupby(['CLNT_ID', 'KWD_NM'])['KWD_NM'].count()
```


```python
train_kwd_w2v.extend(test_kwd_w2v)
```


```python
kwd_w2v_input = oversample(train_kwd_w2v, 10)
```


```python
kwd_w2v = word2vec.Word2Vec(sentences=kwd_w2v_input,
                           size=128,
                           window=3,
                           min_count=1,
                           sg=1).wv
```


```python
kwd_mean_vector = []

for words in tqdm(train_kwd_w2v[:-len(test_kwd_w2v)]):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += kwd_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        kwd_mean_vector.append(tmp)
        
kwd_mean_vector = np.array(kwd_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:06<00:00, 23661.92it/s]



```python
kwd_unweight_vector = []

for i, words in tqdm(enumerate(train_kwd_w2v[:-len(test_kwd_w2v)])):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += kwd_w2v[word] * tt[kk[i]][word]
                cnt += tt[kk[i]][word]
            except:
                pass
        #tmp /= cnt 
        kwd_unweight_vector.append(tmp)
        
kwd_unweight_vector = np.array(kwd_unweight_vector)
```

    150000it [05:42, 437.61it/s]



```python
kwd_weight_vector = []

for i, words in tqdm(enumerate(train_kwd_w2v[:-len(test_kwd_w2v)])):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += kwd_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        kwd_weight_vector.append(tmp)
        
kwd_weight_vector = np.array(kwd_weight_vector)
```

    150000it [15:38, 159.76it/s]



```python
train_kwd_mean = kwd_mean_vector
train_kwd_unweight = kwd_unweight_vector
train_kwd_weight = kwd_weight_vector

train_kwd_mean = pd.DataFrame(train_kwd_mean)
train_kwd_unweight = pd.DataFrame(train_kwd_unweight)
train_kwd_weight = pd.DataFrame(train_kwd_weight)

train_kwd_mean.columns = ['kwd_mean_'+str(i) for i in range(128)]
train_kwd_weight.columns = ['kwd_unweight_'+str(i) for i in range(128)]
train_kwd_weight.columns = ['kwd_weight_'+str(i) for i in range(128)]
```

#### CLAC3_NM word2vec


```python
df['CLAC3_NM'] = df['CLAC3_NM'].astype('string')
df_test['CLAC3_NM'] = df_test['CLAC3_NM'].astype('string')
```


```python
train_clac3_w2v = list(df.groupby('CLNT_ID')['CLAC3_NM'].unique()[y['CLNT_ID'].values])
test_clac3_w2v = list(df_test.groupby('CLNT_ID')['CLAC3_NM'].unique()[df_test['CLNT_ID'].unique()])
```


```python
tt = df.groupby('CLNT_ID')['CLAC3_NM'].value_counts()[y['CLNT_ID'].values]
kk = y['CLNT_ID'].values
ww = df.groupby(['CLNT_ID', 'CLAC3_NM'])['KWD_NM'].count()
```


```python
train_clac3_w2v.extend(test_clac3_w2v)
```


```python
clac3_w2v_input = oversample(train_clac3_w2v, 5)
```


```python
clac3_w2v = word2vec.Word2Vec(sentences=clac3_w2v_input,
                              size=30,
                              window=3,
                              min_count=1,
                              sg=1).wv
```


```python
clac3_mean_vector = []

for words in tqdm(train_clac3_w2v[:-len(test_clac3_w2v)]):
        tmp = np.zeros(30) 
        cnt = 0
        for word in words:
            try:
                tmp += clac3_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        clac3_mean_vector.append(tmp)
        
clac3_mean_vector = np.array(clac3_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:03<00:00, 40631.44it/s]



```python
clac3_unweight_vector = []

for i, words in tqdm(enumerate(train_clac3_w2v[:-len(test_clac3_w2v)])):
        tmp = np.zeros(30) 
        cnt = 0
        for word in words:
            try:
                tmp += clac3_w2v[word] * clac3_matrix[i, clac3_nm_dic[word]]
                cnt += clac3_matrix[i, clac3_nm_dic[word]]
            except:
                pass
        #tmp /= cnt 
        clac3_unweight_vector.append(tmp)
        
clac3_unweight_vector = np.array(clac3_unweight_vector)
```

    150000it [00:05, 26368.42it/s]



```python
clac3_weight_vector = []

for i, words in tqdm(enumerate(train_clac3_w2v[:-len(test_clac3_w2v)])):
        tmp = np.zeros(30) 
        cnt = 0
        for word in words:
            try:
                tmp += clac3_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        clac3_weight_vector.append(tmp)
        
clac3_weight_vector = np.array(clac3_weight_vector)
```

    150000it [08:58, 278.79it/s]



```python
train_clac3_mean = clac3_mean_vector
train_clac3_unweight = clac3_unweight_vector
train_clac3_weight = clac3_weight_vector

train_clac3_mean = pd.DataFrame(train_clac3_mean)
train_clac3_unweight = pd.DataFrame(train_clac3_unweight)
train_clac3_weight = pd.DataFrame(train_clac3_weight)

train_clac3_mean.columns = ['clac3_mean_'+str(i) for i in range(30)]
train_clac3_unweight.columns = ['clac3_unweight_'+str(i) for i in range(30)]
train_clac3_weight.columns = ['clac3_weight_'+str(i) for i in range(30)]
```

#### CLAC2_NM word2vec


```python
df['CLAC2_NM'] = df['CLAC2_NM'].astype('string')
df_test['CLAC2_NM'] = df_test['CLAC2_NM'].astype('string')
```


```python
train_clac2_w2v = list(df.groupby('CLNT_ID')['CLAC2_NM'].unique()[y['CLNT_ID'].values])
test_clac2_w2v = list(df_test.groupby('CLNT_ID')['CLAC2_NM'].unique()[df_test['CLNT_ID'].unique()])
```


```python
tt = df.groupby('CLNT_ID')['CLAC2_NM'].value_counts()[y['CLNT_ID'].values]
kk = y['CLNT_ID'].values
ww = df.groupby(['CLNT_ID', 'CLAC2_NM'])['KWD_NM'].count()
```


```python
train_clac2_w2v.extend(test_clac2_w2v)
```


```python
clac2_w2v_input = oversample(train_clac2_w2v, 5)
```


```python
clac2_w2v = word2vec.Word2Vec(sentences=clac2_w2v_input,
                              size=10,
                              window=3,
                              min_count=1,
                              sg=1).wv
```


```python
clac2_mean_vector = []

for words in tqdm(train_clac2_w2v[:-len(test_clac2_w2v)]):
        tmp = np.zeros(10) 
        cnt = 0
        for word in words:
            try:
                tmp += clac2_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt 
        clac2_mean_vector.append(tmp)
        
clac2_mean_vector = np.array(clac2_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:03<00:00, 45694.59it/s]



```python
clac2_unweight_vector = []

for i, words in tqdm(enumerate(train_clac2_w2v[:-len(test_clac2_w2v)])):
        tmp = np.zeros(10) 
        cnt = 0
        for word in words:
            try:
                tmp += clac2_w2v[word] * clac2_matrix[i, clac2_nm_dic[word]]
                cnt += clac2_matrix[i, clac2_nm_dic[word]]
            except:
                pass
        #tmp /= cnt 
        clac2_unweight_vector.append(tmp)
        
clac2_unweight_vector = np.array(clac2_unweight_vector)
```

    150000it [00:04, 31135.69it/s]



```python
clac2_weight_vector = []

for i, words in tqdm(enumerate(train_clac2_w2v[:-len(test_clac2_w2v)])):
        tmp = np.zeros(10) 
        cnt = 0
        for word in words:
            try:
                tmp += clac2_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        clac2_weight_vector.append(tmp)
        
clac2_weight_vector = np.array(clac2_weight_vector)
```

    150000it [07:40, 325.76it/s]



```python
train_clac2_mean = clac2_mean_vector
train_clac2_unweight = clac2_unweight_vector
train_clac2_weight = clac2_weight_vector

train_clac2_mean = pd.DataFrame(train_clac2_mean)
train_clac2_unweight = pd.DataFrame(train_clac2_unweight)
train_clac2_weight = pd.DataFrame(train_clac2_weight)

train_clac2_mean.columns = ['clac2_mean_'+str(i) for i in range(10)]
train_clac2_unweight.columns = ['clac2_unweight_'+str(i) for i in range(10)]
train_clac2_weight.columns = ['clac2_weight_'+str(i) for i in range(10)]
```

#### CLAC1_NM word2vec


```python
df['CLAC1_NM'] = df['CLAC1_NM'].astype('string')
df_test['CLAC1_NM'] = df_test['CLAC1_NM'].astype('string')
```


```python
train_clac1_w2v = list(df.groupby('CLNT_ID')['CLAC1_NM'].unique()[y['CLNT_ID'].values])
test_clac1_w2v = list(df_test.groupby('CLNT_ID')['CLAC1_NM'].unique()[df_test['CLNT_ID'].unique()])
```


```python
tt = df.groupby('CLNT_ID')['CLAC1_NM'].value_counts()[y['CLNT_ID'].values]
kk = y['CLNT_ID'].values
ww = df.groupby(['CLNT_ID', 'CLAC1_NM'])['KWD_NM'].count()
```


```python
train_clac1_w2v.extend(test_clac1_w2v)
```


```python
clac1_w2v_input = oversample(train_clac1_w2v, 5)
```


```python
clac1_w2v = word2vec.Word2Vec(sentences=clac1_w2v_input,
                              size=5,
                              window=3,
                              min_count=1,
                              sg=1).wv
```


```python
clac1_mean_vector = []

for words in tqdm(train_clac1_w2v[:-len(test_clac1_w2v)]):
        tmp = np.zeros(5) 
        cnt = 0
        for word in words:
            try:
                tmp += clac1_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt 
        clac1_mean_vector.append(tmp)
        
clac1_mean_vector = np.array(clac1_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:02<00:00, 56621.56it/s]



```python
clac1_unweight_vector = []

for i, words in tqdm(enumerate(train_clac1_w2v[:-len(test_clac1_w2v)])):
        tmp = np.zeros(5) 
        cnt = 0
        for word in words:
            try:
                tmp += clac1_w2v[word] * clac1_matrix[i, clac1_nm_dic[word]]
                cnt += clac1_matrix[i, clac1_nm_dic[word]]
            except:
                pass
        #tmp /= cnt 
        clac1_unweight_vector.append(tmp)
        
clac1_unweight_vector = np.array(clac1_unweight_vector)
```

    150000it [00:03, 38266.09it/s]



```python
clac1_weight_vector = []

for i, words in tqdm(enumerate(train_clac1_w2v[:-len(test_clac1_w2v)])):
        tmp = np.zeros(5) 
        cnt = 0
        for word in words:
            try:
                tmp += clac1_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        clac1_weight_vector.append(tmp)
        
clac1_weight_vector = np.array(clac1_weight_vector)
```

    150000it [05:59, 416.81it/s]



```python
train_clac1_mean = clac1_mean_vector
train_clac1_unweight = clac1_unweight_vector
train_clac1_weight = clac1_weight_vector

train_clac1_mean = pd.DataFrame(train_clac1_mean)
train_clac1_unweight = pd.DataFrame(train_clac1_unweight)
train_clac1_weight = pd.DataFrame(train_clac1_weight)

train_clac1_mean.columns = ['clac1_mean_'+str(i) for i in range(5)]
train_clac1_unweight.columns = ['clac1_unweight_'+str(i) for i in range(5)]
train_clac1_weight.columns = ['clac1_weight_'+str(i) for i in range(5)]
```


```python
data_concat = pd.concat([data_concat.iloc[:, 1:11], mat_clac1, mat_buy_clac1, mat_clac2, mat_buy_clac2, mat_clac3, mat_buy_clac3, train_pd_mean, train_bra_mean, train_kwd_mean, train_clac1_mean, train_clac2_mean, train_clac3_mean, train_pd_unweight, train_bra_unweight, train_kwd_unweight, train_clac1_unweight, train_clac2_unweight, train_clac3_unweight, train_pd_weight, train_bra_weight, train_kwd_weight, train_clac1_weight, train_clac2_weight, train_clac3_weight, data_concat.iloc[:, -3:]], axis=1)
```


```python
data_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_shopping</th>
      <th>avg_price</th>
      <th>total_price</th>
      <th>avg_ct</th>
      <th>total_ct</th>
      <th>avg_sess_view</th>
      <th>total_sess_view</th>
      <th>avg_sess_hr</th>
      <th>total_sess_hr</th>
      <th>avg_shopping_interval</th>
      <th>...</th>
      <th>clac3_weight_23</th>
      <th>clac3_weight_24</th>
      <th>clac3_weight_25</th>
      <th>clac3_weight_26</th>
      <th>clac3_weight_27</th>
      <th>clac3_weight_28</th>
      <th>clac3_weight_29</th>
      <th>label</th>
      <th>gender</th>
      <th>age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>43250.000000</td>
      <td>86500</td>
      <td>1.0</td>
      <td>2</td>
      <td>59.000000</td>
      <td>118.0</td>
      <td>922.000000</td>
      <td>1844</td>
      <td>0.000000</td>
      <td>...</td>
      <td>-0.025476</td>
      <td>-0.358678</td>
      <td>-0.186819</td>
      <td>0.192818</td>
      <td>0.434122</td>
      <td>-0.107545</td>
      <td>0.063859</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9</td>
      <td>77777.777778</td>
      <td>700000</td>
      <td>1.0</td>
      <td>9</td>
      <td>132.333333</td>
      <td>1191.0</td>
      <td>1311.111111</td>
      <td>11800</td>
      <td>3.625000</td>
      <td>...</td>
      <td>-0.513720</td>
      <td>0.286228</td>
      <td>0.180560</td>
      <td>0.383307</td>
      <td>0.273483</td>
      <td>0.445723</td>
      <td>-0.301021</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>24225.000000</td>
      <td>96900</td>
      <td>1.0</td>
      <td>4</td>
      <td>21.750000</td>
      <td>87.0</td>
      <td>297.250000</td>
      <td>1189</td>
      <td>5.666667</td>
      <td>...</td>
      <td>-0.164428</td>
      <td>0.047147</td>
      <td>0.298240</td>
      <td>-0.305354</td>
      <td>0.558061</td>
      <td>0.195794</td>
      <td>0.031537</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>10550.000000</td>
      <td>21100</td>
      <td>1.0</td>
      <td>2</td>
      <td>249.000000</td>
      <td>498.0</td>
      <td>5049.000000</td>
      <td>10098</td>
      <td>0.000000</td>
      <td>...</td>
      <td>-0.527290</td>
      <td>-0.260950</td>
      <td>0.230153</td>
      <td>-0.477711</td>
      <td>-0.060666</td>
      <td>-0.118800</td>
      <td>-0.559288</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>16</td>
      <td>14325.625000</td>
      <td>229210</td>
      <td>1.0</td>
      <td>16</td>
      <td>144.937500</td>
      <td>2319.0</td>
      <td>4187.250000</td>
      <td>66996</td>
      <td>4.200000</td>
      <td>...</td>
      <td>-0.257740</td>
      <td>-0.554550</td>
      <td>-0.216515</td>
      <td>-0.312947</td>
      <td>-0.201764</td>
      <td>0.072622</td>
      <td>-0.378338</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>1</td>
      <td>10000.000000</td>
      <td>10000</td>
      <td>1.0</td>
      <td>1</td>
      <td>66.000000</td>
      <td>66.0</td>
      <td>513.000000</td>
      <td>513</td>
      <td>183.000000</td>
      <td>...</td>
      <td>-0.105944</td>
      <td>-1.130829</td>
      <td>0.387124</td>
      <td>0.286016</td>
      <td>0.126334</td>
      <td>0.426996</td>
      <td>0.029120</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>2</td>
      <td>122000.000000</td>
      <td>244000</td>
      <td>1.0</td>
      <td>2</td>
      <td>220.500000</td>
      <td>441.0</td>
      <td>1828.000000</td>
      <td>3656</td>
      <td>83.000000</td>
      <td>...</td>
      <td>-0.719438</td>
      <td>-0.272537</td>
      <td>-0.384826</td>
      <td>-0.215291</td>
      <td>-0.045238</td>
      <td>-0.005138</td>
      <td>-0.127866</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>3</td>
      <td>28500.000000</td>
      <td>85500</td>
      <td>1.0</td>
      <td>3</td>
      <td>256.000000</td>
      <td>768.0</td>
      <td>4237.000000</td>
      <td>12711</td>
      <td>0.000000</td>
      <td>...</td>
      <td>-0.675275</td>
      <td>-0.445863</td>
      <td>-0.125119</td>
      <td>0.457581</td>
      <td>0.359894</td>
      <td>0.512207</td>
      <td>-0.033372</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>1</td>
      <td>1080.000000</td>
      <td>1080</td>
      <td>1.0</td>
      <td>1</td>
      <td>188.000000</td>
      <td>188.0</td>
      <td>1812.000000</td>
      <td>1812</td>
      <td>183.000000</td>
      <td>...</td>
      <td>0.113508</td>
      <td>0.063089</td>
      <td>0.235095</td>
      <td>0.160695</td>
      <td>0.016483</td>
      <td>0.776254</td>
      <td>-0.536355</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>7</td>
      <td>58628.571429</td>
      <td>410400</td>
      <td>1.0</td>
      <td>7</td>
      <td>280.000000</td>
      <td>1960.0</td>
      <td>3249.428571</td>
      <td>22746</td>
      <td>4.666667</td>
      <td>...</td>
      <td>-0.240615</td>
      <td>0.007781</td>
      <td>-0.318051</td>
      <td>0.228597</td>
      <td>0.180339</td>
      <td>0.140052</td>
      <td>-0.201356</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 1720 columns</p>
</div>




```python
data_concat = data_concat.loc[:, list(data_concat.columns[:-3].values) + ['gender', 'age', 'label']]
```


```python
data_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_shopping</th>
      <th>avg_price</th>
      <th>total_price</th>
      <th>avg_ct</th>
      <th>total_ct</th>
      <th>avg_sess_view</th>
      <th>total_sess_view</th>
      <th>avg_sess_hr</th>
      <th>total_sess_hr</th>
      <th>avg_shopping_interval</th>
      <th>...</th>
      <th>clac3_weight_23</th>
      <th>clac3_weight_24</th>
      <th>clac3_weight_25</th>
      <th>clac3_weight_26</th>
      <th>clac3_weight_27</th>
      <th>clac3_weight_28</th>
      <th>clac3_weight_29</th>
      <th>gender</th>
      <th>age</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>43250.000000</td>
      <td>86500</td>
      <td>1.0</td>
      <td>2</td>
      <td>59.000000</td>
      <td>118.0</td>
      <td>922.000000</td>
      <td>1844</td>
      <td>0.000000</td>
      <td>...</td>
      <td>-0.025476</td>
      <td>-0.358678</td>
      <td>-0.186819</td>
      <td>0.192818</td>
      <td>0.434122</td>
      <td>-0.107545</td>
      <td>0.063859</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9</td>
      <td>77777.777778</td>
      <td>700000</td>
      <td>1.0</td>
      <td>9</td>
      <td>132.333333</td>
      <td>1191.0</td>
      <td>1311.111111</td>
      <td>11800</td>
      <td>3.625000</td>
      <td>...</td>
      <td>-0.513720</td>
      <td>0.286228</td>
      <td>0.180560</td>
      <td>0.383307</td>
      <td>0.273483</td>
      <td>0.445723</td>
      <td>-0.301021</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>24225.000000</td>
      <td>96900</td>
      <td>1.0</td>
      <td>4</td>
      <td>21.750000</td>
      <td>87.0</td>
      <td>297.250000</td>
      <td>1189</td>
      <td>5.666667</td>
      <td>...</td>
      <td>-0.164428</td>
      <td>0.047147</td>
      <td>0.298240</td>
      <td>-0.305354</td>
      <td>0.558061</td>
      <td>0.195794</td>
      <td>0.031537</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>10550.000000</td>
      <td>21100</td>
      <td>1.0</td>
      <td>2</td>
      <td>249.000000</td>
      <td>498.0</td>
      <td>5049.000000</td>
      <td>10098</td>
      <td>0.000000</td>
      <td>...</td>
      <td>-0.527290</td>
      <td>-0.260950</td>
      <td>0.230153</td>
      <td>-0.477711</td>
      <td>-0.060666</td>
      <td>-0.118800</td>
      <td>-0.559288</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>16</td>
      <td>14325.625000</td>
      <td>229210</td>
      <td>1.0</td>
      <td>16</td>
      <td>144.937500</td>
      <td>2319.0</td>
      <td>4187.250000</td>
      <td>66996</td>
      <td>4.200000</td>
      <td>...</td>
      <td>-0.257740</td>
      <td>-0.554550</td>
      <td>-0.216515</td>
      <td>-0.312947</td>
      <td>-0.201764</td>
      <td>0.072622</td>
      <td>-0.378338</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>1</td>
      <td>10000.000000</td>
      <td>10000</td>
      <td>1.0</td>
      <td>1</td>
      <td>66.000000</td>
      <td>66.0</td>
      <td>513.000000</td>
      <td>513</td>
      <td>183.000000</td>
      <td>...</td>
      <td>-0.105944</td>
      <td>-1.130829</td>
      <td>0.387124</td>
      <td>0.286016</td>
      <td>0.126334</td>
      <td>0.426996</td>
      <td>0.029120</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>2</td>
      <td>122000.000000</td>
      <td>244000</td>
      <td>1.0</td>
      <td>2</td>
      <td>220.500000</td>
      <td>441.0</td>
      <td>1828.000000</td>
      <td>3656</td>
      <td>83.000000</td>
      <td>...</td>
      <td>-0.719438</td>
      <td>-0.272537</td>
      <td>-0.384826</td>
      <td>-0.215291</td>
      <td>-0.045238</td>
      <td>-0.005138</td>
      <td>-0.127866</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>3</td>
      <td>28500.000000</td>
      <td>85500</td>
      <td>1.0</td>
      <td>3</td>
      <td>256.000000</td>
      <td>768.0</td>
      <td>4237.000000</td>
      <td>12711</td>
      <td>0.000000</td>
      <td>...</td>
      <td>-0.675275</td>
      <td>-0.445863</td>
      <td>-0.125119</td>
      <td>0.457581</td>
      <td>0.359894</td>
      <td>0.512207</td>
      <td>-0.033372</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>1</td>
      <td>1080.000000</td>
      <td>1080</td>
      <td>1.0</td>
      <td>1</td>
      <td>188.000000</td>
      <td>188.0</td>
      <td>1812.000000</td>
      <td>1812</td>
      <td>183.000000</td>
      <td>...</td>
      <td>0.113508</td>
      <td>0.063089</td>
      <td>0.235095</td>
      <td>0.160695</td>
      <td>0.016483</td>
      <td>0.776254</td>
      <td>-0.536355</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>7</td>
      <td>58628.571429</td>
      <td>410400</td>
      <td>1.0</td>
      <td>7</td>
      <td>280.000000</td>
      <td>1960.0</td>
      <td>3249.428571</td>
      <td>22746</td>
      <td>4.666667</td>
      <td>...</td>
      <td>-0.240615</td>
      <td>0.007781</td>
      <td>-0.318051</td>
      <td>0.228597</td>
      <td>0.180339</td>
      <td>0.140052</td>
      <td>-0.201356</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>150000 rows × 1720 columns</p>
</div>




```python
data_concat.to_csv('train.csv', index=False)
```


```python
import pandas as pd
import numpy as np

data_concat = pd.read_csv('train.csv')
train_stat = pd.read_csv('stat_df.csv')
```


```python
data_concat = pd.concat([train_stat.iloc[:, 1:], data_concat.iloc[:, 10:]], axis=1)
```

# train


```python
import tensorflow as tf
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
```


```python
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
```


```python
X_train, X_valid, y_train, y_valid = train_test_split(data_concat.iloc[:, :-1], data_concat.iloc[:, -1], test_size=0.2, stratify=data_concat.iloc[:, -1])
```


```python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.iloc[:, :-2])
X_valid_scaled = scaler.transform(X_valid.iloc[:, :-2])
```


```python
model1 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model1.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model1.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))  # 1.1643
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 14us/sample - loss: 1.2876 - accuracy: 0.4757 - val_loss: 1.1988 - val_accuracy: 0.5098
    Epoch 2/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.2033 - accuracy: 0.5061 - val_loss: 1.1858 - val_accuracy: 0.5216
    Epoch 3/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1860 - accuracy: 0.5135 - val_loss: 1.1775 - val_accuracy: 0.5181
    Epoch 4/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1728 - accuracy: 0.5189 - val_loss: 1.1708 - val_accuracy: 0.5203
    Epoch 5/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1614 - accuracy: 0.5215 - val_loss: 1.1714 - val_accuracy: 0.5197
    Epoch 6/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1547 - accuracy: 0.5244 - val_loss: 1.1686 - val_accuracy: 0.5238
    Epoch 7/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1477 - accuracy: 0.5273 - val_loss: 1.1698 - val_accuracy: 0.5240
    Epoch 8/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1396 - accuracy: 0.5285 - val_loss: 1.1671 - val_accuracy: 0.5233
    Epoch 9/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1311 - accuracy: 0.5331 - val_loss: 1.1669 - val_accuracy: 0.5263
    Epoch 10/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1241 - accuracy: 0.5363 - val_loss: 1.1680 - val_accuracy: 0.5226
    Epoch 11/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1168 - accuracy: 0.5394 - val_loss: 1.1749 - val_accuracy: 0.5262
    Epoch 12/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1100 - accuracy: 0.5412 - val_loss: 1.1693 - val_accuracy: 0.5222
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0998 - accuracy: 0.5455 - val_loss: 1.1746 - val_accuracy: 0.5241
    Epoch 14/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0928 - accuracy: 0.5491 - val_loss: 1.1764 - val_accuracy: 0.5183
    Epoch 15/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0833 - accuracy: 0.5515 - val_loss: 1.1762 - val_accuracy: 0.5215
    Epoch 16/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0752 - accuracy: 0.5545 - val_loss: 1.1845 - val_accuracy: 0.5182
    Epoch 17/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0647 - accuracy: 0.5591 - val_loss: 1.1855 - val_accuracy: 0.5202
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0572 - accuracy: 0.5630 - val_loss: 1.1898 - val_accuracy: 0.5177
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0477 - accuracy: 0.5668 - val_loss: 1.1938 - val_accuracy: 0.5160
    Epoch 20/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0414 - accuracy: 0.5679 - val_loss: 1.1938 - val_accuracy: 0.5190
    Epoch 21/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0305 - accuracy: 0.5741 - val_loss: 1.1996 - val_accuracy: 0.5205
    Epoch 22/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0219 - accuracy: 0.5776 - val_loss: 1.2094 - val_accuracy: 0.5138
    Epoch 23/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0132 - accuracy: 0.5825 - val_loss: 1.2100 - val_accuracy: 0.5175
    Epoch 24/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0028 - accuracy: 0.5836 - val_loss: 1.2150 - val_accuracy: 0.5166



```python
model2 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model2.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model2.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 20us/sample - loss: 1.2883 - accuracy: 0.4768 - val_loss: 1.1937 - val_accuracy: 0.5115
    Epoch 2/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.2037 - accuracy: 0.5047 - val_loss: 1.1856 - val_accuracy: 0.5169
    Epoch 3/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1870 - accuracy: 0.5116 - val_loss: 1.1714 - val_accuracy: 0.5220
    Epoch 4/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1753 - accuracy: 0.5175 - val_loss: 1.1733 - val_accuracy: 0.5183
    Epoch 5/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1671 - accuracy: 0.5186 - val_loss: 1.1650 - val_accuracy: 0.5235
    Epoch 6/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1574 - accuracy: 0.5222 - val_loss: 1.1639 - val_accuracy: 0.5215
    Epoch 7/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1482 - accuracy: 0.5269 - val_loss: 1.1697 - val_accuracy: 0.5242
    Epoch 8/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1407 - accuracy: 0.5287 - val_loss: 1.1659 - val_accuracy: 0.5224
    Epoch 9/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1326 - accuracy: 0.5319 - val_loss: 1.1650 - val_accuracy: 0.5239
    Epoch 10/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1299 - accuracy: 0.5336 - val_loss: 1.1712 - val_accuracy: 0.5198
    Epoch 11/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1207 - accuracy: 0.5364 - val_loss: 1.1685 - val_accuracy: 0.5249
    Epoch 12/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1113 - accuracy: 0.5391 - val_loss: 1.1700 - val_accuracy: 0.5238
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1016 - accuracy: 0.5449 - val_loss: 1.1721 - val_accuracy: 0.5237
    Epoch 14/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0938 - accuracy: 0.5469 - val_loss: 1.1760 - val_accuracy: 0.5224
    Epoch 15/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0920 - accuracy: 0.5491 - val_loss: 1.1775 - val_accuracy: 0.5231
    Epoch 16/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0815 - accuracy: 0.5513 - val_loss: 1.1788 - val_accuracy: 0.5232
    Epoch 17/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0699 - accuracy: 0.5575 - val_loss: 1.1816 - val_accuracy: 0.5170
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0605 - accuracy: 0.5599 - val_loss: 1.1851 - val_accuracy: 0.5183
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0544 - accuracy: 0.5630 - val_loss: 1.1870 - val_accuracy: 0.5181
    Epoch 20/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0445 - accuracy: 0.5677 - val_loss: 1.1968 - val_accuracy: 0.5166
    Epoch 21/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0313 - accuracy: 0.5734 - val_loss: 1.1980 - val_accuracy: 0.5177



```python
model3 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model3.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model3.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 13us/sample - loss: 1.2943 - accuracy: 0.4743 - val_loss: 1.1932 - val_accuracy: 0.5142
    Epoch 2/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.2040 - accuracy: 0.5047 - val_loss: 1.1846 - val_accuracy: 0.5208
    Epoch 3/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1854 - accuracy: 0.5118 - val_loss: 1.1800 - val_accuracy: 0.5165
    Epoch 4/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1731 - accuracy: 0.5170 - val_loss: 1.1680 - val_accuracy: 0.5226
    Epoch 5/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1612 - accuracy: 0.5207 - val_loss: 1.1703 - val_accuracy: 0.5201
    Epoch 6/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1582 - accuracy: 0.5248 - val_loss: 1.1647 - val_accuracy: 0.5209
    Epoch 7/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1482 - accuracy: 0.5284 - val_loss: 1.1669 - val_accuracy: 0.5240
    Epoch 8/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1471 - accuracy: 0.5282 - val_loss: 1.1637 - val_accuracy: 0.5226
    Epoch 9/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1350 - accuracy: 0.5323 - val_loss: 1.1623 - val_accuracy: 0.5271
    Epoch 10/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1264 - accuracy: 0.5356 - val_loss: 1.1682 - val_accuracy: 0.5209
    Epoch 11/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1188 - accuracy: 0.5386 - val_loss: 1.1714 - val_accuracy: 0.5229
    Epoch 12/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1104 - accuracy: 0.5401 - val_loss: 1.1712 - val_accuracy: 0.5230
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1029 - accuracy: 0.5439 - val_loss: 1.1687 - val_accuracy: 0.5228
    Epoch 14/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0960 - accuracy: 0.5459 - val_loss: 1.1703 - val_accuracy: 0.5251
    Epoch 15/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0880 - accuracy: 0.5498 - val_loss: 1.1787 - val_accuracy: 0.5218
    Epoch 16/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0809 - accuracy: 0.5530 - val_loss: 1.1783 - val_accuracy: 0.5242
    Epoch 17/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0695 - accuracy: 0.5559 - val_loss: 1.1844 - val_accuracy: 0.5206
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0642 - accuracy: 0.5592 - val_loss: 1.1886 - val_accuracy: 0.5204
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0524 - accuracy: 0.5621 - val_loss: 1.1909 - val_accuracy: 0.5228
    Epoch 20/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0445 - accuracy: 0.5690 - val_loss: 1.1981 - val_accuracy: 0.5181
    Epoch 21/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0343 - accuracy: 0.5729 - val_loss: 1.1950 - val_accuracy: 0.5169
    Epoch 22/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0262 - accuracy: 0.5761 - val_loss: 1.2008 - val_accuracy: 0.5169
    Epoch 23/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0172 - accuracy: 0.5792 - val_loss: 1.2079 - val_accuracy: 0.5141
    Epoch 24/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0089 - accuracy: 0.5843 - val_loss: 1.2081 - val_accuracy: 0.5143



```python
model4 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model4.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model4.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 13us/sample - loss: 1.2951 - accuracy: 0.4740 - val_loss: 1.1908 - val_accuracy: 0.5107
    Epoch 2/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.2020 - accuracy: 0.5061 - val_loss: 1.1779 - val_accuracy: 0.5170
    Epoch 3/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1874 - accuracy: 0.5119 - val_loss: 1.1815 - val_accuracy: 0.5165
    Epoch 4/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1766 - accuracy: 0.5186 - val_loss: 1.1692 - val_accuracy: 0.5212
    Epoch 5/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1630 - accuracy: 0.5203 - val_loss: 1.1670 - val_accuracy: 0.5247
    Epoch 6/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1538 - accuracy: 0.5250 - val_loss: 1.1682 - val_accuracy: 0.5216
    Epoch 7/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1473 - accuracy: 0.5275 - val_loss: 1.1663 - val_accuracy: 0.5242
    Epoch 8/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1396 - accuracy: 0.5300 - val_loss: 1.1675 - val_accuracy: 0.5204
    Epoch 9/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1312 - accuracy: 0.5339 - val_loss: 1.1672 - val_accuracy: 0.5237
    Epoch 10/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1266 - accuracy: 0.5357 - val_loss: 1.1663 - val_accuracy: 0.5231
    Epoch 11/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1186 - accuracy: 0.5383 - val_loss: 1.1713 - val_accuracy: 0.5235
    Epoch 12/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1114 - accuracy: 0.5422 - val_loss: 1.1750 - val_accuracy: 0.5208
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1013 - accuracy: 0.5446 - val_loss: 1.1702 - val_accuracy: 0.5206
    Epoch 14/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0930 - accuracy: 0.5503 - val_loss: 1.1706 - val_accuracy: 0.5206
    Epoch 15/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0846 - accuracy: 0.5520 - val_loss: 1.1801 - val_accuracy: 0.5181
    Epoch 16/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0779 - accuracy: 0.5547 - val_loss: 1.1785 - val_accuracy: 0.5214
    Epoch 17/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0696 - accuracy: 0.5565 - val_loss: 1.1873 - val_accuracy: 0.5112
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0579 - accuracy: 0.5625 - val_loss: 1.1875 - val_accuracy: 0.5191
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0497 - accuracy: 0.5654 - val_loss: 1.1907 - val_accuracy: 0.5200
    Epoch 20/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0421 - accuracy: 0.5694 - val_loss: 1.1976 - val_accuracy: 0.5152
    Epoch 21/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0335 - accuracy: 0.5727 - val_loss: 1.1989 - val_accuracy: 0.5132
    Epoch 22/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0248 - accuracy: 0.5777 - val_loss: 1.1998 - val_accuracy: 0.5153
    Epoch 23/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0148 - accuracy: 0.5830 - val_loss: 1.2058 - val_accuracy: 0.5125
    Epoch 24/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0064 - accuracy: 0.5848 - val_loss: 1.2153 - val_accuracy: 0.5112
    Epoch 25/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0071 - accuracy: 0.5852 - val_loss: 1.2159 - val_accuracy: 0.5074



```python
model5 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model5.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model5.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 13us/sample - loss: 1.2930 - accuracy: 0.4740 - val_loss: 1.1976 - val_accuracy: 0.5134
    Epoch 2/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.2040 - accuracy: 0.5046 - val_loss: 1.1800 - val_accuracy: 0.5182
    Epoch 3/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1853 - accuracy: 0.5137 - val_loss: 1.1729 - val_accuracy: 0.5201
    Epoch 4/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1721 - accuracy: 0.5186 - val_loss: 1.1771 - val_accuracy: 0.5190
    Epoch 5/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1631 - accuracy: 0.5195 - val_loss: 1.1690 - val_accuracy: 0.5244
    Epoch 6/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1563 - accuracy: 0.5236 - val_loss: 1.1703 - val_accuracy: 0.5247
    Epoch 7/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1507 - accuracy: 0.5268 - val_loss: 1.1635 - val_accuracy: 0.5251
    Epoch 8/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1403 - accuracy: 0.5305 - val_loss: 1.1666 - val_accuracy: 0.5208
    Epoch 9/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1331 - accuracy: 0.5331 - val_loss: 1.1676 - val_accuracy: 0.5223
    Epoch 10/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1230 - accuracy: 0.5363 - val_loss: 1.1728 - val_accuracy: 0.5177
    Epoch 11/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1192 - accuracy: 0.5374 - val_loss: 1.1720 - val_accuracy: 0.5211
    Epoch 12/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1171 - accuracy: 0.5386 - val_loss: 1.1727 - val_accuracy: 0.5219
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1050 - accuracy: 0.5420 - val_loss: 1.1697 - val_accuracy: 0.5249
    Epoch 14/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0959 - accuracy: 0.5462 - val_loss: 1.1731 - val_accuracy: 0.5203
    Epoch 15/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0887 - accuracy: 0.5499 - val_loss: 1.1766 - val_accuracy: 0.5233
    Epoch 16/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0786 - accuracy: 0.5535 - val_loss: 1.1817 - val_accuracy: 0.5207
    Epoch 17/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0806 - accuracy: 0.5548 - val_loss: 1.1892 - val_accuracy: 0.5153
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0698 - accuracy: 0.5584 - val_loss: 1.1856 - val_accuracy: 0.5199
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0542 - accuracy: 0.5616 - val_loss: 1.1866 - val_accuracy: 0.5206
    Epoch 20/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0485 - accuracy: 0.5668 - val_loss: 1.1923 - val_accuracy: 0.5160
    Epoch 21/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0379 - accuracy: 0.5697 - val_loss: 1.1964 - val_accuracy: 0.5183
    Epoch 22/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0314 - accuracy: 0.5734 - val_loss: 1.2004 - val_accuracy: 0.5174



```python
model6 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model6.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model6.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 14us/sample - loss: 1.2860 - accuracy: 0.4771 - val_loss: 1.1983 - val_accuracy: 0.5124
    Epoch 2/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.2021 - accuracy: 0.5062 - val_loss: 1.1923 - val_accuracy: 0.5175
    Epoch 3/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1859 - accuracy: 0.5118 - val_loss: 1.1751 - val_accuracy: 0.5204
    Epoch 4/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1713 - accuracy: 0.5184 - val_loss: 1.1688 - val_accuracy: 0.5214
    Epoch 5/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1660 - accuracy: 0.5202 - val_loss: 1.1671 - val_accuracy: 0.5232
    Epoch 6/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1557 - accuracy: 0.5253 - val_loss: 1.1602 - val_accuracy: 0.5221
    Epoch 7/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1473 - accuracy: 0.5271 - val_loss: 1.1706 - val_accuracy: 0.5229
    Epoch 8/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1395 - accuracy: 0.5309 - val_loss: 1.1674 - val_accuracy: 0.5188
    Epoch 9/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1376 - accuracy: 0.5309 - val_loss: 1.1697 - val_accuracy: 0.5215
    Epoch 10/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1259 - accuracy: 0.5347 - val_loss: 1.1650 - val_accuracy: 0.5224
    Epoch 11/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1203 - accuracy: 0.5387 - val_loss: 1.1711 - val_accuracy: 0.5225
    Epoch 12/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1100 - accuracy: 0.5409 - val_loss: 1.1714 - val_accuracy: 0.5224
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1002 - accuracy: 0.5456 - val_loss: 1.1767 - val_accuracy: 0.5248
    Epoch 14/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0932 - accuracy: 0.5472 - val_loss: 1.1780 - val_accuracy: 0.5170
    Epoch 15/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0930 - accuracy: 0.5481 - val_loss: 1.1783 - val_accuracy: 0.5206
    Epoch 16/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0795 - accuracy: 0.5529 - val_loss: 1.1865 - val_accuracy: 0.5171
    Epoch 17/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0686 - accuracy: 0.5576 - val_loss: 1.1858 - val_accuracy: 0.5170
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0589 - accuracy: 0.5628 - val_loss: 1.1871 - val_accuracy: 0.5176
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0502 - accuracy: 0.5652 - val_loss: 1.1943 - val_accuracy: 0.5133
    Epoch 20/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0434 - accuracy: 0.5677 - val_loss: 1.1954 - val_accuracy: 0.5207
    Epoch 21/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0350 - accuracy: 0.5709 - val_loss: 1.2038 - val_accuracy: 0.5110



```python
model7 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model7.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model7.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 14us/sample - loss: 1.2899 - accuracy: 0.4717 - val_loss: 1.1929 - val_accuracy: 0.5090
    Epoch 2/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.2027 - accuracy: 0.5056 - val_loss: 1.1800 - val_accuracy: 0.5172
    Epoch 3/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1840 - accuracy: 0.5141 - val_loss: 1.1750 - val_accuracy: 0.5214
    Epoch 4/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1723 - accuracy: 0.5174 - val_loss: 1.1762 - val_accuracy: 0.5196
    Epoch 5/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1613 - accuracy: 0.5229 - val_loss: 1.1698 - val_accuracy: 0.5234
    Epoch 6/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1548 - accuracy: 0.5259 - val_loss: 1.1739 - val_accuracy: 0.5193
    Epoch 7/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1455 - accuracy: 0.5266 - val_loss: 1.1678 - val_accuracy: 0.5217
    Epoch 8/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1393 - accuracy: 0.5294 - val_loss: 1.1656 - val_accuracy: 0.5208
    Epoch 9/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1313 - accuracy: 0.5330 - val_loss: 1.1665 - val_accuracy: 0.5224
    Epoch 10/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1235 - accuracy: 0.5358 - val_loss: 1.1748 - val_accuracy: 0.5201
    Epoch 11/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1161 - accuracy: 0.5399 - val_loss: 1.1681 - val_accuracy: 0.5221
    Epoch 12/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1091 - accuracy: 0.5411 - val_loss: 1.1781 - val_accuracy: 0.5160
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1024 - accuracy: 0.5455 - val_loss: 1.1723 - val_accuracy: 0.5224
    Epoch 14/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0929 - accuracy: 0.5491 - val_loss: 1.1795 - val_accuracy: 0.5196
    Epoch 15/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0837 - accuracy: 0.5517 - val_loss: 1.1800 - val_accuracy: 0.5174
    Epoch 16/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0739 - accuracy: 0.5544 - val_loss: 1.1837 - val_accuracy: 0.5157
    Epoch 17/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0686 - accuracy: 0.5574 - val_loss: 1.1911 - val_accuracy: 0.5174
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0572 - accuracy: 0.5621 - val_loss: 1.1934 - val_accuracy: 0.5146
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0492 - accuracy: 0.5667 - val_loss: 1.1964 - val_accuracy: 0.5127
    Epoch 20/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0404 - accuracy: 0.5687 - val_loss: 1.1958 - val_accuracy: 0.5181
    Epoch 21/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0308 - accuracy: 0.5747 - val_loss: 1.2008 - val_accuracy: 0.5140
    Epoch 22/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0209 - accuracy: 0.5799 - val_loss: 1.2083 - val_accuracy: 0.5166
    Epoch 23/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0167 - accuracy: 0.5801 - val_loss: 1.2089 - val_accuracy: 0.5119



```python
model8 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model8.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model8.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 14us/sample - loss: 1.2908 - accuracy: 0.4766 - val_loss: 1.1975 - val_accuracy: 0.5047
    Epoch 2/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.2042 - accuracy: 0.5065 - val_loss: 1.1811 - val_accuracy: 0.5183
    Epoch 3/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1839 - accuracy: 0.5130 - val_loss: 1.1677 - val_accuracy: 0.5185
    Epoch 4/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1734 - accuracy: 0.5169 - val_loss: 1.1724 - val_accuracy: 0.5209
    Epoch 5/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1649 - accuracy: 0.5195 - val_loss: 1.1728 - val_accuracy: 0.5232
    Epoch 6/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1558 - accuracy: 0.5231 - val_loss: 1.1637 - val_accuracy: 0.5260
    Epoch 7/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1482 - accuracy: 0.5286 - val_loss: 1.1695 - val_accuracy: 0.5235
    Epoch 8/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1399 - accuracy: 0.5296 - val_loss: 1.1628 - val_accuracy: 0.5224
    Epoch 9/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1327 - accuracy: 0.5322 - val_loss: 1.1750 - val_accuracy: 0.5127
    Epoch 10/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1247 - accuracy: 0.5349 - val_loss: 1.1735 - val_accuracy: 0.5205
    Epoch 11/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1175 - accuracy: 0.5389 - val_loss: 1.1686 - val_accuracy: 0.5248
    Epoch 12/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1107 - accuracy: 0.5418 - val_loss: 1.1730 - val_accuracy: 0.5185
    Epoch 13/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1035 - accuracy: 0.5439 - val_loss: 1.1769 - val_accuracy: 0.5096
    Epoch 14/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0947 - accuracy: 0.5478 - val_loss: 1.1767 - val_accuracy: 0.5184
    Epoch 15/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0897 - accuracy: 0.5495 - val_loss: 1.1793 - val_accuracy: 0.5207
    Epoch 16/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0793 - accuracy: 0.5536 - val_loss: 1.1799 - val_accuracy: 0.5187
    Epoch 17/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0668 - accuracy: 0.5591 - val_loss: 1.1837 - val_accuracy: 0.5190
    Epoch 18/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0584 - accuracy: 0.5622 - val_loss: 1.1871 - val_accuracy: 0.5213
    Epoch 19/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0512 - accuracy: 0.5643 - val_loss: 1.1939 - val_accuracy: 0.5163
    Epoch 20/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0417 - accuracy: 0.5699 - val_loss: 1.1928 - val_accuracy: 0.5194
    Epoch 21/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0342 - accuracy: 0.5737 - val_loss: 1.2008 - val_accuracy: 0.5174
    Epoch 22/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0234 - accuracy: 0.5777 - val_loss: 1.2110 - val_accuracy: 0.5142
    Epoch 23/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0175 - accuracy: 0.5799 - val_loss: 1.2115 - val_accuracy: 0.5156



```python
model9 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model9.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model9.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 13us/sample - loss: 1.2875 - accuracy: 0.4755 - val_loss: 1.1945 - val_accuracy: 0.5144
    Epoch 2/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.2027 - accuracy: 0.5052 - val_loss: 1.1797 - val_accuracy: 0.5165
    Epoch 3/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1842 - accuracy: 0.5137 - val_loss: 1.1764 - val_accuracy: 0.5203
    Epoch 4/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1724 - accuracy: 0.5181 - val_loss: 1.1704 - val_accuracy: 0.5241
    Epoch 5/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1661 - accuracy: 0.5199 - val_loss: 1.1699 - val_accuracy: 0.5212
    Epoch 6/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1567 - accuracy: 0.5242 - val_loss: 1.1631 - val_accuracy: 0.5213
    Epoch 7/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1517 - accuracy: 0.5276 - val_loss: 1.1663 - val_accuracy: 0.5245
    Epoch 8/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1416 - accuracy: 0.5274 - val_loss: 1.1630 - val_accuracy: 0.5251
    Epoch 9/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1335 - accuracy: 0.5320 - val_loss: 1.1681 - val_accuracy: 0.5204
    Epoch 10/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1273 - accuracy: 0.5350 - val_loss: 1.1664 - val_accuracy: 0.5211
    Epoch 11/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1158 - accuracy: 0.5380 - val_loss: 1.1703 - val_accuracy: 0.5230
    Epoch 12/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1097 - accuracy: 0.5409 - val_loss: 1.1713 - val_accuracy: 0.5218
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1031 - accuracy: 0.5430 - val_loss: 1.1724 - val_accuracy: 0.5214
    Epoch 14/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0940 - accuracy: 0.5475 - val_loss: 1.1744 - val_accuracy: 0.5197
    Epoch 15/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0854 - accuracy: 0.5513 - val_loss: 1.1804 - val_accuracy: 0.5191
    Epoch 16/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0781 - accuracy: 0.5543 - val_loss: 1.1814 - val_accuracy: 0.5195
    Epoch 17/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0726 - accuracy: 0.5570 - val_loss: 1.1851 - val_accuracy: 0.5197
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0636 - accuracy: 0.5588 - val_loss: 1.1912 - val_accuracy: 0.5189
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0512 - accuracy: 0.5653 - val_loss: 1.1921 - val_accuracy: 0.5203
    Epoch 20/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0437 - accuracy: 0.5669 - val_loss: 1.1958 - val_accuracy: 0.5175
    Epoch 21/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0358 - accuracy: 0.5716 - val_loss: 1.1995 - val_accuracy: 0.5165
    Epoch 22/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0263 - accuracy: 0.5764 - val_loss: 1.1990 - val_accuracy: 0.5162
    Epoch 23/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0182 - accuracy: 0.5780 - val_loss: 1.2084 - val_accuracy: 0.5071



```python
model10 = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, input_shape=[1175,]),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(512),
     tf.keras.layers.ReLU(),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(6, activation="softmax")
])

model10.compile(loss="sparse_categorical_crossentropy",
               optimizer='adam',
               metrics=["accuracy"])

history = model10.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop],
                     validation_data=(X_valid_scaled[:, :-557], y_valid))
```

    Train on 120000 samples, validate on 30000 samples
    Epoch 1/1000
    120000/120000 [==============================] - 2s 13us/sample - loss: 1.2838 - accuracy: 0.4745 - val_loss: 1.1907 - val_accuracy: 0.5142
    Epoch 2/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.2023 - accuracy: 0.5058 - val_loss: 1.1769 - val_accuracy: 0.5170
    Epoch 3/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1862 - accuracy: 0.5131 - val_loss: 1.1721 - val_accuracy: 0.5176
    Epoch 4/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1715 - accuracy: 0.5184 - val_loss: 1.1686 - val_accuracy: 0.5201
    Epoch 5/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1621 - accuracy: 0.5195 - val_loss: 1.1705 - val_accuracy: 0.5208
    Epoch 6/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1539 - accuracy: 0.5249 - val_loss: 1.1696 - val_accuracy: 0.5246
    Epoch 7/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.1470 - accuracy: 0.5261 - val_loss: 1.1696 - val_accuracy: 0.5193
    Epoch 8/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1382 - accuracy: 0.5307 - val_loss: 1.1715 - val_accuracy: 0.5221
    Epoch 9/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1349 - accuracy: 0.5309 - val_loss: 1.1713 - val_accuracy: 0.5199
    Epoch 10/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1246 - accuracy: 0.5346 - val_loss: 1.1696 - val_accuracy: 0.5232
    Epoch 11/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1153 - accuracy: 0.5385 - val_loss: 1.1677 - val_accuracy: 0.5244
    Epoch 12/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1089 - accuracy: 0.5404 - val_loss: 1.1711 - val_accuracy: 0.5219
    Epoch 13/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.1016 - accuracy: 0.5443 - val_loss: 1.1720 - val_accuracy: 0.5249
    Epoch 14/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0937 - accuracy: 0.5474 - val_loss: 1.1824 - val_accuracy: 0.5193
    Epoch 15/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0828 - accuracy: 0.5512 - val_loss: 1.1799 - val_accuracy: 0.5218
    Epoch 16/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0754 - accuracy: 0.5535 - val_loss: 1.1849 - val_accuracy: 0.5214
    Epoch 17/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0659 - accuracy: 0.5568 - val_loss: 1.1852 - val_accuracy: 0.5185
    Epoch 18/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0600 - accuracy: 0.5611 - val_loss: 1.1873 - val_accuracy: 0.5173
    Epoch 19/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0513 - accuracy: 0.5650 - val_loss: 1.1919 - val_accuracy: 0.5188
    Epoch 20/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0414 - accuracy: 0.5688 - val_loss: 1.1965 - val_accuracy: 0.5144
    Epoch 21/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0318 - accuracy: 0.5723 - val_loss: 1.2072 - val_accuracy: 0.5132
    Epoch 22/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0246 - accuracy: 0.5766 - val_loss: 1.2045 - val_accuracy: 0.5159
    Epoch 23/1000
    120000/120000 [==============================] - 1s 9us/sample - loss: 1.0138 - accuracy: 0.5798 - val_loss: 1.2109 - val_accuracy: 0.5128
    Epoch 24/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 1.0076 - accuracy: 0.5838 - val_loss: 1.2152 - val_accuracy: 0.5164
    Epoch 25/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 0.9975 - accuracy: 0.5874 - val_loss: 1.2164 - val_accuracy: 0.5154
    Epoch 26/1000
    120000/120000 [==============================] - 1s 10us/sample - loss: 0.9899 - accuracy: 0.5902 - val_loss: 1.2231 - val_accuracy: 0.5160



```python
pred1 = model1.predict(X_valid_scaled[:, :-557])
pred2 = model2.predict(X_valid_scaled[:, :-557])
pred3 = model3.predict(X_valid_scaled[:, :-557])
pred4 = model4.predict(X_valid_scaled[:, :-557])
pred5 = model5.predict(X_valid_scaled[:, :-557])
pred6 = model6.predict(X_valid_scaled[:, :-557])
pred7 = model7.predict(X_valid_scaled[:, :-557])
pred8 = model8.predict(X_valid_scaled[:, :-557])
pred9 = model9.predict(X_valid_scaled[:, :-557])
pred10 = model10.predict(X_valid_scaled[:, :-557])
```


```python
pred_mlp = (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10) / 10
log_loss(y_valid, pred_mlp)  # 1.1524238256888075
```




    1.1524238256888075




```python
import lightgbm as lgb
```


```python
bagging_fraction = 0.7  # 0.7
feature_fraction = 0.9  # 0.9
n_model = 10  # 10
```


```python
model_dict = {}

for i in range(n_model):
    print('Training %d model'%(i+1))
    d_train = lgb.Dataset(X_train_scaled, label=y_train)
    params = {}
    params['learning_rate'] = 0.1
    params['boosting_type'] = 'gbdt'
    params['objective'] = 'multiclass'
    params['num_classes'] = 6
    params['metric'] = 'multi_logloss'
    params['bagging_fraction'] = bagging_fraction
    params['bagging_freq'] = 1
    params['bagging_seed'] = i+1
    params['feature_fraction'] = feature_fraction
    params['feature_fraction_seed'] = i+1
    params['max_depth'] = 10
    params['num_leaves'] = 32
    params['lambda_l2'] = 24
    #params['max_bin'] = 64

    clf = lgb.train(params, d_train, 100)
    model_dict['model_'+str(i)] = clf  # 1.1645216634935185
```

    Training 1 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.835240 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 2 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.719457 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 3 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.793521 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 4 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.748699 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 5 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.689484 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 6 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.815211 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 7 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.769633 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 8 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.716562 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 9 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.788493 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722
    Training 10 model
    [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.829176 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 438357
    [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732
    [LightGBM] [Info] Start training from score -2.135518
    [LightGBM] [Info] Start training from score -0.918084
    [LightGBM] [Info] Start training from score -1.060618
    [LightGBM] [Info] Start training from score -4.059943
    [LightGBM] [Info] Start training from score -2.937149
    [LightGBM] [Info] Start training from score -2.717722



```python
pred_lgb = np.zeros((len(y_valid), 6))
for i in range(n_model):
    pred_lgb += model_dict['model_'+str(i)].predict(X_valid_scaled)
pred_lgb = pred_lgb / n_model   

log_loss(y_valid, pred_lgb)  # 1.1590863715913093
```




    1.1590863715913093




```python
pred = (pred_mlp + pred_lgb) / 2
```


```python
pred
```




    array([[3.57072546e-03, 5.57289383e-02, 8.97353582e-01, 2.22444551e-04,
            1.85221898e-03, 4.12720756e-02],
           [6.92599978e-02, 2.74608884e-01, 6.21422029e-01, 1.17425804e-03,
            7.18916246e-03, 2.63456698e-02],
           [2.28711285e-01, 2.70365329e-01, 1.99727329e-01, 1.52601798e-01,
            8.85579810e-02, 6.00362813e-02],
           ...,
           [6.39873077e-02, 3.29615641e-01, 4.69737381e-01, 2.69650596e-03,
            3.71115569e-02, 9.68516209e-02],
           [1.86162125e-01, 3.25978530e-01, 3.96424325e-01, 1.27466881e-02,
            3.73194014e-02, 4.13689147e-02],
           [5.89294910e-01, 3.01848296e-01, 8.59463698e-02, 7.33122963e-03,
            7.73586397e-03, 7.84330762e-03]])




```python
log_loss(y_valid, pred)  # 1.147216886499985
```




    1.147216886499985



# test


```python
clnt_id = df_test['CLNT_ID'].unique()
```


```python
df_test['PD_BUY_AM'] = df_test['PD_BUY_AM'].astype('string')
df_test['PD_BUY_CT'] = df_test['PD_BUY_CT'].astype('string')
df_test['TOT_SESS_HR_V'] = df_test['TOT_SESS_HR_V'].astype('string')

df_test['PD_BUY_AM'] = df_test['PD_BUY_AM'].map(lambda x: x.replace(',', ''))
df_test['PD_BUY_CT'] = df_test['PD_BUY_CT'].map(lambda x: x.replace(',', ''))
df_test['TOT_SESS_HR_V'] = df_test['TOT_SESS_HR_V'].map(lambda x: x.replace(',', ''))
```


```python
df_test['PD_BUY_AM'] = df_test['PD_BUY_AM'].astype('int')
df_test['PD_BUY_CT'] = df_test['PD_BUY_CT'].astype('int')
df_test['TOT_SESS_HR_V'] = df_test['TOT_SESS_HR_V'].astype('int')
```


```python
df_test['SESS_DT'] = df_test['SESS_DT'].map(lambda x: date.fromisoformat(str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))
```


```python
num_shoppings = []
avg_prices = []
total_prices = []
avg_cts = []
total_cts = []
avg_sess_views = []
total_sess_views = []
avg_sess_hrs = []
total_sess_hrs = []
avg_shopping_intervals = []
main_devices = []
pd_cs = []
clac1_nms = []
clac2_nms = []
clac3_nms = []

for i in tqdm(range(len(clnt_id))):
    temp_df_test = df_test[df_test['CLNT_ID'] == clnt_id[i]]
    temp_df_test = temp_df_test.sort_values(by=['SESS_DT', 'HITS_SEQ', 'PD_C'])
    temp_df_test = temp_df_test[~temp_df_test.duplicated(subset=['SESS_ID', 'HITS_SEQ', 'PD_C'], keep='last')]
    
    num_shopping = len(temp_df_test)
    avg_price, total_price, avg_ct, total_ct = calc_avg_total_price_ct(temp_df_test)
    avg_sess_view = temp_df_test['TOT_PAG_VIEW_CT'].values.mean()
    total_sess_view = temp_df_test['TOT_PAG_VIEW_CT'].values.sum()
    avg_sess_hr = temp_df_test['TOT_SESS_HR_V'].values.mean()
    total_sess_hr = temp_df_test['TOT_SESS_HR_V'].values.sum()
    avg_shopping_interval = calc_avg_shopping_interval(temp_df_test)
    main_device = scipy.stats.mode(temp_df_test['DVC_CTG_NM'].values).mode[0]
    pd_c = temp_df_test['PD_C'].values
    clac1_nm = temp_df_test['CLAC1_NM'].values
    clac2_nm = temp_df_test['CLAC2_NM'].values
    clac3_nm = temp_df_test['CLAC3_NM'].values
    
    num_shoppings.append(num_shopping)
    avg_prices.append(avg_price)
    total_prices.append(total_price)
    avg_cts.append(avg_ct)
    total_cts.append(total_ct)
    avg_sess_views.append(avg_sess_view)
    total_sess_views.append(total_sess_view)
    avg_sess_hrs.append(avg_sess_hr)
    total_sess_hrs.append(total_sess_hr)
    avg_shopping_intervals.append(avg_shopping_interval)
    main_devices.append(main_device)
    pd_cs.append(pd_c)
    clac1_nms.append(clac1_nm)
    clac2_nms.append(clac2_nm)
    clac3_nms.append(clac3_nm)
```

    100%|█████████████████████████████████████████████████████████████████████████| 113104/113104 [13:29<00:00, 139.73it/s]



```python
data_test = pd.DataFrame([clnt_id, num_shoppings, avg_prices, total_prices, avg_cts, total_cts, avg_sess_views, 
                          total_sess_views, avg_sess_hrs, total_sess_hrs, avg_shopping_intervals, main_devices, 
                          pd_cs, clac1_nms, clac2_nms, clac3_nms]).T
```


```python
data_test.columns = ['clnt_id', 'num_shopping', 'avg_price', 'total_price', 'avg_ct', 'total_ct', 'avg_sess_view', 
                    'total_sess_view', 'avg_sess_hr', 'total_sess_hr', 'avg_shopping_interval', 'main_device', 
                    'pd_c', 'clac1_nm', 'clac2_nm', 'clac3_nm']
```


```python
data_test
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clnt_id</th>
      <th>num_shopping</th>
      <th>avg_price</th>
      <th>total_price</th>
      <th>avg_ct</th>
      <th>total_ct</th>
      <th>avg_sess_view</th>
      <th>total_sess_view</th>
      <th>avg_sess_hr</th>
      <th>total_sess_hr</th>
      <th>avg_shopping_interval</th>
      <th>main_device</th>
      <th>pd_c</th>
      <th>clac1_nm</th>
      <th>clac2_nm</th>
      <th>clac3_nm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>3</td>
      <td>32000</td>
      <td>128000</td>
      <td>1.33333</td>
      <td>4</td>
      <td>211.333</td>
      <td>634</td>
      <td>3315</td>
      <td>9945</td>
      <td>40.5</td>
      <td>mobile</td>
      <td>[234664, 325761, 752670]</td>
      <td>[스포츠패션, 스포츠패션, 출산/육아용품]</td>
      <td>[유아동스포츠화, 유아동스포츠화, 유아스킨/바디케어]</td>
      <td>[유아동런닝/트레이닝화, 유아동스포츠샌들/슬리퍼, 유아용화장품]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>2</td>
      <td>106400</td>
      <td>212800</td>
      <td>1</td>
      <td>2</td>
      <td>93</td>
      <td>186</td>
      <td>1051</td>
      <td>2102</td>
      <td>0</td>
      <td>mobile</td>
      <td>[567605, 556431]</td>
      <td>[패션잡화, 패션잡화]</td>
      <td>[여성지갑, 여성지갑]</td>
      <td>[여성일반지갑, 여성일반지갑]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>3</td>
      <td>123000</td>
      <td>369000</td>
      <td>1</td>
      <td>3</td>
      <td>134.667</td>
      <td>404</td>
      <td>1745.67</td>
      <td>5237</td>
      <td>9.5</td>
      <td>mobile</td>
      <td>[269142, 269142, 797432]</td>
      <td>[남성의류, 남성의류, 패션잡화]</td>
      <td>[남성의류하의, 남성의류하의, 남성가방]</td>
      <td>[남성캐주얼바지, 남성캐주얼바지, 남성서류가방]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15</td>
      <td>17</td>
      <td>17587.9</td>
      <td>334170</td>
      <td>1.11765</td>
      <td>19</td>
      <td>105.412</td>
      <td>1792</td>
      <td>1356.71</td>
      <td>23064</td>
      <td>7.25</td>
      <td>mobile</td>
      <td>[738180, 514099, 566303, 843952, 843603, 76426...</td>
      <td>[모바일, 여성의류, 식기/조리기구, 화장품/뷰티케어, 출산/육아용품, 문구/사무용...</td>
      <td>[모바일액세서리, 여성의류상의, 밀폐/보관용기, 메이크업, 유아위생용품, 일반문구/...</td>
      <td>[기타모바일액세서리, 여성티셔츠/탑, 보온병/텀블러, BB/파운데이션/컴팩트류, 유...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>29</td>
      <td>2</td>
      <td>63000</td>
      <td>126000</td>
      <td>1</td>
      <td>2</td>
      <td>69</td>
      <td>138</td>
      <td>2120</td>
      <td>4240</td>
      <td>0</td>
      <td>mobile</td>
      <td>[674855, 674855]</td>
      <td>[화장품/뷰티케어, 화장품/뷰티케어]</td>
      <td>[메이크업, 메이크업]</td>
      <td>[BB/파운데이션/컴팩트류, BB/파운데이션/컴팩트류]</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>113099</th>
      <td>263089</td>
      <td>3</td>
      <td>68633.3</td>
      <td>205900</td>
      <td>1</td>
      <td>3</td>
      <td>63.3333</td>
      <td>190</td>
      <td>1700.33</td>
      <td>5101</td>
      <td>6</td>
      <td>mobile</td>
      <td>[50185, 760443, 9617]</td>
      <td>[화장품/뷰티케어, 화장품/뷰티케어, 원예/애완]</td>
      <td>[스킨케어, 메이크업, 고양이용품]</td>
      <td>[스킨케어세트, BB/파운데이션/컴팩트류, 고양이사료]</td>
    </tr>
    <tr>
      <th>113100</th>
      <td>263097</td>
      <td>4</td>
      <td>60610</td>
      <td>242440</td>
      <td>1</td>
      <td>4</td>
      <td>469.75</td>
      <td>1879</td>
      <td>3422</td>
      <td>13688</td>
      <td>17</td>
      <td>mobile</td>
      <td>[737465, 537882, 755981, 301542]</td>
      <td>[화장품/뷰티케어, 구기/필드스포츠, 건강식품, 스포츠패션]</td>
      <td>[스킨케어, 골프, 홍삼/인삼가공식품, 여성일반스포츠의류]</td>
      <td>[스킨케어세트, 골프패션잡화, 홍삼/인삼혼합세트, 여성스포츠티셔츠/탑]</td>
    </tr>
    <tr>
      <th>113101</th>
      <td>263098</td>
      <td>1</td>
      <td>100200</td>
      <td>100200</td>
      <td>1</td>
      <td>1</td>
      <td>136</td>
      <td>136</td>
      <td>1175</td>
      <td>1175</td>
      <td>183</td>
      <td>mobile</td>
      <td>[593872]</td>
      <td>[남성의류]</td>
      <td>[남성의류상의]</td>
      <td>[남성남방셔츠]</td>
    </tr>
    <tr>
      <th>113102</th>
      <td>263099</td>
      <td>4</td>
      <td>12250</td>
      <td>49000</td>
      <td>1</td>
      <td>4</td>
      <td>274</td>
      <td>1096</td>
      <td>3890</td>
      <td>15560</td>
      <td>0</td>
      <td>mobile</td>
      <td>[193132, 470807, 665269, 692330]</td>
      <td>[남성의류, 남성의류, 남성의류, 남성의류]</td>
      <td>[남성의류하의, 남성의류상의, 남성의류하의, 남성의류상의]</td>
      <td>[남성캐주얼바지, 남성티셔츠, 남성캐주얼바지, 남성티셔츠]</td>
    </tr>
    <tr>
      <th>113103</th>
      <td>263100</td>
      <td>2</td>
      <td>11200</td>
      <td>22400</td>
      <td>1</td>
      <td>2</td>
      <td>104</td>
      <td>208</td>
      <td>1474.5</td>
      <td>2949</td>
      <td>25</td>
      <td>mobile</td>
      <td>[310231, 678538]</td>
      <td>[여성의류, 남성의류]</td>
      <td>[여성의류하의, 남성의류상의]</td>
      <td>[여성바지, 남성티셔츠]</td>
    </tr>
  </tbody>
</table>
<p>113104 rows × 16 columns</p>
</div>




```python
for nm in df_test['CLAC3_NM'].unique():
    if nm not in clac3_nm_dic.keys():
        clac3_nm_dic[nm] = len(clac3_nm_dic.keys())
```


```python
clac3_nm_dic
```




    {'블랜더': 0,
     '거실수예소품': 1,
     '남성수영복': 2,
     '페이셜클렌저': 3,
     '선크림류': 4,
     '여성비치웨어': 5,
     '홍삼액': 6,
     'BB/파운데이션/컴팩트류': 7,
     '에센스/세럼': 8,
     '여성속옷세트': 9,
     '브래지어': 10,
     '여성팬티': 11,
     '여성스포츠샌들/슬리퍼': 12,
     '유아동팬티': 13,
     '유아동일반양말': 14,
     '여성샌들': 15,
     '유아동타이즈': 16,
     '주방칼/가위': 17,
     '영유아점프수트/오버롤': 18,
     '아동모': 19,
     '프라이팬': 20,
     '롤티슈': 21,
     '플라스틱서랍장': 22,
     '유아용화장품': 23,
     '일반형냉장고': 24,
     '핸드로션/크림': 25,
     '남성스포츠티셔츠': 26,
     '크림/밤/오일': 27,
     '골프공': 28,
     '골프연습장비': 29,
     '립글로즈/틴트': 30,
     '남성일반지갑': 31,
     '남성런닝/트레이닝화': 32,
     '애견주거/실내용품': 33,
     '생수': 34,
     '여성트레이닝복': 35,
     '여성런닝셔츠/캐미솔': 36,
     '채소즙': 37,
     '남성용스킨케어류': 38,
     '3단우산': 39,
     '스포츠가방': 40,
     '남성팬티': 41,
     '여성남방셔츠': 42,
     '여성원피스': 43,
     '유아동이불/이불커버': 44,
     '남성티셔츠': 45,
     '남성캐주얼바지': 46,
     '여성코트': 47,
     '남성청바지': 48,
     '기타여성속옷': 49,
     '기타냉방가전': 50,
     '영유아티셔츠/탑': 51,
     '여아티셔츠/탑': 52,
     '생리대': 53,
     '유아용기저귀': 54,
     '유산균/프로바이오틱스': 55,
     '김치류': 56,
     '남성등산바지': 57,
     '유아동스포츠샌들/슬리퍼': 58,
     '골프패션잡화': 59,
     '염모제': 60,
     '장우산': 61,
     '골프필드용품': 62,
     '텐트': 63,
     '기타에어컨': 64,
     '여성스니커즈': 65,
     '남성정장셔츠': 66,
     '토스터/제빵기': 67,
     '선풍기': 68,
     '유아동런닝셔츠': 69,
     '배낭': 70,
     '여성크로스백': 71,
     '남성런닝셔츠': 72,
     '남성남방셔츠': 73,
     '여성스웨터/풀오버': 74,
     '영유아스커트': 75,
     '여아가디건': 76,
     '아동수영복': 77,
     '미스트': 78,
     '스킨/토너': 79,
     '애견장난감/훈련': 80,
     '고양이캣타워/실내용품': 81,
     '기타구강관리용품': 82,
     '남성등산티셔츠': 83,
     '남성점퍼': 84,
     '팬티라이너': 85,
     '여성로퍼': 86,
     '여성티셔츠/탑': 87,
     '냉동국탕류': 88,
     '서랍장/수납장': 89,
     '책상의자': 90,
     '스킨케어세트': 91,
     '페이셜팩류': 92,
     '출산/신생아용품세트': 93,
     '아기띠/캐리어': 94,
     '여성스포츠티셔츠/탑': 95,
     '유아동샌들': 96,
     '여성바지': 97,
     '영유아원피스': 98,
     '남성정장바지': 99,
     '남성일반스포츠바지': 100,
     '미술/창작완구': 101,
     '기타요가/필라테스소품': 102,
     '숟가락/젓가락': 103,
     '아쿠아슈즈': 104,
     '여성재킷': 105,
     '영유아바지': 106,
     '남성골프바지': 107,
     '유아동침구세트': 108,
     '유아/아동용치약': 109,
     '스포츠모자': 110,
     '유아동슬리퍼': 111,
     '여성골프패딩': 112,
     '여성숄더백': 113,
     '야구모자': 114,
     '유아동스니커즈': 115,
     '헤어에센스': 116,
     '탁자': 117,
     '닭가슴살': 118,
     '유아동내의': 119,
     '남아티셔츠/탑': 120,
     '오리발/스노클링': 121,
     '남아잠옷': 122,
     '식탁의자': 123,
     '유아용물티슈': 124,
     '애견간식': 125,
     '여성플랫': 126,
     '애견사료': 127,
     '반찬통/밀폐용기': 128,
     '어린이홍삼': 129,
     '스포츠시계': 130,
     '고양이모래/배변용품': 131,
     '남성스포츠샌들/슬리퍼': 132,
     '혼합즙': 133,
     '아이브로우': 134,
     '아이케어': 135,
     '바디워시': 136,
     '유아용샴푸/바디워시': 137,
     '샴푸': 138,
     '남성가디건': 139,
     '여성가디건': 140,
     '캐리어': 141,
     '한방음료': 142,
     '아동용가방': 143,
     '여성일반스포츠바지': 144,
     '노트북': 145,
     '메이크업세트': 146,
     '여성임부속옷': 147,
     '접시': 148,
     '반상기세트/홈세트': 149,
     '장롱': 150,
     '매트리스': 151,
     '여성점퍼': 152,
     '홈웨어세트': 153,
     '남성골프티셔츠': 154,
     '여성스웨트셔츠/후드/집업': 155,
     '남성비치웨어': 156,
     '기타모바일액세서리': 157,
     '성인침구속통/솜': 158,
     '바구니': 159,
     '테이프': 160,
     '칼/가위': 161,
     '수정용품': 162,
     '유아동선글라스': 163,
     '여성클러치백': 164,
     '영유아청바지': 165,
     '캠핑테이블/의자': 166,
     '여아바지': 167,
     '남아청바지': 168,
     '고양이사료': 169,
     '커튼': 170,
     '기타물놀이용품': 171,
     '여성신발부속품': 172,
     '노트북가방': 173,
     '여성카드/명함지갑': 174,
     '책상': 175,
     '여성스커트': 176,
     '욕실발판': 177,
     '마스카라': 178,
     '남성시계': 179,
     '남성패딩': 180,
     '식탁세트': 181,
     '피규어': 182,
     '욕실소품': 183,
     '롤플레잉완구': 184,
     '기타캠핑용품': 185,
     '분말표백제': 186,
     '풀': 187,
     '종합영양제': 188,
     '피트니스용품': 189,
     '스툴/리빙의자': 190,
     '책장': 191,
     '칫솔': 192,
     '수건': 193,
     '액상세탁세제': 194,
     '영유아블라우스': 195,
     '남아바지': 196,
     '헤어케어선물세트': 197,
     '여성등산티셔츠/탑': 198,
     '남성베스트': 199,
     '썬캡': 200,
     '스카프': 201,
     '블러셔/쉐이딩/하이라이터': 202,
     '여아레깅스': 203,
     '애견목욕/위생용품': 204,
     '기타일반문구/사무용품': 205,
     '린스/컨디셔너': 206,
     '샴푸/린스세트': 207,
     '미용비누': 208,
     '성인매트리스커버': 209,
     '스냅백': 210,
     '기타컴퓨터액세서리': 211,
     '유아동플랫': 212,
     '기타영양제': 213,
     '여아남방셔츠': 214,
     '남아의류세트': 215,
     '탄산수': 216,
     '일반청소기': 217,
     '손싸개/발싸개': 218,
     '유아동런닝/트레이닝화': 219,
     '여성토트백': 220,
     '여성일반양말': 221,
     '여성런닝/트레이닝화': 222,
     '치약': 223,
     '샤워/목욕도구/목욕헤어밴드': 224,
     '봉제인형': 225,
     '유아동베개/베개커버': 226,
     '놀이방매트': 227,
     '식음료모바일상품권': 228,
     '여아베스트': 229,
     '성인베개/베개커버': 230,
     '이유식용품': 231,
     '아이섀도우': 232,
     '남성샌들': 233,
     '기타이미용가전': 234,
     '요가/필라테스복': 235,
     '여성향수': 236,
     '기타정리용품': 237,
     '트리트먼트/팩': 238,
     '욕실청소용품': 239,
     '기타주방정리용품/소모품': 240,
     '여아청바지': 241,
     '기름종이': 242,
     '아이라이너': 243,
     '홍삼/인삼혼합세트': 244,
     '여성시계': 245,
     '손수건': 246,
     '팔찌': 247,
     '남성숄더/크로스백': 248,
     '지퍼백/비닐백': 249,
     '성인패드/스프레드': 250,
     '여성펌프스': 251,
     '구강청정제': 252,
     '남성일반양말': 253,
     '여성오픈토': 254,
     '공기청정기': 255,
     '여성등산바지': 256,
     '커튼링/커튼봉/부속품': 257,
     '메이크업베이스/프라이머': 258,
     '여아점퍼': 259,
     '남성등산점퍼/재킷': 260,
     '유아동슬립온': 261,
     '인삼가공식품': 262,
     '여성덧신류': 263,
     '냉동핫도그': 264,
     '여성블라우스': 265,
     '식기건조대/수저통': 266,
     '국자/뒤지개/주걱': 267,
     '여성부츠': 268,
     '여성베스트': 269,
     '에멀젼/로션': 270,
     '기타조리도구': 271,
     '바디보습': 272,
     '여성패딩': 273,
     '성인이불/이불커버': 274,
     '솥': 275,
     '이불/옷압축팩': 276,
     '섬유유연제/향기지속제': 277,
     '양산': 278,
     '여성수영복': 279,
     '남성내의': 280,
     '기타등산용품': 281,
     '유아용세척용품': 282,
     '도마': 283,
     '생활모바일상품권': 284,
     '젖병/젖꼭지': 285,
     '아동우산': 286,
     '케이스/보호필름': 287,
     '남성스킨케어세트': 288,
     '여성백팩': 289,
     '커피머신': 290,
     '방석/방석커버': 291,
     '장식장/진열장': 292,
     '다이어트보조식품': 293,
     '남성스니커즈': 294,
     '남성용클렌저': 295,
     '소품가방': 296,
     '변기시트/커버': 297,
     '헤드웨어': 298,
     '여행용소품': 299,
     '목걸이': 300,
     '미용보조식품': 301,
     '제빵용품': 302,
     'PC부품': 303,
     '등산화': 304,
     '젤네일/케어류': 305,
     '핸디형청소기': 306,
     '주방선반/걸이대': 307,
     '옷걸이': 308,
     '유아동침대': 309,
     '로봇청소기': 310,
     '헤어드라이어': 311,
     '레저모바일상품권': 312,
     '스폰지/퍼프': 313,
     '커피용품': 314,
     '커피잔': 315,
     '저장장치': 316,
     '우주복': 317,
     '립스틱/립라이너': 318,
     '조립/프라모델': 319,
     '스케이트보드/킥보드': 320,
     '남성로퍼': 321,
     '여성쪼리': 322,
     '여성트렌치코트': 323,
     '남성정장재킷': 324,
     '골프화': 325,
     '남성트레이닝복': 326,
     '전기찜기': 327,
     '남성향수': 328,
     '일반비타민': 329,
     '여성슬링백': 330,
     '촉각놀이/오뚝이': 331,
     '기타패션잡화': 332,
     '데오도란트': 333,
     '수영모자': 334,
     '여성슬리퍼': 335,
     '고무장갑': 336,
     '일반두유': 337,
     '속눈썹/쌍꺼풀': 338,
     '물안경': 339,
     '기타여행용가방': 340,
     '남성트렌치코트': 341,
     '발찌': 342,
     '남성스웨터/풀오버': 343,
     '호두': 344,
     '캠핑침구': 345,
     '아동비치웨어': 346,
     '애견의류/악세서리': 347,
     '성인침구세트': 348,
     '남성등산패딩': 349,
     '냉동만두': 350,
     '냄비': 351,
     '남성캐주얼재킷': 352,
     '승마운동기': 353,
     '바디케어세트': 354,
     '거들': 355,
     '성인요/요커버': 356,
     '여성내의': 357,
     '루테인': 358,
     '여성청바지': 359,
     '여성잠옷': 360,
     '인덕션/가스레인지': 361,
     '캠핑취사': 362,
     '마우스': 363,
     '블랙박스': 364,
     '포크/나이프': 365,
     '남성속옷세트': 366,
     '여성슬립온': 367,
     '오메가3/기타추출오일': 368,
     '헤어세팅기': 369,
     '키보드': 370,
     '모바일배터리/충전기': 371,
     '채반/바구니/쟁반': 372,
     '선반장/행거': 373,
     '안경테': 374,
     '여성골프바지': 375,
     '커피메이커/포트': 376,
     '유아동스포츠티셔츠/탑': 377,
     '유아동스포츠스웨트셔츠/후드/집업': 378,
     '고양이간식': 379,
     '사무용/학생용가구세트': 380,
     '벽걸이형에어컨': 381,
     '여성등산점퍼/재킷': 382,
     '홍삼정/분말/환': 383,
     '토마토': 384,
     '스킨케어디바이스': 385,
     '입욕제/스파제품': 386,
     '과일즙': 387,
     '전기튀김기': 388,
     '성인담요': 389,
     '귀걸이': 390,
     '소파': 391,
     '행주': 392,
     '여성골프남방셔츠': 393,
     '영유아남방셔츠': 394,
     '스텝퍼/트위스트': 395,
     '여성등산패딩': 396,
     '식탁': 397,
     '전기밥솥': 398,
     '대접/볼': 399,
     '밥공기': 400,
     '찬기/종지': 401,
     '여성발가락양말': 402,
     '역할놀이': 403,
     '유아용카시트/매트': 404,
     '젖병소독/건조용품': 405,
     '유아/아동용칫솔': 406,
     '기타냉동간편식': 407,
     '유아목욕용품': 408,
     '유아동트레이닝복': 409,
     '여아잠옷': 410,
     '선반/걸이': 411,
     '여성양말선물세트': 412,
     '물티슈': 413,
     '남성코트': 414,
     '분말세탁세제': 415,
     '수유패드/보조용품': 416,
     '유아동의자': 417,
     '학생용가방': 418,
     '남성스포츠점퍼/재킷': 419,
     '유아공부상/디딤대': 420,
     '잉크/토너': 421,
     '여성향수세트': 422,
     '펜던트': 423,
     '여성일반지갑': 424,
     '보드게임': 425,
     '레고': 426,
     '유아동스포츠점퍼/재킷': 427,
     '치약/칫솔세트': 428,
     '수영가방': 429,
     '패션인형': 430,
     '도시락/찬합': 431,
     '발효원액': 432,
     '남성등산베스트': 433,
     '여성선글라스': 434,
     '여성점프수트/오버롤': 435,
     '치아발육기/딸랑이': 436,
     '남성카드/명함지갑': 437,
     '남성용선크림/메이크업류': 438,
     '남성골프점퍼/재킷': 439,
     '여성골프티셔츠/탑': 440,
     '기타국산과일류': 441,
     '여아스커트': 442,
     '건조기': 443,
     '기타기능성음료': 444,
     '스피커': 445,
     '얼음/빙수용품': 446,
     '스타킹': 447,
     '보온병/텀블러': 448,
     '전동칫솔/칫솔모': 449,
     '여아스웨트셔츠/후드/집업': 450,
     '혼합견과': 451,
     '볼펜': 452,
     '필통': 453,
     '샤프/샤프심': 454,
     '필기구세트': 455,
     '엽산/철분': 456,
     '유아패션잡화': 457,
     '휴대폰': 458,
     '각티슈/미용티슈': 459,
     '골프가방': 460,
     '여성타이즈': 461,
     '요가/스포츠매트': 462,
     '여성골프스커트': 463,
     '골프장갑': 464,
     '여성골프니트/가디건': 465,
     '여성골프베스트': 466,
     '영유아점퍼': 467,
     '영유아가디건': 468,
     '남성스웨트셔츠/후드/집업': 469,
     '사인펜': 470,
     '남성선글라스': 471,
     '반지': 472,
     '이어폰/헤드폰': 473,
     '조리도구세트': 474,
     '키친타올': 475,
     '남녀공용향수': 476,
     '유아동레인부츠/슈즈': 477,
     '유아건강보조제': 478,
     '여성가운': 479,
     '남성잠옷': 480,
     '유아동스포츠패딩': 481,
     '여아패딩': 482,
     '전통/종교장신구': 483,
     '복근/벨트마사지기구': 484,
     '벙거지': 485,
     '슬립': 486,
     '만년필': 487,
     '공병/모델링팩전용도구': 488,
     '화장대': 489,
     '핸드카트': 490,
     '여성레인부츠/슈즈': 491,
     '퍼즐': 492,
     '캐쥬얼크로스백': 493,
     '음악/악기완구': 494,
     '아기체육관/러닝홈': 495,
     '면봉/화장솜': 496,
     '남성골프남방셔츠': 497,
     '수예소품속통/솜': 498,
     '쿠션/쿠션커버': 499,
     '붙박이장': 500,
     '기타견과류': 501,
     '아몬드': 502,
     '여성등산베스트': 503,
     '영유아레깅스': 504,
     '여성컴포트화': 505,
     '남성정장화': 506,
     '블라인드/버티컬': 507,
     '스포츠두건/머플러/마스크': 508,
     '남성골프패딩': 509,
     '스포츠양말': 510,
     '남성정장세트': 511,
     '음료용컵': 512,
     '인라인/스케이트보드/킥보드안전용품': 513,
     '애견식기/물병': 514,
     '복숭아': 515,
     '글루코사민': 516,
     '헤어브러쉬/롤': 517,
     '거실화/실내화': 518,
     '유아동담요': 519,
     '고데기': 520,
     '칼슘/미네랄': 521,
     '주방수예소품': 522,
     '무선조종': 523,
     '수세미/솔': 524,
     '유모차': 525,
     '남성덧신류': 526,
     '참외': 527,
     '사과': 528,
     '제습기': 529,
     '양문형냉장고': 530,
     '메이크업브러쉬': 531,
     '가습기': 532,
     '유아동요/요커버': 533,
     '구명조끼/안전용품': 534,
     '네일케어도구': 535,
     '애견건강용품': 536,
     '오븐/전자레인지': 537,
     '압력솥': 538,
     '기타유아동화': 539,
     '유아동일반스포츠바지': 540,
     '여성세정제': 541,
     '비닐장갑': 542,
     '스포츠선글라스': 543,
     '미니자동차': 544,
     '전자교육완구': 545,
     '수박': 546,
     '바디슬리밍/리프팅': 547,
     '남아셔츠': 548,
     '헤어무스/젤': 549,
     '남아실내복': 550,
     '풋케어': 551,
     '여아재킷': 552,
     '제기': 553,
     '일반교육완구': 554,
     '올인원': 555,
     '유축기': 556,
     '욕실화': 557,
     '필기도구소모품': 558,
     '남성머니클립': 559,
     '헤어스프레이': 560,
     '스팀청소기': 561,
     '여성스포츠점퍼/재킷': 562,
     '기타보석류': 563,
     '자두': 564,
     '메론': 565,
     '멀티형에어컨': 566,
     '기타모자': 567,
     '유아동패드/스프레드': 568,
     '카메라액세서리': 569,
     '전기면도기': 570,
     '유아동방한화': 571,
     '남성서류가방': 572,
     '책상정리용품': 573,
     '비니': 574,
     '남성슬립온': 575,
     '디저트포크/스푼': 576,
     '목욕용장난감': 577,
     '우비': 578,
     '주방수납장': 579,
     '유아동수납장': 580,
     '냉동떡볶이': 581,
     '건강보조식품세트': 582,
     '특수용세탁세제': 583,
     '여성레깅스': 584,
     '집게/클립': 585,
     '캐노피': 586,
     '핸드워시/손세정제': 587,
     '유아용욕조': 588,
     '모유보관용품': 589,
     '바운서/쏘서/보행기': 590,
     '발포비타민': 591,
     '여성스포츠베스트': 592,
     '드럼세탁기': 593,
     '고양이건강용품': 594,
     '스탠드형에어컨': 595,
     '남성신발부속품': 596,
     '유아동속옷세트': 597,
     '자연유래영양제': 598,
     '조리기구세트': 599,
     '남성골프스웨트셔츠/후드/집업': 600,
     '여성방한화': 601,
     'LED': 602,
     'UHD': 603,
     '냉동튀김': 604,
     '캐쥬얼백팩': 605,
     '욕실수납용품': 606,
     '연필깎이': 607,
     '연필': 608,
     '탐폰': 609,
     '운동보조식품': 610,
     '남성백팩': 611,
     '유아동로퍼': 612,
     '헤어왁스': 613,
     '배냇저고리': 614,
     '영유아베스트': 615,
     '고양이장난감': 616,
     '고양이목욕/위생용품': 617,
     '여성스포츠스웨트셔츠/후드/집업': 618,
     '국그릇': 619,
     '남성힙색': 620,
     '남성스포츠화부속품': 621,
     '여아블라우스': 622,
     '기타주방가전': 623,
     '남성스포츠스웨트셔츠/후드/집업': 624,
     '서류정리용품': 625,
     '숙취해소음료': 626,
     '배': 627,
     '영화/문화모바일상품권': 628,
     '남성스포츠속옷': 629,
     '보온도시락': 630,
     '남아스웨트셔츠/후드/집업': 631,
     '홍삼절편': 632,
     '침실가구세트': 633,
     '일반네일/케어류': 634,
     '남성슬리퍼': 635,
     '시계세트': 636,
     '한우선물세트': 637,
     '남성클러치백': 638,
     '붕붕카/스프링카/흔들말': 639,
     '이발기': 640,
     '삼계탕용닭': 641,
     '남아레깅스': 642,
     '블록': 643,
     '기타청소기': 644,
     '남성양말선물세트': 645,
     '남성캐쥬얼스포츠양말': 646,
     '목욕타올': 647,
     '여성골프스웨트셔츠/후드/집업': 648,
     '호일/랩/기름종이': 649,
     '파일/바인더': 650,
     '기타피트니스기구': 651,
     '영양제세트': 652,
     '튜브/보트': 653,
     '에어로빅복': 654,
     '여성사파리': 655,
     '러닝/워킹머신': 656,
     '롤스크린': 657,
     '애견이동장': 658,
     '액상표백제': 659,
     '야외용돗자리': 660,
     '침대': 661,
     '여성등산전신/원피스': 662,
     '메탈미용소도구': 663,
     '모빌': 664,
     '주전자': 665,
     '주류잔': 666,
     '오븐팬/피자팬': 667,
     '홍삼근': 668,
     '남성사파리': 669,
     '여성골프점퍼/재킷': 670,
     '형광펜': 671,
     '독서대': 672,
     '보석세트': 673,
     '열쇠고리': 674,
     '기타여성의류아우터': 675,
     '영유아코트': 676,
     '군모': 677,
     '헬스바이크': 678,
     '남성등산/아웃도어세트': 679,
     '남성등산전신': 680,
     '유아동부츠': 681,
     '남성컴포트화': 682,
     '영유아스웨터/풀오버': 683,
     '스탠드형김치냉장고': 684,
     '부분세탁제': 685,
     '닭윗날개(봉)': 686,
     '샤워커튼': 687,
     '골프채': 688,
     '미러리스': 689,
     '기타유아안전용품': 690,
     '영유아재킷': 691,
     '등산지팡이/스틱': 692,
     '땅콩': 693,
     '냉동밥': 694,
     '스포츠아대/헤어밴드': 695,
     '순금/순은/장식품': 696,
     '가발/부분가발': 697,
     '여성등산/아웃도어세트': 698,
     '전기그릴': 699,
     '그릴/구이불판': 700,
     '컵/행주살균기': 701,
     '뚝배기': 702,
     '기타유아동양말류': 703,
     '유아변기/배변훈련기': 704,
     '여성골프전신/원피스': 705,
     '무릎담요': 706,
     '태블릿PC': 707,
     '캐슈넛': 708,
     '남성스포츠패딩': 709,
     '싱크대/배수구용품': 710,
     '여성스포츠속옷': 711,
     '교자상/다용도상': 712,
     '캐쥬얼힙색': 713,
     '매직/보드마카': 714,
     '영유아스웨트셔츠/후드/집업': 715,
     '남성골프베스트': 716,
     '여성스포츠스커트': 717,
     '자/제도용품': 718,
     '문구세트': 719,
     '남성수면양말': 720,
     '남아스웨터/풀오버': 721,
     '일반세탁기': 722,
     '캐쥬얼숄더백': 723,
     '이불/옷커버류': 724,
     '전기냄비/뚝배기': 725,
     '피스타치오': 726,
     '돼지고기선물세트': 727,
     '전자계산기': 728,
     '힙색/사이드백': 729,
     '머플러': 730,
     '넥워머': 731,
     '젓갈': 732,
     '세탁비누': 733,
     '목욕가운': 734,
     '남성발가락양말': 735,
     '공유기': 736,
     '협탁': 737,
     '성인침대커버/스커트': 738,
     '명함정리용품': 739,
     '절임반찬': 740,
     '과실주병': 741,
     '립밤/립스크럽': 742,
     '제모용품': 743,
     '제모기': 744,
     '계량도구': 745,
     '보드류': 746,
     '프린터/복합기/스캐너': 747,
     '양념통': 748,
     '방울토마토': 749,
     '밤': 750,
     '스포츠목걸이/팔찌': 751,
     '고양이식기/급수': 752,
     '그늘막/타프': 753,
     '기타모바일기기': 754,
     '유아동옷장': 755,
     '남아가디건': 756,
     '테이블데코': 757,
     '여성스포츠전신/원피스': 758,
     '냅킨': 759,
     '바란스': 760,
     '뚜껑형김치냉장고': 761,
     '남성골프니트/가디건': 762,
     '여아코트': 763,
     '유아동매트리스커버': 764,
     '여성스포츠패딩': 765,
     '다기류': 766,
     '컴팩트': 767,
     '2단우산': 768,
     '기타냉장고': 769,
     '물병': 770,
     '데스크탑/올인원PC': 771,
     '기타카메라': 772,
     '마카다미아': 773,
     '냉동부침': 774,
     '브로치': 775,
     '남아베스트': 776,
     '여행용세트': 777,
     '귤류': 778,
     '하이앤드': 779,
     '항아리/쌀독류': 780,
     '헤어롤': 781,
     '여아스웨터/풀오버': 782,
     '냉동고': 783,
     '하이브리드': 784,
     '기타여성양말류': 785,
     '패션액세서리세트': 786,
     '기타영유아아우터': 787,
     '잣': 788,
     '스포츠음료': 789,
     '스테이플러': 790,
     '영유아패딩': 791,
     '기타배낭소품': 792,
     '시공가구': 793,
     'DIY가구': 794,
     '살구': 795,
     '여성수면양말': 796,
     '유아동침구매트': 797,
     '기타남성양말류': 798,
     '정수기': 799,
     '여아실내복': 800,
     '모니터': 801,
     '유아동침구속통/솜': 802,
     '전동보드/전동킥보드': 803,
     '공간박스': 804,
     '하이패스': 805,
     '신발장': 806,
     'DSLR': 807,
     '남성실내복': 808,
     '기타남성화': 809,
     '네비게이션': 810,
     '전기프라이팬': 811,
     '여성실내복': 812,
     '오프너/와인스크류': 813,
     '유아동시계': 814,
     '환풍기': 815,
     '볶음반찬': 816,
     '파티션': 817,
     '냉온풍기': 818,
     '펀치류': 819,
     '냉동피자': 820,
     '기차/레일완구': 821,
     '인라인/롤러스케이트': 822,
     '매실': 823,
     '주방용탈수기': 824,
     '유아동침대커버/스커트': 825,
     '기타자동차가전기기': 826,
     '여성캐쥬얼스포츠양말': 827,
     '네일세트': 828,
     '남성스포츠베스트': 829,
     '채칼/강판/절구': 830,
     '캐쥬얼시계': 831,
     '니삭스/오버니삭스': 832,
     '싸인물/자석/압핀': 833,
     '고양이의류/악세서리': 834,
     '비타민/에너지음료': 835,
     '에어워셔': 836,
     '냉장/냉동가전소모품': 837,
     '남성부츠': 838,
     '물걸레청소기': 839,
     '음식물건조기': 840,
     '요구르트/청국장제조기': 841,
     '골프채세트': 842,
     '고양이이동장': 843,
     '냉동면': 844,
     '소프트웨어': 845,
     '캠코더': 846,
     '무화과': 847,
     'OLED': 848,
     '탈수기': 849,
     '육가공품선물세트': 850,
     '딸기': 851,
     '식기세척기': 852,
     '차량용충전기': 853,
     '유아두유': 854,
     '닭근위': 855,
     '닭아랫날개(윙)': 856,
     '안경소품': 857,
     '여성가방액세서리': 858,
     '태닝/애프터선케어': 859,
     '유아동스포츠스커트': 860,
     '카메라렌즈': 861,
     '닭안심': 862,
     '포도': 863,
     '볶음탕용닭': 864,
     '콜렉션인형': 865,
     'LCD': 866,
     '리모컨/액세서리': 867,
     '반죽기/제면기': 868,
     '식기건조기': 869,
     '단무지': 870,
     '닭다리': 871,
     '감': 872,
     '오리고기': 873,
     '침구청소기': 874,
     '싱크대': 875,
     '미용거울': 876,
     '남녀공용향수세트': 877,
     '인라인/스케이트보드/킥보드기타액세서리': 878,
     '남성등산스웨트셔츠/후드/집업': 879,
     '커튼류세트': 880,
     '오토캠핑용품세트': 881,
     '수도용품': 882,
     '석류': 883,
     '양푼/믹싱볼': 884,
     '자연/과학완구': 885,
     '기저귀크림/파우더': 886,
     '세탁기소모품': 887,
     '냉동디저트': 888,
     '기타광학기기': 889}




```python
data_test['clac1_nm'] = data_test['clac1_nm'].map(lambda x: [clac1_nm_dic[nm] for nm in x])
data_test['clac2_nm'] = data_test['clac2_nm'].map(lambda x: [clac2_nm_dic[nm] for nm in x])
data_test['clac3_nm'] = data_test['clac3_nm'].map(lambda x: [clac3_nm_dic[nm] for nm in x])
```


```python
clac1_matrix = np.zeros((len(data_test), len(clac1_nm_dic)))
for i in range(len(data_test)):
    for j in data_test['clac1_nm'][i]:
        clac1_matrix[i, j] += 1
```


```python
cols = ['clac1_nm_' + str(i) for i in range(len(clac1_nm_dic))]
```


```python
clac1_df = pd.DataFrame(clac1_matrix)
clac1_df.columns = cols
```


```python
clac2_matrix = np.zeros((len(data_test), len(clac2_nm_dic)))
for i in range(len(data_test)):
    for j in data_test['clac2_nm'][i]:
        clac2_matrix[i, j] += 1
```


```python
cols = ['clac2_nm_' + str(i) for i in range(len(clac2_nm_dic))]
```


```python
clac2_df = pd.DataFrame(clac2_matrix)
clac2_df.columns = cols
```


```python
clac3_matrix = np.zeros((len(data_test), len(clac3_nm_dic)))
for i in range(len(data_test)):
    for j in data_test['clac3_nm'][i]:
        clac3_matrix[i, j] += 1
```


```python
cols = ['clac3_nm_' + str(i) for i in range(len(clac3_nm_dic))]
```


```python
clac3_df = pd.DataFrame(clac3_matrix)
clac3_df.columns = cols
```


```python
data_test_concat = pd.concat([data_test.iloc[:, :-5], clac1_df, clac2_df, clac3_df], axis=1)
```


```python
data_test_concat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clnt_id</th>
      <th>num_shopping</th>
      <th>avg_price</th>
      <th>total_price</th>
      <th>avg_ct</th>
      <th>total_ct</th>
      <th>avg_sess_view</th>
      <th>total_sess_view</th>
      <th>avg_sess_hr</th>
      <th>total_sess_hr</th>
      <th>...</th>
      <th>clac3_nm_880</th>
      <th>clac3_nm_881</th>
      <th>clac3_nm_882</th>
      <th>clac3_nm_883</th>
      <th>clac3_nm_884</th>
      <th>clac3_nm_885</th>
      <th>clac3_nm_886</th>
      <th>clac3_nm_887</th>
      <th>clac3_nm_888</th>
      <th>clac3_nm_889</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>3</td>
      <td>32000</td>
      <td>128000</td>
      <td>1.33333</td>
      <td>4</td>
      <td>211.333</td>
      <td>634</td>
      <td>3315</td>
      <td>9945</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>2</td>
      <td>106400</td>
      <td>212800</td>
      <td>1</td>
      <td>2</td>
      <td>93</td>
      <td>186</td>
      <td>1051</td>
      <td>2102</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>3</td>
      <td>123000</td>
      <td>369000</td>
      <td>1</td>
      <td>3</td>
      <td>134.667</td>
      <td>404</td>
      <td>1745.67</td>
      <td>5237</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15</td>
      <td>17</td>
      <td>17587.9</td>
      <td>334170</td>
      <td>1.11765</td>
      <td>19</td>
      <td>105.412</td>
      <td>1792</td>
      <td>1356.71</td>
      <td>23064</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>29</td>
      <td>2</td>
      <td>63000</td>
      <td>126000</td>
      <td>1</td>
      <td>2</td>
      <td>69</td>
      <td>138</td>
      <td>2120</td>
      <td>4240</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>113099</th>
      <td>263089</td>
      <td>3</td>
      <td>68633.3</td>
      <td>205900</td>
      <td>1</td>
      <td>3</td>
      <td>63.3333</td>
      <td>190</td>
      <td>1700.33</td>
      <td>5101</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>113100</th>
      <td>263097</td>
      <td>4</td>
      <td>60610</td>
      <td>242440</td>
      <td>1</td>
      <td>4</td>
      <td>469.75</td>
      <td>1879</td>
      <td>3422</td>
      <td>13688</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>113101</th>
      <td>263098</td>
      <td>1</td>
      <td>100200</td>
      <td>100200</td>
      <td>1</td>
      <td>1</td>
      <td>136</td>
      <td>136</td>
      <td>1175</td>
      <td>1175</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>113102</th>
      <td>263099</td>
      <td>4</td>
      <td>12250</td>
      <td>49000</td>
      <td>1</td>
      <td>4</td>
      <td>274</td>
      <td>1096</td>
      <td>3890</td>
      <td>15560</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>113103</th>
      <td>263100</td>
      <td>2</td>
      <td>11200</td>
      <td>22400</td>
      <td>1</td>
      <td>2</td>
      <td>104</td>
      <td>208</td>
      <td>1474.5</td>
      <td>2949</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>113104 rows × 1066 columns</p>
</div>




```python
mat_clac1 = np.array(data_test_concat.iloc[:, 11:48]).dot(lookup_table_clac1)
mat_clac1 = mat_clac1 / mat_clac1.sum(1).reshape(-1, 1)
```


```python
mat_clac1 = pd.DataFrame(mat_clac1)
mat_clac1 = mat_clac1.rename(columns={0: 'given_clac1_F20_prob', 1:'given_clac1_F30_prob', 2:'given_clac1_F40_prob', 3:'given_clac1_M20_prob', 4:'given_clac1_M30_prob', 5:'given_clac1_M40_prob'})
```


```python
mat_buy_clac1 = np.where(np.array(data_test_concat.iloc[:, 11:48]) >= 1, 1, 0).dot(lookup_table_clac1)
mat_buy_clac1 = mat_buy_clac1 / mat_buy_clac1.sum(1).reshape(-1, 1)
```


```python
mat_buy_clac1 = pd.DataFrame(mat_buy_clac1)
mat_buy_clac1 = mat_buy_clac1.rename(columns={0: 'buy_clac1_F20_prob', 1:'buy_clac1_F30_prob', 2:'buy_clac1_F40_prob', 3:'buy_clac1_M20_prob', 4:'buy_clac1_M30_prob', 5:'buy_clac1_M40_prob'})
```


```python
mat_clac2 = np.array(data_test_concat.iloc[:, 48:-890]).dot(lookup_table_clac2)
mat_clac2 = mat_clac2 / mat_clac2.sum(1).reshape(-1, 1)
```


```python
mat_clac2 = pd.DataFrame(mat_clac2)
mat_clac2 = mat_clac2.rename(columns={0: 'given_clac2_F20_prob', 1:'given_clac2_F30_prob', 2:'given_clac2_F40_prob', 3:'given_clac2_M20_prob', 4:'given_clac2_M30_prob', 5:'given_clac2_M40_prob'})
```


```python
mat_buy_clac2 = np.where(np.array(data_test_concat.iloc[:, 48:-890]) >= 1, 1, 0).dot(lookup_table_clac2)
mat_buy_clac2 = mat_buy_clac2 / mat_buy_clac2.sum(1).reshape(-1, 1)
```


```python
mat_buy_clac2 = pd.DataFrame(mat_buy_clac2)
mat_buy_clac2 = mat_buy_clac2.rename(columns={0: 'buy_clac2_F20_prob', 1:'buy_clac2_F30_prob', 2:'buy_clac2_F40_prob', 3:'buy_clac2_M20_prob', 4:'buy_clac2_M30_prob', 5:'buy_clac2_M40_prob'})
```


```python
lookup_table_clac3 = np.concatenate([lookup_table_clac3, np.ones([7, 6]) * 1/6], axis=0)
```


```python
mat_clac3 = np.array(data_test_concat.iloc[:, -890:]).dot(lookup_table_clac3)
mat_clac3 = mat_clac3 / mat_clac3.sum(1).reshape(-1, 1)
```


```python
mat_clac3 = pd.DataFrame(mat_clac3)
mat_clac3 = mat_clac3.rename(columns={0: 'given_clac3_F20_prob', 1:'given_clac3_F30_prob', 2:'given_clac3_F40_prob', 3:'given_clac3_M20_prob', 4:'given_clac3_M30_prob', 5:'given_clac3_M40_prob'})
```


```python
mat_buy_clac3 = np.where(np.array(data_test_concat.iloc[:, -890:]) >= 1, 1, 0).dot(lookup_table_clac3)
mat_buy_clac3 = mat_buy_clac3 / mat_buy_clac3.sum(1).reshape(-1, 1)
```


```python
mat_buy_clac3 = pd.DataFrame(mat_buy_clac3)
mat_buy_clac3 = mat_buy_clac3.rename(columns={0: 'buy_clac3_F20_prob', 1:'buy_clac3_F30_prob', 2:'buy_clac3_F40_prob', 3:'buy_clac3_M20_prob', 4:'buy_clac3_M30_prob', 5:'buy_clac3_M40_prob'})
```


```python
tt = df_test.groupby('CLNT_ID')['PD_C'].value_counts()[df_test['CLNT_ID'].unique()]
kk = df_test['CLNT_ID'].unique()
ww = df_test.groupby(['CLNT_ID', 'PD_C'])['KWD_NM'].count()
```


```python
pd_mean_vector = []

for words in tqdm(train_pd_w2v[-len(test_pd_w2v):]):
        tmp = np.zeros(256) 
        cnt = 0
        for word in words:
            try:
                tmp += pd_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        pd_mean_vector.append(tmp)
        
pd_mean_vector = np.array(pd_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:04<00:00, 27884.46it/s]



```python
pd_unweight_vector = []

for i, words in tqdm(enumerate(train_pd_w2v[-len(test_pd_w2v):])):
        tmp = np.zeros(256) 
        cnt = 0
        for word in words:
            try:
                tmp += pd_w2v[word] * tt[kk[i]][word]
                cnt += tt[kk[i]][word]
            except:
                pass
        #tmp /= cnt 
        pd_unweight_vector.append(tmp)
        
pd_unweight_vector = np.array(pd_unweight_vector)
```

    113104it [17:48, 105.88it/s]



```python
pd_weight_vector = []

for i, words in tqdm(enumerate(train_pd_w2v[-len(test_pd_w2v):])):
        tmp = np.zeros(256) 
        cnt = 0
        for word in words:
            try:
                tmp += pd_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        pd_weight_vector.append(tmp)
        
pd_weight_vector = np.array(pd_weight_vector)
```

    113104it [09:06, 207.03it/s]



```python
test_pd_mean = pd_mean_vector
test_pd_unweight = pd_unweight_vector
test_pd_weight = pd_weight_vector

test_pd_mean = pd.DataFrame(test_pd_mean)
test_pd_unweight = pd.DataFrame(test_pd_unweight)
test_pd_weight = pd.DataFrame(test_pd_weight)

test_pd_mean.columns = ['pd_mean_'+str(i) for i in range(256)]
test_pd_unweight.columns = ['pd_unweight_'+str(i) for i in range(256)]
test_pd_weight.columns = ['pd_weight_'+str(i) for i in range(256)]
```


```python
tt = df_test.groupby('CLNT_ID')['PD_BRA_NM'].value_counts()[df_test['CLNT_ID'].unique()]
kk = df_test['CLNT_ID'].unique()
ww = df_test.groupby(['CLNT_ID', 'PD_BRA_NM'])['KWD_NM'].count()
```


```python
bra_mean_vector = []

for words in tqdm(train_bra_w2v[-len(test_bra_w2v):]):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += bra_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        bra_mean_vector.append(tmp)
        
bra_mean_vector = np.array(bra_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:02<00:00, 39453.33it/s]



```python
bra_unweight_vector = []

for i, words in tqdm(enumerate(train_bra_w2v[-len(test_bra_w2v):])):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += bra_w2v[word] * tt[kk[i]][word]
                cnt += tt[kk[i]][word]
            except:
                pass
        #tmp /= cnt 
        bra_unweight_vector.append(tmp)
        
bra_unweight_vector = np.array(bra_unweight_vector)
```

    113104it [10:29, 179.56it/s]



```python
bra_weight_vector = []

for i, words in tqdm(enumerate(train_bra_w2v[-len(test_bra_w2v):])):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += bra_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        bra_weight_vector.append(tmp)
        
bra_weight_vector = np.array(bra_weight_vector)
```

    113104it [06:47, 277.46it/s]



```python
test_bra_mean = bra_mean_vector
test_bra_unweight = bra_unweight_vector
test_bra_weight = bra_weight_vector

test_bra_mean = pd.DataFrame(test_bra_mean)
test_bra_unweight = pd.DataFrame(test_bra_unweight)
test_bra_weight = pd.DataFrame(test_bra_weight)

test_bra_mean.columns = ['bra_mean_'+str(i) for i in range(128)]
test_bra_unweight.columns = ['bra_unweight_'+str(i) for i in range(128)]
test_bra_weight.columns = ['bra_weight_'+str(i) for i in range(128)]
```


```python
tt = df_test.groupby('CLNT_ID')['KWD_NM'].value_counts()[df_test['CLNT_ID'].unique()]
kk = df_test['CLNT_ID'].unique()
ww = df_test.groupby(['CLNT_ID', 'KWD_NM'])['KWD_NM'].count()
```


```python
kwd_mean_vector = []

for words in tqdm(train_kwd_w2v[-len(test_kwd_w2v):]):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += kwd_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        kwd_mean_vector.append(tmp)
        
kwd_mean_vector = np.array(kwd_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:04<00:00, 23558.67it/s]



```python
kwd_unweight_vector = []

for i, words in tqdm(enumerate(train_kwd_w2v[-len(test_kwd_w2v):])):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += kwd_w2v[word] * tt[kk[i]][word]
                cnt += tt[kk[i]][word]
            except:
                pass
        #tmp /= cnt 
        kwd_unweight_vector.append(tmp)
        
kwd_unweight_vector = np.array(kwd_unweight_vector)
```

    113104it [27:14, 69.20it/s] 



```python
kwd_weight_vector = []

for i, words in tqdm(enumerate(train_kwd_w2v[-len(test_kwd_w2v):])):
        tmp = np.zeros(128) 
        cnt = 0
        for word in words:
            try:
                tmp += kwd_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        kwd_weight_vector.append(tmp)
        
kwd_weight_vector = np.array(kwd_weight_vector)
```

    113104it [11:43, 160.87it/s]



```python
test_kwd_mean = kwd_mean_vector
test_kwd_unweight = kwd_unweight_vector
test_kwd_weight = kwd_weight_vector

test_kwd_mean = pd.DataFrame(test_kwd_mean)
test_kwd_unweight = pd.DataFrame(test_kwd_unweight)
test_kwd_weight = pd.DataFrame(test_kwd_weight)

test_kwd_mean.columns = ['kwd_mean_'+str(i) for i in range(128)]
test_kwd_unweight.columns = ['kwd_unweight_'+str(i) for i in range(128)]
test_kwd_weight.columns = ['kwd_weight_'+str(i) for i in range(128)]
```


```python
tt = df_test.groupby('CLNT_ID')['CLAC3_NM'].value_counts()[df_test['CLNT_ID'].unique()]
kk = df_test['CLNT_ID'].unique()
ww = df_test.groupby(['CLNT_ID', 'CLAC3_NM'])['KWD_NM'].count()
```


```python
clac3_mean_vector = []

for words in tqdm(train_clac3_w2v[-len(test_clac3_w2v):]):
        tmp = np.zeros(30) 
        cnt = 0
        for word in words:
            try:
                tmp += clac3_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        clac3_mean_vector.append(tmp)
        
clac3_mean_vector = np.array(clac3_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:02<00:00, 41843.84it/s]



```python
clac3_unweight_vector = []

for i, words in tqdm(enumerate(train_clac3_w2v[-len(test_clac3_w2v):])):
        tmp = np.zeros(30) 
        cnt = 0
        for word in words:
            try:
                tmp += clac3_w2v[word] * clac3_matrix[i, clac3_nm_dic[word]]
                cnt += clac3_matrix[i, clac3_nm_dic[word]]
            except:
                pass
        #tmp /= cnt 
        clac3_unweight_vector.append(tmp)
        
clac3_unweight_vector = np.array(clac3_unweight_vector)
```

    113104it [00:04, 26316.69it/s]



```python
clac3_weight_vector = []

for i, words in tqdm(enumerate(train_clac3_w2v[-len(test_clac3_w2v):])):
        tmp = np.zeros(30) 
        cnt = 0
        for word in words:
            try:
                tmp += clac3_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        clac3_weight_vector.append(tmp)
        
clac3_weight_vector = np.array(clac3_weight_vector)
```

    113104it [06:47, 277.63it/s]



```python
test_clac3_mean = clac3_mean_vector
test_clac3_unweight = clac3_unweight_vector
test_clac3_weight = clac3_weight_vector

test_clac3_mean = pd.DataFrame(test_clac3_mean)
test_clac3_unweight = pd.DataFrame(test_clac3_unweight)
test_clac3_weight = pd.DataFrame(test_clac3_weight)

test_clac3_mean.columns = ['clac3_mean_'+str(i) for i in range(30)]
test_clac3_unweight.columns = ['clac3_unweight_'+str(i) for i in range(30)]
test_clac3_weight.columns = ['clac3_weight_'+str(i) for i in range(30)]
```


```python
tt = df_test.groupby('CLNT_ID')['CLAC2_NM'].value_counts()[df_test['CLNT_ID'].unique()]
kk = df_test['CLNT_ID'].unique()
ww = df_test.groupby(['CLNT_ID', 'CLAC2_NM'])['KWD_NM'].count()
```


```python
clac2_mean_vector = []

for words in tqdm(train_clac2_w2v[-len(test_clac2_w2v):]):
        tmp = np.zeros(10) 
        cnt = 0
        for word in words:
            try:
                tmp += clac2_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        clac2_mean_vector.append(tmp)
        
clac2_mean_vector = np.array(clac2_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:02<00:00, 46386.39it/s]



```python
clac2_unweight_vector = []

for i, words in tqdm(enumerate(train_clac2_w2v[-len(test_clac2_w2v):])):
        tmp = np.zeros(10) 
        cnt = 0
        for word in words:
            try:
                tmp += clac2_w2v[word] * clac2_matrix[i, clac2_nm_dic[word]]
                cnt += clac2_matrix[i, clac2_nm_dic[word]]
            except:
                pass
        #tmp /= cnt 
        clac2_unweight_vector.append(tmp)
        
clac2_unweight_vector = np.array(clac2_unweight_vector)
```

    113104it [00:03, 31331.93it/s]



```python
clac2_weight_vector = []

for i, words in tqdm(enumerate(train_clac2_w2v[-len(test_clac2_w2v):])):
        tmp = np.zeros(10) 
        cnt = 0
        for word in words:
            try:
                tmp += clac2_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        clac2_weight_vector.append(tmp)
        
clac2_weight_vector = np.array(clac2_weight_vector)
```

    113104it [05:47, 325.31it/s]



```python
test_clac2_mean = clac2_mean_vector
test_clac2_unweight = clac2_unweight_vector
test_clac2_weight = clac2_weight_vector

test_clac2_mean = pd.DataFrame(test_clac2_mean)
test_clac2_unweight = pd.DataFrame(test_clac2_unweight)
test_clac2_weight = pd.DataFrame(test_clac2_weight)

test_clac2_mean.columns = ['clac2_mean_'+str(i) for i in range(10)]
test_clac2_unweight.columns = ['clac2_unweight_'+str(i) for i in range(10)]
test_clac2_weight.columns = ['clac2_weight_'+str(i) for i in range(10)]
```


```python
tt = df_test.groupby('CLNT_ID')['CLAC1_NM'].value_counts()[df_test['CLNT_ID'].unique()]
kk = df_test['CLNT_ID'].unique()
ww = df_test.groupby(['CLNT_ID', 'CLAC1_NM'])['KWD_NM'].count()
```


```python
clac1_mean_vector = []

for words in tqdm(train_clac1_w2v[-len(test_clac1_w2v):]):
        tmp = np.zeros(5) 
        cnt = 0
        for word in words:
            try:
                tmp += clac1_w2v[word]
                cnt += 1
            except:
                pass
        #tmp /= cnt  
        clac1_mean_vector.append(tmp)
        
clac1_mean_vector = np.array(clac1_mean_vector)
```

    100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:01<00:00, 57800.50it/s]



```python
clac1_unweight_vector = []

for i, words in tqdm(enumerate(train_clac1_w2v[-len(test_clac1_w2v):])):
        tmp = np.zeros(5) 
        cnt = 0
        for word in words:
            try:
                tmp += clac1_w2v[word] * clac1_matrix[i, clac1_nm_dic[word]]
                cnt += clac1_matrix[i, clac1_nm_dic[word]]
            except:
                pass
        #tmp /= cnt 
        clac1_unweight_vector.append(tmp)
        
clac1_unweight_vector = np.array(clac1_unweight_vector)
```

    113104it [00:02, 38104.20it/s]



```python
clac1_weight_vector = []

for i, words in tqdm(enumerate(train_clac1_w2v[-len(test_clac1_w2v):])):
        tmp = np.zeros(5) 
        cnt = 0
        for word in words:
            try:
                tmp += clac1_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word]
                cnt += (ww[kk[i]] / ww[kk[i]].sum())[word]
            except:
                pass
        #tmp /= cnt 
        clac1_weight_vector.append(tmp)
        
clac1_weight_vector = np.array(clac1_weight_vector)
```

    113104it [04:28, 421.84it/s]



```python
test_clac1_mean = clac1_mean_vector
test_clac1_unweight = clac1_unweight_vector
test_clac1_weight = clac1_weight_vector

test_clac1_mean = pd.DataFrame(test_clac1_mean)
test_clac1_unweight = pd.DataFrame(test_clac1_unweight)
test_clac1_weight = pd.DataFrame(test_clac1_weight)

test_clac1_mean.columns = ['clac1_mean_'+str(i) for i in range(5)]
test_clac1_unweight.columns = ['clac1_unweight_'+str(i) for i in range(5)]
test_clac1_weight.columns = ['clac1_weight_'+str(i) for i in range(5)]
```


```python
data_test_concat = pd.concat([data_test_concat.iloc[:, 1:11], mat_clac1, mat_buy_clac1, mat_clac2, mat_buy_clac2, mat_clac3, mat_buy_clac3, test_pd_mean, test_bra_mean, test_kwd_mean, test_clac1_mean, test_clac2_mean, test_clac3_mean, test_pd_unweight, test_bra_unweight, test_kwd_unweight, test_clac1_unweight, test_clac2_unweight, test_clac3_unweight, test_pd_weight, test_bra_weight, test_kwd_weight, test_clac1_weight, test_clac2_weight, test_clac3_weight], axis=1)
```


```python
data_test_concat.to_csv('test.csv', index=False)
```


```python
data_test_concat = pd.read_csv('test.csv')
test_stat = pd.read_csv('stat_test_df.csv')
```


```python
data_test_concat = pd.concat([test_stat.iloc[:, 1:], data_test_concat.iloc[:, 10:]], axis=1)
```


```python
X_test_scaled = scaler.transform(data_test_concat)
```


```python
pred1 = model1.predict(X_test_scaled[:, :-557])
pred2 = model2.predict(X_test_scaled[:, :-557])
pred3 = model3.predict(X_test_scaled[:, :-557])
pred4 = model4.predict(X_test_scaled[:, :-557])
pred5 = model5.predict(X_test_scaled[:, :-557])
pred6 = model6.predict(X_test_scaled[:, :-557])
pred7 = model7.predict(X_test_scaled[:, :-557])
pred8 = model8.predict(X_test_scaled[:, :-557])
pred9 = model9.predict(X_test_scaled[:, :-557])
pred10 = model10.predict(X_test_scaled[:, :-557])
```


```python
pred_mlp = (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10) / 10
```


```python
pred_lgb = np.zeros((len(X_test_scaled), 6))
for i in range(n_model):
    pred_lgb += model_dict['model_'+str(i)].predict(X_test_scaled)
pred_lgb = pred_lgb / n_model   
```


```python
pred_lgb
```




    array([[0.01483922, 0.75846819, 0.12915512, 0.00084974, 0.06176788,
            0.03491985],
           [0.49494485, 0.2411636 , 0.16966314, 0.05907518, 0.01124558,
            0.02390764],
           [0.04323738, 0.2232262 , 0.49311356, 0.01315682, 0.09265073,
            0.13461531],
           ...,
           [0.05330672, 0.37161363, 0.31219931, 0.02595297, 0.09278512,
            0.14414225],
           [0.17001405, 0.47227626, 0.16736071, 0.05017716, 0.09524455,
            0.04492727],
           [0.0470015 , 0.20116348, 0.6817618 , 0.00733369, 0.01636128,
            0.04637825]])




```python
pred = (pred_mlp + pred_lgb) / 2
```


```python
pred
```




    array([[0.03059318, 0.72713347, 0.1353143 , 0.00091057, 0.07431102,
            0.03173749],
           [0.57986506, 0.20258786, 0.12928684, 0.06264269, 0.01032955,
            0.01528801],
           [0.04360476, 0.2405963 , 0.4654439 , 0.01737814, 0.09299009,
            0.13998683],
           ...,
           [0.04565929, 0.35647236, 0.34295776, 0.01862472, 0.08863805,
            0.14764782],
           [0.13791403, 0.45997391, 0.23494042, 0.0298647 , 0.08869868,
            0.04860827],
           [0.05066791, 0.18960662, 0.67846334, 0.00921454, 0.01420041,
            0.05784716]])




```python
y_test = pd.read_csv('./sample_submission.csv')
```


```python
y_test.iloc[:, 1:] = pred
```


```python
y_test
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CLNT_ID</th>
      <th>F20</th>
      <th>F30</th>
      <th>F40</th>
      <th>M20</th>
      <th>M30</th>
      <th>M40</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>0.030593</td>
      <td>0.727133</td>
      <td>0.135314</td>
      <td>0.000911</td>
      <td>0.074311</td>
      <td>0.031737</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>0.579865</td>
      <td>0.202588</td>
      <td>0.129287</td>
      <td>0.062643</td>
      <td>0.010330</td>
      <td>0.015288</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>0.043605</td>
      <td>0.240596</td>
      <td>0.465444</td>
      <td>0.017378</td>
      <td>0.092990</td>
      <td>0.139987</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15</td>
      <td>0.017765</td>
      <td>0.550447</td>
      <td>0.280376</td>
      <td>0.001969</td>
      <td>0.073255</td>
      <td>0.076189</td>
    </tr>
    <tr>
      <th>4</th>
      <td>29</td>
      <td>0.552116</td>
      <td>0.329472</td>
      <td>0.091156</td>
      <td>0.007910</td>
      <td>0.011378</td>
      <td>0.007968</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>113099</th>
      <td>263089</td>
      <td>0.173268</td>
      <td>0.391064</td>
      <td>0.402903</td>
      <td>0.002203</td>
      <td>0.011206</td>
      <td>0.019356</td>
    </tr>
    <tr>
      <th>113100</th>
      <td>263097</td>
      <td>0.143175</td>
      <td>0.268077</td>
      <td>0.522866</td>
      <td>0.004085</td>
      <td>0.011733</td>
      <td>0.050065</td>
    </tr>
    <tr>
      <th>113101</th>
      <td>263098</td>
      <td>0.045659</td>
      <td>0.356472</td>
      <td>0.342958</td>
      <td>0.018625</td>
      <td>0.088638</td>
      <td>0.147648</td>
    </tr>
    <tr>
      <th>113102</th>
      <td>263099</td>
      <td>0.137914</td>
      <td>0.459974</td>
      <td>0.234940</td>
      <td>0.029865</td>
      <td>0.088699</td>
      <td>0.048608</td>
    </tr>
    <tr>
      <th>113103</th>
      <td>263100</td>
      <td>0.050668</td>
      <td>0.189607</td>
      <td>0.678463</td>
      <td>0.009215</td>
      <td>0.014200</td>
      <td>0.057847</td>
    </tr>
  </tbody>
</table>
<p>113104 rows × 7 columns</p>
</div>




```python
y_test.to_csv('./sample_submission.csv', index=False)
```


```python

```


```python

```
