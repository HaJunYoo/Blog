{
  
    
        "post0": {
            "title": "Jekyll에서 liquid warning(Liquid Exception) 처리",
            "content": "Jekyll에서 liquid warning 처리 . Liquid Exception: Liquid syntax error (line 46): Unknown tag &#39;url&#39; in markdown(md) . 예제를 작성하거나 할 때에 liquid syntax를 포함시켜야 하는 경우가 있는데.. 그대로 작성하게 되는 경우에 해당 tag가 동작을 해서 . Liquid Exception: Liquid syntax error (line 47): Tag . 과 같은 에러가 발생을 하거나 원하지 않는 동작을 하는 경우가 발생한다. 이런 경우에는 raw tag를 이용하면 된다. . Jekyll에서 liquid warning 처리하는 방법은 다음과 같습니다. . Jekyll 에서 사용되는 liquid가 {{와 }} , {% %}를 escape 문자로 사용합니다. 문서에 {{, }} 가 들어 있는 경우 jekyll engine이 경고 메시지를 출력하고, {{ … }} 사이에 있는 내용은 무시됩니다. . 문서에는 x-success={{drafts://}} 라는 문장이 들어 있습니다. . Liquid Exception: Liquid syntax error (line 46): Unknown tag &#39;url&#39; in markdown(md) . 해당 내용을 liquid parsing을 하지 않기 위해서는 문장 앞뒤로 다음과 같은 tag를 추가해 주면 warning과 출력 문제를 해결할 수 있습니다. . . 결론 . 아예 문서 시작 전에 . raw tag 를 사용하고 . 문서 끝에 raw tag 를 사용하자 . {% raw %} 문서 {% endraw %} . 혹은 markdown header에 아래와 같은 설정을 포함시키자 . render_with_liquid: false . reference . https://jekyllrb-ko.github.io/docs/liquid/tags/ .",
            "url": "https://hajunyoo.github.io/Blog/etc/2022/07/17/liquidwarning.html",
            "relUrl": "/etc/2022/07/17/liquidwarning.html",
            "date": " • Jul 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "URL Reverse를 통해 유연하게 URL 생성",
            "content": "URL Dispatcher . 장고는 urls.py를 통해 각 뷰에 대한 URL이 변경되는 유연한 URL 시스템을 갖는다. . 뷰에 대한 이름은 path에 name 파라미터를 지정하면 된다. . app_name은 앱 내의 네임스페이스 역할을 한다. post_list가 다른 앱에 존재하더라도 app_name을 통해 구분할 수 있다. . URL Reverse를 수행함으로써 개발자가 URL을 계산할 필요가 없다. . 추후에 path의 url이 변경되더라도 URL Reverse가 변경된 url을 추적하기 때문에 누락될 일이 없는 유연한 대응이 가능하다. . urls.py 정의 . # project/urls.py urlpatterns = [ # ... path(&#39;instagram/&#39;, include(&#39;instagram.urls&#39;)), ] . # instagram/urls.py app_name = &#39;instagram&#39; # app_name 설정. 네임스페이스 역할 urlpatterns = [ path(&#39;&#39;, views.post_list, name=&#39;post_list&#39;), # name 파라미터 입력. 뷰의 이름. path(&#39;&lt;int:pk&gt;/&#39;, views.post_detail, name=&#39;post_detail&#39;), ] . URL Reverse 수행하기 . instagram앱의 urls.py에서 정의한 url을 활용하는 예시 . 템플릿 예시 및 reverse, resolve_url, redirect 등의 함수를 이용해서 서버코드에서도 활용이 가능하다. . url 내용을 변경하더라도 아래 예시들과 같이 URL Reverse를 사용한 코드는 변경할 필요가 없게된다. . URL Reverse를 수행하는 4가지 함수 . url 템플릿태그 내부적으로 reverse 함수를 사용 . {% url 인자 %} . &lt;a href=&quot;instagram/{{post.pk}}&quot;&gt;포스트 리스트&lt;/a&gt; # (이런 하드코딩은 나중에 URL을 변경 시 함께 변경되어야 한다.) &lt;a href=&quot;{% url &#39;instagram:post_list&#39; %}&quot;&gt;포스트 리스트&lt;/a&gt; - # instagram앱에서 post_detail에 대한 pk=100일 때의 URL을 가져온다. # (`instagram/100`)파라미터 이름(pk)는 생략이 가능하다. &lt;a href=&quot;{% url &#39;instagram:post_detail&#39; 100 %}&quot;&gt;포스트 리스트&lt;/a&gt; &lt;a href=&quot;{% url &#39;instagram:post_detail&#39; pk=100 %}&quot;&gt;포스트 리스트&lt;/a&gt; &lt;a href=&quot;{% url &#39;instagram:post_detail&#39; post.pk %}&quot;&gt;포스트 리스트&lt;/a&gt; . | reverse 함수 . 반환값 : URL 문자열 . args 또는 kwargs라는 파라미터 이름을 지정해서 정해진 타입으로 입력해야 한다. . (args: 리스트 - 여러 개일 경우 파라미터 순서대로 지정, kwargs: 딕셔너리 - 파라미터 이름 지정) . 매칭 URL이 없으면 NoReverseMatch 예외 발생 . from django.urls import reverse reverse(&#39;instagram:post_detail&#39;, args=[100]) # &#39;/instagram/100/&#39; 반환 reverse(&#39;instagram:post_detail&#39;, kwargs={&#39;pk&#39;: 100}) # &#39;/instagram/100/&#39; 반환 . | resolve_url 함수 . 반환값 : URL 문자열 . reverse를 래핑해서 편리하게 사용할 수 있도록 만든 함수. . 동적으로 여러개의 파라미터를 입력하거나 파라미터 이름을 지정해서 입력한다. . 매핑 URL이 없으면 “인자 문자열”을 그대로 리턴 내부적으로 reverse 함수를 사용 . from django.shortcuts import resolve_url resolve_url(&#39;instagram:post_detail&#39;, 100) # &#39;/instagram/100/&#39; 반환 resolve_url(&#39;instagram:post_detail&#39;, pk=100) # &#39;/instagram/100/&#39; 반환 resolve_url(&#39;/instagram/100/&#39;) # &#39;/instagram/100/&#39; 반환 . | redirect 함수 . 반환값 : HttpResponse(301 or 302) 인스턴스를 반환 뷰에서 특정 로직에 대한 리다이렉트 응답이 필요할 경우 사용가능. . 매칭 URL이 없으면 “인자 문자열”을 그대로 URL로 사용 내부적으로 resolve_url 함수를 사용 . from django.shortcuts import redirect redirect(&#39;instagram:post_detail&#39;, 100) # /instagram/100/에 대한 HttpResponse 반환 redirect(&#39;instagram:post_detail&#39;, pk=100) # /instagram/100/에 대한 HttpResponse 반환 redirect(&#39;/instagram/100/&#39;) # /instagram/100/에 대한 HttpResponse 반환 . | . 모델 객체에 대한 detail 주소 계산을 위한 코드 간소화 . 인자로 모델 객체(post)를 넘겨준다 . 다음 코드를 매번 입력하는 것도 좋지만 . resolve_url(&#39;instagram:post_detail&#39;, pk=post.pk) redirect(&#39;instagram:post_detail&#39;, pk=post.pk) {% url &#39;instagram:post_detail&#39; post.pk %} . 아래와 같이 사용할 수 있다. . resolve_url(post) # 파이썬 상 redirect(post) # 파이썬 상 **{{ post.get_absolute_url }} # 템플릿 문법** . 모델 클래스에 **get_absolute_url()** 구현 . 모델 클래스를 만들면 내부 클래스 메소드로 get_absolute_url 메서드를 구현하면 된다. . resolve_url 함수는 가장 먼저 get_absolute_url() 함수의 존재 여부를 확인하고 존재한다면 reverse를 수행하지 않고 그 리턴값을 즉시 리턴하게 되어 있다. . redirect 또한 내부적으로는 resolve_url을 사용하기에 get_absolute_url() 구현 여부에 따라 영향을 받는다. . class Post(models.Model): # ... def get_absolute_url(self): return reverse(&#39;instagram:post_detail&#39;, args=[self.pk]) . 기타 . CreateView, UpdateView와 같은 Generic CBV에서 success_url을 지정할 수 있다. . 만약 지정하지 않았을 경우, 해당 모델 인스턴스의 get_absolute_url() 주소로 이동 가능한지 여부를 체크하고, 이동이 가능할 경우 이동한다. . | 특정 모델에 대한 Detail 뷰를 작성할 경우, Detail view에 대한 url configure 설정을 하자마자, get_absolute_url()을 구현할 것을 추천한다. 코드가 간결해 진다. . | . . Reference . https://velog.io/@joje/URL-Reverse를-통해-유연하게-URL-생성하기 . **파이썬/장고 웹서비스 개발 완벽 가이드 with AskCompany - Inflearn .",
            "url": "https://hajunyoo.github.io/Blog/django/2022/07/17/reverse-django.html",
            "relUrl": "/django/2022/07/17/reverse-django.html",
            "date": " • Jul 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "미로 탈출",
            "content": "백준 2178 - 미로탐색 문제 . 문제 . N×M크기의 배열로 표현되는 미로가 있다. . 미로에서 1은 이동할 수 있는 칸을 나타내고, 0은 이동할 수 없는 칸을 나타낸다. 이러한 미로가 주어졌을 때, (1, 1)에서 출발하여 (N, M)의 위치로 이동할 때 지나야 하는 최소의 칸 수를 구하는 프로그램을 작성하시오. 한 칸에서 다른 칸으로 이동할 때, 서로 인접한 칸으로만 이동할 수 있다. . 위의 예에서는 15칸을 지나야 (N, M)의 위치로 이동할 수 있다. 칸을 셀 때에는 시작 위치와 도착 위치도 포함한다. . 입력 . 첫째 줄에 두 정수 N, M(2 ≤ N, M ≤ 100)이 주어진다. 다음 N개의 줄에는 M개의 정수로 미로가 주어진다. 각각의 수들은 붙어서 입력으로 주어진다. . 출력 . 첫째 줄에 지나야 하는 최소의 칸 수를 출력한다. 항상 도착위치로 이동할 수 있는 경우만 입력으로 주어진다. . 상하좌우 방향으로 체크를 해준다 . 입력 4 6 101111 101010 101011 111011 출력 15 . . from collections import deque n, m = map(int, input().split()) # 4방향 방향 벡터 dy = (0, 0, 1, -1) dx = (1, -1, 0, 0) board = [] for _ in range(n): board.append(list(map(int, input()))) def bfs(x, y): # 덱을 생성 queue = deque() # 덱에 현재 좌표을 대입 queue.append((x,y)) # 큐에 원소가 있을 경우 while queue : # pop left x, y = queue.popleft() # 방향 탐색 for i in range(4): nx = x + dx[i] ny = y + dy[i] # 유효성 검사 if nx &lt; 0 or ny &lt; 0 or nx &gt;= n or ny &gt;= m : continue # 만약 0이면 스킵 if board[nx][ny] == 0 : continue # 1이면 방문 if board[nx][ny] == 1 : # 방문한 곳에 1 증가 board[nx][ny] = board[x][y] + 1 # 큐 끝에 방문한 곳 추가 queue.append((nx, ny)) # 0,0 부터 탐색 (0,0)은 1이라서 1부터 증가시켜가면서 각 방문한 장소에 카운트를 기록하면서 이동 bfs(0, 0) # 0, 0 부터 n-1, m-1 의 위치로 가야함 -&gt; (1,1) ~ (N,M) print(board[n-1][m-1]) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/15/%EB%AF%B8%EB%A1%9C%ED%83%88%EC%B6%9C.html",
            "relUrl": "/algorithm/2022/07/15/%EB%AF%B8%EB%A1%9C%ED%83%88%EC%B6%9C.html",
            "date": " • Jul 15, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "음료수 얼려먹기(DFS)",
            "content": "문제 설명 . N × M 크기의 얼음 틀이 있다. 구멍이 뚫려 있는 부분은 0, 칸막이가 존재하는 부분은 1로 표시된다. . 구멍이 뚫려 있는 부분끼리 상, 하, 좌, 우로 붙어 있는 경우 서로 연결되어 있는 것으로 간주한다. . 이때 얼음 틀의 모양이 주어졌을 때 생성되는 총 아이스크림의 개수를 구하는 프로그램을 작성하라. . 다음의 4 × 5 얼음 틀 예시에서는 아이스크림이 총 3개가 생성된다 . . 입력 . 첫 번째 줄에 얼음 틀의 세로 길이 N과 가로 길이 M이 주어진다. (1 &lt;= N, M &lt;= 1,000) | 두 번째 줄부터 N + 1 번째 줄까지 얼음 틀의 형태가 주어진다. | 이때 구멍이 뚫려있는 부분은 0, 그렇지 않은 부분은 1이다. | . 출력 . 한 번에 만들 수 있는 아이스크림의 개수를 출력한다. . 입력 예시 1 . 4 5 00110 00011 11111 00000 . 출력 예시 1 . 3 . 입력 예시 2 . 15 14 00000111100000 11111101111110 11011101101110 11011101100000 11011111111111 11011111111100 11000000011111 01111111111111 00000000011111 01111111111000 00011111111000 00000001111000 11111111110011 11100011111111 11100011111111 . 출력 예시2 . 8 . DFS를 활용하는 알고리즘은 다음과 같습니다 특정한 지점의 주변 상, 하, 좌, 우를 살펴본 뒤에 주변 지점 중에서 값이 0 이면서 아직 방문하지 않은 지점이 있다면 해당 지점을 방문합니다. | 방문한 지점에서 다시 상, 하, 좌, 우를 살펴보면서 방문을 진행하는 과정을 반복하면, 연결된 모든 지점을 방문 할 수 있습니다. | 모든 노드에 대하여 1 ~ 2번 과정을 반복하며, 방문하지 않은 지점의 수를 카운트합니다. | . | . n, m = map(int,input().split()) # 2차원 인접 행렬 맵 정보 받기 graph = [list(map(int, input())) for _ in range(n)] # print(graph) # 방향 벡터 dx = (1, -1, 0, 0) dy = (0, 0 , 1, -1) def dfs(x, y): # 0 ~ n-1, m-1 &lt;- 주어진 범위를 벗어난 경우 False 반환 if x &lt; 0 or y &lt; 0 or x &gt;= n or y &gt;= m: return False # 현재 노드를 아직 방문하지 않았다면 if graph[x][y] == 0: # 현재 노드 방문 처리 graph[x][y] = 1 # 4가지 방향 모두 dfs 재귀 호출 for k in range(4): nx = x+ dx[k] ny = y+ dy[k] dfs(nx, ny) # 완전 탐색이 끝나면 True 반환 return True # 모두 1일 경우 False 반환 return False cnt = 0 # 행 for i in range(n) : # 열 for j in range(m) : # 현재 위치에서 DFS 수행하여 1이 나올 때까지 완전 탐색 수행 if dfs(i, j) == True : # 해당 구역 전부 1일 경우 cnt 1 증가 cnt +=1 print(cnt) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/14/%EC%9D%8C%EB%A3%8C%EC%88%98%EC%96%BC%EB%A0%A4%EB%A8%B9%EA%B8%B0.html",
            "relUrl": "/algorithm/2022/07/14/%EC%9D%8C%EB%A3%8C%EC%88%98%EC%96%BC%EB%A0%A4%EB%A8%B9%EA%B8%B0.html",
            "date": " • Jul 14, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "BFS 개념 및 구현",
            "content": "BFS (Breadth First Search) . BFS는 너비 우선 탐색이라고도 부르며, 그래프에서 가까운 노드부터 우선적으로 탐색하는 알고리즘입니다. | BFS는 큐 자료구조를 이용하며 구체적인 동작과정은 다음과 같습니다. 탐색 시작 노드를 큐에 삽입하고 방문 처리를 합니다 | 큐에서 노드를 꺼낸 뒤에 해당 노드의 인접 노드 중에서 방문하지 않은 노드를 모두 큐에 삽입하고 방문 처리합니다. | 더 이상 2번의 과정을 수행할 수 없을 때까지 반복합니다. | | 큐를 사용해서 구현 | 트리 깊이를 한단계씩 내려가면서 좌 → 우 를 훑는 방식으로 진행 | popleft을 한 노드에 연결되어 있는 노드(인접 노드)들을 큐에 append 해준다 | . BFS는 특정 조건(각 간선의 가중치가 동일)에서는 최단 경로 알고리즘 문제로써 사용되기도 한다. . . 방문 기준 : 번호가 낮은 인접 노드부터 방문 . popleft(node) 한 node 인접 노드 큐에 삽입 후 방문 처리 . queue.append(1) → [ 1 ] → 시작 노드 | popleft(1) → [2, 3, 8] | popleft(2) → [3, 8, 7] | popleft(3) → [8, 7, 4, 5] | popleft(8) → [7, 4, 5] | popleft(7) → [4, 5, 6] | popleft(4) → [5, 6] | popleft(5) → [ 6 ] | popleft(6) → [ ] | 탐색 순서 : 1 → 2 → 3 → 8 → 7 → 4 → 5 → 6 . 구현 . 다음은 2차원 인접 리스트를 사용해서 구현한 BFS. | from collections import deque # BFS 메소드 정의 def bfs(graph, start, visited): # 큐(Queue) 구현을 위해 deque 라이브러리 사용 queue = deque([start]) # 현재 노드를 방문 처리 visited[start] = True # 큐가 빌 때까지 반복 while queue: # 큐에서 하나의 원소를 뽑아 출력 -&gt; popleft로 뽑기 -&gt; FIFO v = queue.popleft() print(v, end=&#39; &#39;) # 아직 방문하지 않은 인접한 원소들을 큐에 삽입 for i in graph[v]: if not visited[i]: queue.append(i) visited[i] = True # 각 노드가 연결된 정보를 표현 (2차원 리스트) graph = [ [], [2, 3, 8], # 1번 노드 [1, 7], # 2번 노드 [1, 4, 5], # 3번 노드 [3, 5], # 4번 노드 [3, 4], # 5번 노드 [7], # 6번 노드 [2, 6, 8], # 7번 노드 [1, 7] # 8번 노드 ] # 각 노드 별 방문 정보를 담은 1차원 리스트 visited = [False] * 9 # 정의된 BFS 함수를 호출 bfs(graph, 1, visited) . 또 다른 구현 방식 → 2차원 인접 행렬을 initialize 해서 구현 | 직접 간선 연결 정보를 1로 설정해준다. → 단방향 그래프 . from collections import deque # 2차원 인접 행렬 adj = [[0]*13 for _ in range(13)] # 간선 연결 정보 -&gt; 단방향 그래프 adj[0][1] = adj[0][2] = 1 adj[1][3] = adj[1][4] = 1 def bfs(): dq = deque() dq.append(0) # 큐에 원소가 존재하는 동안 while dq : now = dq.popleft() # pop한 노드에 연결되어 있는 노드를 왼쪽부터 큐에 추가 for nxt in range(13): if adj[now][nxt]: dq.append(nxt) bfs() .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/14/BFS%EA%B0%9C%EB%85%90.html",
            "relUrl": "/algorithm/2022/07/14/BFS%EA%B0%9C%EB%85%90.html",
            "date": " • Jul 14, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "최대 공약수(재귀)",
            "content": "최대공약수 (재귀) . 두 개의 자연수에 대한 최대 공약수를 구하는 대표적인 알고리즘인 유클리드 호제법 . . 유클리드 호제법 . 두 자연수 A,B 에 대하여 (A&gt;B) A를 B로 나눈 나머지를 R이라고 합시다. | 이 때 A와 B의 최대공약수는 B와 R의 최대공약수와 같습니다. . | 유클리드 호제법의 아이디어를 그대로 재귀함수로 작성할 수 있습니다. 예시 GCD(192, 162) 1단계 : 192, 162 | 2단계 : 162, 30 | 3단계 : 30, 12 | 4단계 : 12, 6 | . | 12는 6의 배수 → 6이 최대 공약수 | . . | . # a &gt; b def gcd(a, b): # a를 b로 나눴을 때 나머지가 0이라면 if a % b == 0: # 최대 공약수는 b return b else : return gcd(b, a % b) . . 재귀 함수를 연속적으로 호출할 경우 컴퓨터 메모리 내부의 스택 프레임에 쌓입니다. . → 그래서 스택을 사용해야 할 때, 구현상 스택 라이브러리 대신에 재귀함수를 이용하는 경우가 많습니다. . ex) DFS → 스택 자료구조를 재귀 호출로 대체할 때가 있음 | . | .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/13/%EC%B5%9C%EB%8C%80%EA%B3%B5%EC%95%BD%EC%88%98.html",
            "relUrl": "/algorithm/2022/07/13/%EC%B5%9C%EB%8C%80%EA%B3%B5%EC%95%BD%EC%88%98.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "DFS 개념 및 구현",
            "content": "DFS (Depth-First Search) . DFS는 깊이 우선 탐색 이라고도 부르며 그래프에서 깊은 부분을 우선적으로 탐색하는 알고리즘입니다. | DFS는 스택 자료구조 (혹은 재귀 함수)를 이용하며, 구체적인 동작 과정은 다음과 같습니다. 재귀를 활용 자체가 스택을 활용하는 것 | 방문 기준 : 번호가 낮은 인접 노드부터 탐색 시작 노드를 스택에 삽입하고 방문 처리를 합니다. | 스택의 최상단 노드에 방문하지 않은 인접한 노드가 하나라도 있으면 그 노드를 스택에 넣고 방문 처리합니다. | 더 이상 2번의 과정을 수행할 수 없을 때까지 반복합니다. | | . | . DFS 는 완전탐색이기 때문에 모든 노드를 깊이 우선적으로 살펴봅니다. (번호가 낮은 인접 노드부터 우선적으로!) . . 0→ 1 → 2→ 3 → 4 → 5 → 6→ 7 → 8 → 9 → 10 → 11 → 12 . 순서로 탐색하게 됩니다. . 쉽게 생각하면, 최대한 계속 깊게 파고 내려간 후 올라온다 . 해당 방법을 재귀적으로 반복한다 . 이번에는 트리의 깊이가 헷갈리는 예제로 검토해보겠습니다. . . (1 → 2 → 7 → 6 → 8) → (3 → 4 → 5) . 위의 노드를 위의 순서대로 스택에 쌓으면서 탐색을 진행한다. . ### 인접 행렬로 구현 # 13 * 13 크기의 행렬 생성 adj = [[0] * 13 for _ in range(13)] # 간선 별 1 연결 부여 adj[0][1] = adj[0][7] = 1 adj[1][2] = adj[1][5] = 1 for row in adj : print(row) # 현재 방문한 노드 now를 인자로 받음 def dfs(now): # 방문한 노드 출력 print(now, end = &#39; &#39;) for nxt in range(13): # 다음으로 가는 노드가 있을 때 if adj[now][nxt]: # 다음 노드의 dfs 호출 dfs(nxt) def(0) . 이번에는 노드별 방문된 정보를 담은 리스트(visited)를 이용해서 DFS를 구현해보겠습니다. . # 각 노드가 연결된 정보를 표현 -&gt; 2차원 리스트 # 0번 노드부터 표현 -&gt; 1번 노드부터 연결이 시작 graph = [ [], [2,3,8], [1, 7], [1, 4, 5], [3, 5], [3, 4], [7], [2, 6, 8], [1, 7] ] # 각 노드가 방문된 정보를 표현 visited = [False]*9 # dfs 정의 # v(vertex)는 방문 노드 def dfs(graph, v, visited) : # 현재 노드를 방문 처리 visited[v] = True # 방문한 노드 출력 print(v, end = &#39; &#39;) # 현재 노드와 연결된 다른 노드를 재귀적으로 방문 for i in graph[v]: if not visited[i] : dfs(graph, i, visited) # 노드 1번부터 DFS 방식으로 탐색 시작 dfs(graph, 1, visited) -&gt; 1 2 7 6 8 3 4 5 .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/13/DFS%EA%B0%9C%EB%85%90.html",
            "relUrl": "/algorithm/2022/07/13/DFS%EA%B0%9C%EB%85%90.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "뷰 장식자 (View Decorators)",
            "content": "장식자 (Decorators) . 어떤 함수를 감싸는 (Wrapping) 함수 . 함수를 장식자가 wrapping 함 | 외부에서 장식한 함수를 보면 장식자가 먼저 인식됨 | 경우에 따라서 내부 로직에 들어가서 장식한 함수를 호출하기도 함 | View Decorator(뷰 데코레이터) in Django . FBV 에서의 사용 . from django.contrib.auth.decorators import login_required from django.shortcuts import render @login_required def protected_view1(request): return render(request, &#39;myapp/secret.html&#39;) def protected_view2(request): return render(request, &#39;myapp/secret.html&#39;) protected_view2 = login_required(protected_view2) . 위의 두 함수 1,2 는 동일한 기능을 합니다. . 몇 가지 장고 기본 Decorators . django.views.decorators.http . require_http_methods (request_method_list) : 뷰가 특정 요청 메소드만 허용하도록 요구 | require_GET : 뷰가 GET 메소드만 허용하도록 요구 | require_POST : 뷰가 POST 메소드만 허용하도록 요구 | require_safe : 뷰가 GET 및 HEAD 메소드만 허용하도록 요구 | 지정 method가 아닐 경우, HttpResponseNotAllowed 응답 (상태코드 405) | . django.contrib.auth.decorators . user_passes_test : 지정 함수가 False를 반환하면 login_url로 redirect | login_required : 로그아웃 상황에서 login_url로 redirect | permission_required : 지정 퍼미션이 없을 때 login_url로 redirect | . django.contrib.admin.views.decorators . staff_member_required : staff member가 아닐 경우 login_url로 이동 | . CBV에서의 decorator . 1. as_view를 이용해 함수를 만든 후 함수를 감싸줌 . from django.contrib.auth.decorators import login_required from django.views.generic import TemplateView # TemplateView를 상속 class MyTemplateView(TemplateView): template_name = &#39;core/index.html&#39; index = MyTemplate.as_view() index = login_required(index) . 2. dispatch 함수 사용 . dispatch는 클래스가 새로운 함수를 만들 때 마다 항상 실행되는 함수 | dispatch에 새로운 내용을 추가하는 것 아닌데 재정의하기 때문에 가독성 떨어뜨림 | Class의 멤버 함수에는 method_decorator를 활용 인자에도 decorator 내용을 넣는다. | . | . from django.utils.decorators import method_decorator class MyTemplateView(TemplateView): template_name = &#39;core/index.html&#39; # Class의 멤버 함수에는 method_decorator를 활용 @method_decorator(login_required) def dispatch(self, *args, **kwargs): return super().dispatch(*args, **kwargs) index = MyTmeplateView.as_view() . 3. Class에 직접 적용 ← 가장 권장 . @method_decorator에 name 지정해 직접 클래스 메소드에 대해 decorator 사용하기 | . @method_decorator(login_required, name=&#39;dispatch&#39;) class MyTemplateView(TemplateView): template_name = &#39;core/index.html&#39; index = MyTemplateView.as_view() . PS. 데코레이터를 사용하지 않고 비슷한 기능의 LoginRequiredMixin 상속 . 마찬가지로 사용하려는 데코레이터와 비슷한 기능을 가진 클래스를 상속받아 재정의 가능. | . class PostListView(LoginRequiredMixin, ListView): model = Post paginate_by = 100 post_list = PostListView.as_view() . . Reference . https://velog.io/@chldppwls12/django-view-decorators .",
            "url": "https://hajunyoo.github.io/Blog/django/2022/07/13/view-decorator.html",
            "relUrl": "/django/2022/07/13/view-decorator.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "나이트의 이동",
            "content": "백준7562 나이트의 이동 - 문제 링크 . 문제 설명 . 체스판 위에 한 나이트가 놓여져 있다. 나이트가 한 번에 이동할 수 있는 칸은 아래 그림에 나와있다. 나이트가 이동하려고 하는 칸이 주어진다. 나이트는 몇 번 움직이면 이 칸으로 이동할 수 있을까? . . 입력 입력의 첫째 줄에는 테스트 케이스의 개수가 주어진다. | 각 테스트 케이스는 세 줄로 이루어져 있다. 첫째 줄에는 체스판의 한 변의 길이 l(4 ≤ l ≤ 300)이 주어진다. 체스판의 크기는 l × l이다. 체스판의 각 칸은 두 수의 쌍 {0, …, l-1} × {0, …, l-1}로 나타낼 수 있다. 둘째 줄과 셋째 줄에는 나이트가 현재 있는 칸, 나이트가 이동하려고 하는 칸이 주어진다. | . | 출력 각 테스트 케이스마다 나이트가 최소 몇 번만에 이동할 수 있는지 출력한다. | . | . 예제 입력 3 8 0 0 7 0 100 0 0 30 50 10 1 1 1 1 예제 출력 5 28 0 . 해당 문제는 DFS로 먼저 풀었지만 정답이 나오지 않았다. 그래서 최단거리 알고리즘으로 유명한 BFS 로 풀게 되었다. 문제 풀이 아이디어 는 다음과 같다 . 8개의 지정 방향 벡터 (나이트의 이동 경우의 수) | BFS 전용 데크(deque)를 선언 popleft와 append를 이용 -&gt; FIFO 형식으로 BFS 구현 | . | 좌표에 이동 횟수 카운터를 기록하기 다음 좌표로 이동했을 때 방문한 적이 없을 경우만 카운터를 증가시키고 | 이동한 좌표를 데크(deque)에 추가 | . | 목적지에 도착했을 때 좌표에 기록된 이동횟수 출력 | . from collections import deque # 나이트가 이동할 수 있는 방향 벡터 steps = [(-2, -1), (-1, -2), (1, -2), (2, -1), (2, 1), (1, 2), (-1, 2), (-2, 1)] def bfs(): queue = deque() queue.append((a,b)) # 큐 -&gt; FIFO while queue : # 큐 가장 첫번째 인덱스 pop -&gt; 기준점으로 함 x, y = queue.popleft() # 목적지에 도착했다면 if x == c and y == d: print(visited[x][y]) return for step in steps: nx = x + step[0] ny = y + step[1] # 0 ~ n-1 if nx &lt; 0 or ny &lt; 0 or nx &gt;= n or ny &gt;= n: continue # 방문 안했을 경우만 이동 횟수 증가시키고 큐에 추가 -&gt; 이동횟수 최소화 if not visited[nx][ny] : # 이동한 좌표에 이동횟수 1 증가해서 기록 visited[nx][ny] = visited[x][y] + 1 # 이동한 좌표 큐 끝에 추가 queue.append((nx, ny)) test_num = int(input()) for _ in range(test_num): n = int(input()) visited = [[0] * n for _ in range(n)] a, b = map(int, input().split()) c, d = map(int, input().split()) bfs() .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/12/%EB%82%98%EC%9D%B4%ED%8A%B8%EC%9D%98%EC%9D%B4%EB%8F%99(BFS).html",
            "relUrl": "/algorithm/2022/07/12/%EB%82%98%EC%9D%B4%ED%8A%B8%EC%9D%98%EC%9D%B4%EB%8F%99(BFS).html",
            "date": " • Jul 12, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "왕실의 나이트",
            "content": "문제 설명 . 행복 왕국의 왕실 정원은 체스판과 같은 8 × 8 좌표 평면이다. 왕실 정원의 특정한 한 칸에 나이트가 서있다. . 나이트는 매우 충성스러운 신하로서 매일 무술을 연마한다 | 나이트는 말을 타고 있기 때문에 이동을 할 때는 L자 형태로만 이동할 수 있으며 정원 밖으로는 나갈 수 없다 | 나이트는 특정 위치에서 다음과 같은 2가지 경우로 이동할 수 있다 . 수평으로 두 칸 이동한 뒤에 수직으로 한 칸 이동하기 | 수직으로 두 칸 이동한 뒤에 수평으로 한 칸 이동하기 | . | . . 이처럼 8 × 8 좌표 평면상에서 나이트의 위치가 주어졌을 때 나이트가 이동할 수 있는 경우의 수를 출력하는 프로그램을 작성하라. 왕실의 정원에서 행 위치를 표현할 때는 1부터 8로 표현하며, 열 위치를 표현할 때는 a 부터 h로 표현한다 . c2에 있을 때 이동할 수 있는 경우의 수는 6가지이다 | a1에 있을 때 이동할 수 있는 경우의 수는 2가지이다 . | 입력 첫째 줄에 8x8 좌표 평면상에서 현재 나이트가 위치한 곳의 좌표를 나타내는 두 문자로 구성된 문자열이 입력된다. 입력 문자는 a1 처럼 열과 행으로 이뤄진다. | . | 출력 첫째 줄에 나이트가 이동할 수 있는 경우의 수를 출력하시오. | . | . 입력 예시 a1 출력 예시 2 . 문제의 아이디어는 방향벡터를 하나씩 다 정의해주는 것이다. 나이트가 갈 수 있는 경우의 수는 8가지이다. 해당 경우의 수에서 좌표 범위 안에 들어가는 경우만 count해주면 된다. . place = input() place_num = {&#39;a&#39; : 1, &quot;b&quot; : 2, &quot;c&quot; : 3, &quot;d&quot; : 4, &quot;e&quot; : 5, &quot;f&quot; : 6, &quot;g&quot; : 7, &quot;h&quot; : 8} x, y = tuple(x for x in place) # 행 x = place_num[x] # 열 y = int(y) print(x) print(y) # 나이트가 갈 수 있는 경우의 수 방향벡터 steps = [(-2, -1), (-1, -2), (1, -2), (2, -1), (2, 1), (1, 2), (-1, 2), (-2, 1)] cnt = 0 for step in steps: nx = x + step[0] ny = y + step[1] if nx &gt;= 1 and nx &lt;= 8 and ny &gt;= 1 and ny &lt;= 8: # print(f&#39;{nx}, {ny}&#39;) cnt += 1 print(cnt) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/11/%EC%99%95%EC%8B%A4%EC%9D%98-%EB%82%98%EC%9D%B4%ED%8A%B8.html",
            "relUrl": "/algorithm/2022/07/11/%EC%99%95%EC%8B%A4%EC%9D%98-%EB%82%98%EC%9D%B4%ED%8A%B8.html",
            "date": " • Jul 11, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "시각",
            "content": "문제 설명 . 정수 N이 입력되면 00시 00분 00초부터 N시 59분 59초까지의 모든 시각 중에서 3이 하나라도 포함되는 모든 경우의 수를 구하는 프로그램을 작성하시오. 예를 들어 1을 입력했을 때 다음은 3이 하나라도 포함되어 있으므로 세어야하는 시각이다. . 00시 00분 03초 00시 13분 30초 반면에 다음은 3이 하나도 포함되어 있지 않으므로 세면 안되는 시각이다. . 00시 02분 55초 01시 27분 45초 —————————– . 입력 5 출력 11475 . 해당 문제는 완전 탐색 문제 입니다 하루는 24 * 60 * 60 이므로 86400초 즉 모든 시간을 다 탐색하면서 경우의 수를 찾아도 겨우 86400번의 loop만 돌면 되기 때문에 연산은 시간 복잡도를 고려하지 않아도 1초 안에 끝납니다. (파이썬 기준 20만번 loop -&gt; 1초) 그렇기 때문에 3중 for 문을 만들어 탐색을 하며 모든 경우의 수를 검토해봅니다. . n = int(input()) count = 0 for i in range(n+1): for j in range(60): for k in range(60): if &#39;3&#39; in str(i) + str(j) + str(k): count+=1 print(count) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/11/%EC%8B%9C%EA%B0%81(%EC%99%84%EC%A0%84%ED%83%90%EC%83%89).html",
            "relUrl": "/algorithm/2022/07/11/%EC%8B%9C%EA%B0%81(%EC%99%84%EC%A0%84%ED%83%90%EC%83%89).html",
            "date": " • Jul 11, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "상하좌우",
            "content": "문제에 필요한 기술 . 시뮬리에션 및 완전 탐색에는 2차원 공간에서의 방향 벡터 기술이 자주 활용됨 | . 일반 유클리드 좌표계랑 다르다. . 세로축(행) : x축 | 가로축(열) : y축 북 쪽으로 가려면 열을 고정하고 행이 -1 되야 한다. | . | . # 동, 북, 서, 남 dx = [0, -1, 0, 1] dy = [1, 0, -1, 0] # 현재 위치 x, y = 2, 2 for i in range(4): # 다음 위치 nx = x + dx[i] ny = y + dy[i] print(nx, ny) . 문제 설명 . 여행가 A는 NxN 크기의 정사각형 공간에 서 있고, 이 공간은 1 x 1 크기의 정사각형으로 나누어져 있다. 가장 왼쪽 위 좌표는 (1, 1)이고 가장 오른쪽 아래 좌표는 (N, N)이다. 상하좌우로 이동할 수 있으며, 시작 좌표는 (1,1)이다. . 계획서대로 이동하면 되는데 L, R, U, D는 각각 왼쪽, 오른쪽, 위, 아래로 한칸씩 이동하라는 뜻이다. . 만약 공간을 벗어나는 움직임이 있다면 그 움직임은 무시하고 다음으로 넘어간다. . 입력 조건 첫째 줄에 공간의 크기를 나타내는 N이 주어진다 (1 &lt;= N &lt;= 100) | 둘째 줄에 여행가 A가 이동할 계획서 내용이 주어진다. ( 1&lt;= 이동 횟수 &lt;= 100) | . | 출력 조건 첫째 줄에 여행가 A가 최종적으로 도착할 지점의 좌표 (X, Y)를 공백으로 구분하여 출력한다. | . | . 입력 예시 5 R R R U D D 출력 예시 3 4 . n = int(input()) direction = list(map(str, input().split())) # 동 북 서 남 dx = [0, -1, 0, 1] dy = [1, 0, -1, 0] # 현재 위치 =&gt; (1,1) x, y = 1, 1 def move(dir): global x, y if dir == &quot;R&quot;: nx = x + dx[0] ny = y + dy[0] elif dir == &quot;U&quot; : nx = x + dx[1] ny = y + dy[1] elif dir == &quot;L&quot;: nx = x + dx[2] ny = y + dy[2] else : nx = x + dx[3] ny = y + dy[3] return nx, ny for direct in direction: nx, ny = move(direct) if nx &lt; 1 or ny &lt; 1 or nx &gt; n or ny &gt; n : continue # 아래의 코드를 실행하지 않고 건너 뜀 x, y = nx, ny print(f&#39;{x} {y}&#39;) &#39;&#39;&#39; 5 R R R U D D &#39;&#39;&#39; . 해당 시간 복잡도는 log(n)log(n)log(n) 이다. . 아래 코드는 move_types라는 딕셔너리를 이용해 함수를 더 간결하게 만들었다. . n = int(input()) direction = list(map(str, input().split())) # 동 북 서 남 dx = [0, -1, 0, 1] dy = [1, 0, -1, 0] move_types = {&quot;R&quot; : 0, &quot;U&quot; : 1, &quot;L&quot; : 2, &quot;D&quot; : 3} # 현재 위치 =&gt; (1,1) x, y = 1, 1 def move(dir): global x, y # 딕셔너리를 이용해 방향 dx, dy의 인덱스를 반환 move_num = move_types[dir] nx = x + dx[move_num] ny = y + dy[move_num] return nx, ny for direct in direction: nx, ny = move(direct) if nx &lt; 1 or ny &lt; 1 or nx &gt; n or ny &gt; n : continue # 아래의 코드를 실행하지 않고 건너 뜀 x, y = nx, ny print(f&#39;{x} {y}&#39;) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/11/%EC%83%81%ED%95%98%EC%A2%8C%EC%9A%B0.html",
            "relUrl": "/algorithm/2022/07/11/%EC%83%81%ED%95%98%EC%A2%8C%EC%9A%B0.html",
            "date": " • Jul 11, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "모험가 길드",
            "content": "이 문제는 [이코테 2021 - 그리디] 에 나온 문제이다. . 문제 정의 . 한 마을에 모험가가 N명 있다. 모험가 길드에서는 N명의 모험가를 대상으로 공포도를 측정했는데, 공포도가 높은 모험가는 쉽게 공포를 느껴 위험 상황에서 제대로 대처할 능력이 떨어진다. . 모험가 길드장인 동빈이는 모험가 그룹을 안전하게 구성하고자 공포도가 X인 모험가는 반드시 X명 이상으로 구성한 모험가 그룹에 참여해야 여행을 떠날 수 있도록 규정했다. . 동빈이는 최대 몇 개의 모험가 그룹을 만들 수 있는 지 궁금하다. N명의 모험가에 대한 정보가 주어졌을 때, 여행을 떠날 수 있는 그룹 수의 최댓값을 구하는 프로그램을 작성하시오. . 입력 | . 첫째 줄에 모험가의 수 N이 주어진다. (1 ≤ N ≤ 100,000) . 둘째 줄에 각 모험가의 공포도의 값을 N 이하의 자연수로 주어지며, 각 자연수는 공백으로 구분한다. . 출력 | . 여행을 떠날 수 있는 그룹 수의 최댓값을 출력한다. . 예제 입력 1 5 2 3 1 2 2 예제 출력 1 2 . 풀이 . 2개의 변수를 만드는 것이 핵심 그룹에 포함된 모험가의 수 | 그룹의 수 | . | 오름차순으로 정렬하여 그룹에 포함된 모험가의 수를 증가시키면서 반복문 수행 | 만약 현재 그룹에 포함된 모험가의 수가 현재의 공포도 이상이라면, 그룹을 결성한 후 그룹 모함가수 변수 초기화 | . n = int(input()) people = list(map(int, input().split())) people.sort() # 변수를 2개 만든다. &lt;- 핵심 # 그룹에 포함된 모험가의 수 count = 0 # 그룹의 수 result = 0 for person in people : # 공포도를 낮은 것부터 확인 count += 1 # 현재 그룹에 해당 모험가를 포함시키기 if count &gt;= person : # 현재 그룹에 포함된 모험가의 수가 현재의 공포도 이상이라면, 그룹을 결성 result += 1 # 그룹 수 count = 0 # 현재 그룹에 포함된 모험가의 수는 초기화 print(result) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/11/%EB%AA%A8%ED%97%98%EA%B0%80%EA%B8%B8%EB%93%9C.html",
            "relUrl": "/algorithm/2022/07/11/%EB%AA%A8%ED%97%98%EA%B0%80%EA%B8%B8%EB%93%9C.html",
            "date": " • Jul 11, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "럭키 스테이트",
            "content": "[문제] . 백준 18406 : 럭키스테이트 . 어떤 게임의 아웃복서 캐릭터에게는 럭키 스트레이트라는 기술이 존재한다. 이 기술은 매우 강력한 대신에 항상 사용할 수는 없으며, 현재 게임 내에서 점수가 특정 조건을 만족할 때만 사용할 수 있다. . 특정 조건이란 현재 캐릭터의 점수를 N이라고 할 때 점수 N을 자릿수를 기준으로 반으로 나누어 왼쪽 부분의 각 자릿수의 합과 오른쪽 부분의 각 자릿수의 합을 더한 값이 동일한 상황을 의미한다. 예를 들어 현재 점수가 123,402라면 왼쪽 부분의 각 자릿수의 합은 1+2+3, 오른쪽 부분의 각 자릿수의 합은 4+0+2이므로 두 합이 6으로 동일하여 럭키 스트레이트를 사용할 수 있다. . 현재 점수 N이 주어졌을 때, 럭키 스트레이트를 사용할 수 있는 상태인지 아닌지를 알려주는 프로그램을 작성하시오. 럭키 스트레이트를 사용할 수 있다면 “LUCKY”를, 사용할 수 없다면 “READY”라는 단어를 출력한다. 또한 점수 N의 자릿수는 항상 짝수 형태로만 주어진다. 예를 들어 자릿수가 5인 12,345와 같은 수는 입력으로 들어오지 않는다. . 입력 첫째 줄에 점수 N이 정수로 주어진다. (10 ≤ N ≤ 99,999,999) 단, 점수 N의 자릿수는 항상 짝수 형태로만 주어진다. . | 출력 첫째 줄에 럭키 스트레이트를 사용할 수 있다면 “LUCKY”를, 사용할 수 없다면 “READY”라는 단어를 출력한다. . | . 예제 입력 1 123402 예제 출력 1 LUCKY -- 예제 입력 2 7755 예제 출력 2 READY . . 문제 풀이 (내 풀이) . number = input() # 6 //2 = 3 -&gt; 0 ~ 2, 3 ~ 5 num = len(number)//2 a, b = number[:num], number[num:] a_list = [int(x) for x in a] b_list = [int(y) for y in b] if sum(a_list) == sum(b_list): print(&quot;LUCKY&quot;) else : print(&quot;READY&quot;) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/11/%EB%9F%AD%ED%82%A4%EC%8A%A4%ED%85%8C%EC%9D%B4%ED%8A%B8.html",
            "relUrl": "/algorithm/2022/07/11/%EB%9F%AD%ED%82%A4%EC%8A%A4%ED%85%8C%EC%9D%B4%ED%8A%B8.html",
            "date": " • Jul 11, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "곱하기 혹은 더하기",
            "content": "[문제] . 각 자리가 숫자(0부터 9)로만 이루어진 문자열 S가 주어졌을 때, 왼쪽부터 오른쪽으로 하나씩 모든 숫자를 확인하며 숫자 사이에 ‘x’ 혹은 ‘+’ 연산자를 넣어 결과적으로 만들어질 수 있는 가장 큰 수를 구하는 프로그램을 작성하세요. 단, +보다 x를 먼저 계산하는 일반적인 방식과는 달리, 모든 연산은 왼쪽에서부터 순서대로 이루어진다고 가정합니다. . 예를 들어 02984라는 문자열로 만들 수 있는 가장 큰 수는 ((((0 + 2) x 9) x 8) x 4) = 576입니다. 또한 만들어질 수 있는 가장 큰 수는 항상 20억 이하의 정수가 되도록 입력이 주어집니다. . 입력 | . 첫째 줄에 여러 개의 숫자로 구성된 하나의 문자열 S가 주어집니다. (1 &lt;= S의 길이 &lt;= 20) . 출력 | . 첫째 줄에 만들어질 수 있는 가장 큰 수를 출력합니다. . 예제 입력 1 02984 예제 출력 1 576 . 내 풀이 . 먼저 DFS 방식으로 풀어보았다. 하지만 해당 문제는 완전탐색이 아닌 그리디 문제이니 어디까지나 참고만 하는 것이 좋을 것 같다. 그렇기 때문에 완전탐색의 경우, 조건부 pruning이 필요할 것이다. 그냥 DFS를 활용해서 풀어보고 싶었다. . ## DFS 방식의 완전탐색 string = input() num_list = [int(x) for x in string] print(num_list) n = len(num_list) max_num = 0 def dfs(num, sum): global max_num # print(f&#39;{num}, {sum}&#39;) # 인덱스 끝에 도달했을 때 if num == n-1: if max_num &lt; sum : max_num = sum print(f&#39;경우의 수 : {sum}&#39;) else : temp = num_list[num+1] # 만약 두 수 중에서 하나라도 1보다 작거나 같다면 더하기 수행 if temp &lt;= 1 or num_list[num] &lt;= 1: # 더한다 dfs(num + 1, sum + temp) else : # 곱한다 dfs(num+1, sum * temp) # 더한다 dfs(num+1, sum + temp) dfs(0, num_list[0]) print(max_num) . 문제 풀이 아이디어 . 대부분의 경우 ‘+’보다는 ‘x’가 더 값을 크게 만듭니다 | 다만 두 수 중에서 하나라도 0 혹은 1인 경우 곱하기보다 더하기를 수행하는 것이 효율적입니다. | 따라서 두 수에 대하여 연산을 수행할 때, 두 수 중에서 하나라도 1 이하인 경우에는 더하며, 두 수가 모두 2 이상인 경우에는 곱하면 정답입니다. | . 단순 그리디 방식 . data = input() # 첫 번째 문자를 숫자로 변경하여 대입 result = int(data[0]) for i in range(1, len(data)): # 두 수 중에서 하나라도 0 혹은 1일 경우 곱하기보다 더하는 것이 낫다 num = int(data[i]) if num &lt;= 1 or result &lt;= 1 : result += num else : result *= num print(result) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/10/%EA%B3%B1%ED%95%98%EA%B8%B0-%EB%8D%94%ED%95%98%EA%B8%B0.html",
            "relUrl": "/algorithm/2022/07/10/%EA%B3%B1%ED%95%98%EA%B8%B0-%EB%8D%94%ED%95%98%EA%B8%B0.html",
            "date": " • Jul 10, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "1이 될 때까지",
            "content": "[문제] . 어떠한 수 N이 1이 될 때까지 다음의 두 과정 중 하나를 반복적으로 선택하여 수행하려고 한다. 단, 두 번째 연산은 N이 K로 나누어떨어질 때만 선택할 수 있다. . N에서 1을 뺀다. | N을 K로 나눈다. 예를 들어 N이 17, K가 4라고 가정하자. 이때 1번의 과정을 한 번 수행하면 N은 16이 된다. 이후에 2번의 과정을 두 번 | 수행하면 N은 1이 된다. 결과적으로 이 경우 전체 과정을 실행한 횟수는 3이 된다. 이는 N을 1로 만드는 최소 횟수이다. . N과 K가 주어질 때 N이 1이 될 때까지 1번 혹은 2번의 과정을 수행해야 하는 최소 횟수를 구하는 프로그램을 작성하시오. . &lt;입력 예시&gt; 25 5 &lt;출력 예시&gt; 2 &lt;입력 예시&gt; 17 4 &lt;출력 예시&gt; 3 . 내 풀이 . n, k = map(int, input().split()) # 26 -&gt; 5 -&gt; 1 cnt = 0 flag = True while( n%k != 0 ) : n -= 1 cnt += 1 print(f&#39;{n} : {cnt}&#39;) while(flag) : print(f&#39;{n} : {cnt}&#39;) n = n//k cnt += 1 if n == 1 : print(f&#39;{n} : {cnt}&#39;) flag = False print(cnt) . 25 5 25 : 0 5 : 1 1 : 2 2 -- 17 4 16 : 1 16 : 1 4 : 2 1 : 3 3 . 아래의 코드는 시간복잡도가 log(n)log(n)log(n) 이 나오게 된다. . n, k = map(int, input().split()) result = 0 while True: # n 이 k로 나누어 떨어지는 수가 될 때까지 빼기 target = (n // k) * k # 마지막 n이 1일 때 0 을 빼서 1이 더해짐 =&gt; 반복문을 빠져나와서 1을 빼줘야함 result += (n - target) n = target print(f&#39;n :{n} result : {result} target : {target}&#39;) # n이 k보다 작을 때 반복문 탈출 -&gt; 더 이상 나눌 수 없음 if n &lt; k: break # k로 나누기 result += 1 n //= k print(f&#39;n :{n} result : {result} target : {target}&#39;) # 마지막으로 남은 수에 대하여 1씩 빼기 result += (n - 1) print(f&#39;n :{n} result : {result} target : {target}&#39;) print(result) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/10/tothe1(greedy).html",
            "relUrl": "/algorithm/2022/07/10/tothe1(greedy).html",
            "date": " • Jul 10, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "무신사 selenium 페이지 순회",
            "content": "import numpy as np import pandas as pd import requests import time import tqdm import warnings import re import random from bs4 import BeautifulSoup from selenium import webdriver from tqdm import notebook warnings.filterwarnings(action=&#39;ignore&#39;) . 페이지를 이동하면서 무신사 홈페이지 크롤링 . . . # # 무신사 접속 # 카테고리 url 접속 url = &#39;https://www.musinsa.com/category/001001&#39; driver = webdriver.Chrome(&#39;/Users/yoohajun/Library/Mobile Documents/com~apple~CloudDocs/Hajun/scrapy/musinsa/chromedriver&#39;) driver.implicitly_wait(3) # 웹 페이지 로딩 최대 5초 대기 driver.get(url) . url을 순회할 상품 코드들을 얻어오자 . 카테고리 페이지(상의-전체)에는 상품들이 나열되어 있다 | 해당 상품들은 goods code를 가지고 있는데 | 클릭해서 상품 detail로 가보면 goods code를 가지고 url이 반복되고 있는 것을 알 수 있다. | https://www.musinsa.com/app/goods/{goods_code} | 카테고리 페이지에서 data-no 태그를 통해서 우리는 페이지 내의 상품들의 goods code를 가지고 와 리스트 형식으로 저장할 것 이다. | . # # 상품 - 페이지 번호 # 카테고리 페이지 -&gt; https://www.musinsa.com/category/001001 # css #searchList &gt; li # # searchList &gt; li:nth-child(2) # 리스트로 가져오기 # 1848166, 1921901 등등 # 인덱스 오류를 방지하기 위해 반복문을 순회할 때 예외처리 코드를 넣어준다. code_list = list() for i in range(1,91): css_selector = f&#39;#searchList &gt; li:nth-child({i})&#39; try : data_list = driver.find_element_by_css_selector(css_selector) # data-no 태그를 가져온다 data_no = data_list.get_attribute(&#39;data-no&#39;) code_list.append(data_no) except : print(&#39;data_no out of index&#39;) code_list . [&#39;1848166&#39;, &#39;1921901&#39;, &#39;996177&#39;, &#39;1841764&#39;, &#39;2442409&#39;, &#39;996178&#39;, &#39;1884943&#39;, &#39;2471760&#39;, &#39;2034137&#39;, &#39;1420730&#39;, &#39;903340&#39;, &#39;2035287&#39;, &#39;2391261&#39;, &#39;1911516&#39;, &#39;2479911&#39;, &#39;2453556&#39;, &#39;1388775&#39;, &#39;2402005&#39;, &#39;1939099&#39;, ............... ] . detail 상품 페이지 스크래핑 . # # 우선 base url을 만들어서 해당 url로 이동을 해준다 # https://www.musinsa.com/app/goods/{code_list} base_url = &quot;https://www.musinsa.com/app/goods/&quot; driver = webdriver.Chrome(&#39;/Users/yoohajun/Library/Mobile Documents/com~apple~CloudDocs/Hajun/scrapy/musinsa/chromedriver&#39;) driver.implicitly_wait(5) # 웹 페이지 로딩 최대 5초 대기 driver.get(base_url) # 리스트 - dict 형식으로 저장하자 item_list = list() . https://www.musinsa.com/app/goods/{상품 코드} | 위의 상품 코드를 미리 만들어놓은 리스트 원소를 대입하면서 반복문을 돌리자 | 상품마다 동일한 코드가 적용되기 어렵다 예외가 많기 때문에 예외 처리가 필수적으로 동반되어야한다. | 페이지마다 특정 요소들이 동일한 위치에 있지 않고 약간 변경되거나 | 어떤 요소들은 문자열 패턴이 다양해서 예외처리를 하지 않고 그대로 가져와야하는 경우도 발생한다 2022 S/S / 남 여 or 남 여 or ALL ALL / 남 | . | 예외처리는 try - except 구문을 이용해서 진행 | . | . . # # 세부 상품 크롤링을 진행하자 ## 상품 코드 번호를 순회하며 url에 대입 후 스크래핑 for idx in tqdm(code_list) : # 아이템 객체 생성 item = dict() # url을 변경해서 get해온다 item_url = base_url+idx driver.get(item_url) # url 변경 후 2초간 대기 time.sleep(2) # 상세 페이지 옷 사진 가져오기 # img css selector을 가져온 후 image = driver.find_element_by_css_selector(&#39;#detail_bigimg &gt; div.product-img &gt; img&#39;) # src 태그를 가져온다 src = image.get_attribute(&#39;src&#39;) item[&#39;src&#39;] = src print(src) # 상품 이름 product_name = driver.find_element_by_css_selector(&#39;#page_product_detail &gt; div.right_area.page_detail_product &gt; div.right_contents.section_product_summary &gt; span &gt; em&#39;).text print(product_name) # #product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(1) &gt; p.product_article_contents &gt; strong # 브랜드 이름과 품번 가져오기 product_brand = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(1) &gt; p.product_article_contents &gt; strong&#39;).text brand_id = product_brand.split(&#39;/&#39;) brand_id = [elem.strip() for elem in brand_id] brand_name = brand_id[0] product_id = brand_id[1] print(brand_name) print(product_id) item[&#39;brand_name&#39;] = brand_name item[&#39;product_id&#39;] = product_id # 시즌 정보와 성별 가져오기 season_gender = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(2) &gt; p.product_article_contents&#39;).text item[&#39;season_gender&#39;] = season_gender print(season_gender) # 상품 가격 가져오기 price = driver.find_element_by_css_selector(&#39;#goods_price&#39;).text price = re.sub(&#39;[-=.,#/?:$}원]&#39;, &#39;&#39;, price) print(price) item[&#39;price&#39;] = price # 해시태그 가져오기 ## 해시태그가 없는 상품도 존재하기 때문에 예외처리가 필요하다. ## #product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li.article-tag-list.list &gt; p try : hashtag = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li.article-tag-list.list &gt; p&#39;).text hashtag = hashtag.split(&#39; n&#39;) # 정규표현식을 통해 #(특수기호) 제거 hashtag = [re.sub(&#39;[-=.#/?:$}]&#39;, &#39;&#39;, elem) for elem in hashtag] print(hashtag) item[&#39;hashtag&#39;] = hashtag except : hashtag = None print(hashtag) item[&#39;hashtag&#39;] = hashtag # 좋야요 개수 # # product-top-like &gt; p.product_article_contents.goods_like_{상품 코드} &gt; span temp_selector = f&#39;#product-top-like &gt; p.product_article_contents.goods_like_{idx} &gt; span&#39; like = driver.find_element_by_css_selector(temp_selector).text like = re.sub(&#39;[^0-9]&#39;, &#39;&#39;, like) print(like) item[&#39;like&#39;] = like # 평점 # 평점은 list child의 위치가 다르게 나올 수 있기 때문에 예외처리를 2가지 케이스로 나눠서 해보았다. try : rate = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(6) &gt; p.product_article_contents &gt; a &gt; span.prd-score__rating&#39;).text print(rate) item[&#39;rate&#39;] = rate except : try : rate = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(7) &gt; p.product_article_contents &gt; a &gt; span.prd-score__rating&#39;).text print(rate) item[&#39;rate&#39;] = rate except: rate = None item[&#39;rate&#39;] = rate # 구매 후기 개수 -&gt; 평점과 구매 후기 개수를 곱해서 유의미한 feature을 만들어 낼 수 있을 것 같음 # 구매 후기 개수 또한 위와 같이 예외처리를 해주어야 한다. =&gt; 평점과 같은 위치에 있기 때문에 try : rate_num = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(6) &gt; p.product_article_contents &gt; a &gt; span.prd-score__review-count&#39;).text rate_num = re.sub(&#39;[^0-9]&#39;, &#39;&#39;, rate_num) print(rate_num) item[&#39;rate_num&#39;] = rate_num except : try : rate_num = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(7) &gt; p.product_article_contents &gt; a &gt; span.prd-score__review-count&#39;).text rate_num = re.sub(&#39;[^0-9]&#39;, &#39;&#39;, rate_num) print(rate_num) item[&#39;rate_num&#39;] = rate_num except : rate_num = None item[&#39;rate_num&#39;] = rate_num # 구매 현황 (purchase status) # ~18세 / 19 ~ 23세 / 24 ~ 28세 / 29 ~ 33세 / 34 ~ 39세 / 40세 ~ try : purchase_status = driver.find_element_by_css_selector(&#39;#page_product_detail &gt; div.right_area.page_detail_product &gt; div.section_graph_detail &gt; div &gt; div &gt; div.graph_bar_wrap &gt; div &gt; ul&#39;).text purchase_status = purchase_status.split(&#39; n&#39;) cleaned_purchase_status = [elem for elem in purchase_status if &#39;%&#39; in elem] print(cleaned_purchase_status) item[&#39;purchase_status&#39;] = cleaned_purchase_status except : try : purchase_status = driver.find_element_by_css_selector(&#39;#page_product_detail &gt; div.right_area.page_detail_product &gt; font &gt; font &gt; div.section_graph_detail &gt; div &gt; div &gt; div.graph_bar_wrap &gt; div &gt; ul&#39;).text purchase_status = purchase_status.split(&#39; n&#39;) cleaned_purchase_status = [elem for elem in purchase_status if &#39;%&#39; in elem] print(cleaned_purchase_status) item[&#39;purchase_status&#39;] = cleaned_purchase_status except : cleaned_purchase_status = None print(cleaned_purchase_status) item[&#39;purchase_status&#39;] = cleaned_purchase_status # 남성 구매 비율 (파이 차트) try : purchase_men = driver.find_element_by_css_selector(&#39;#graph_doughnut_label &gt; ul &gt; li:nth-child(1) &gt; dl &gt; dd&#39;).text print(purchase_men) item[&#39;purchase_men&#39;] = purchase_men except : purchase_men = None print(purchase_men) item[&#39;purchase_men&#39;] = purchase_men # 여성 구매 비율 (파이 차트) try : purchase_women = driver.find_element_by_css_selector(&#39;#graph_doughnut_label &gt; ul &gt; li:nth-child(2) &gt; dl &gt; dd&#39;).text print(purchase_women) item[&#39;purchase_women&#39;] = purchase_women except : purchase_women = None print(purchase_women) item[&#39;purchase_women&#39;] = purchase_women # 스크래핑한 딕셔너리 객체를 리스트에 추가해준다. item_list.append(item) # 랜덤하게 대기를 해준다 (1~5초 사이) driver.implicitly_wait(random.randint(1, 5)) # 구분선 print(&#39;-&#39;*20) . https://image.msscdn.net/images/goods_img/20210511/1944612/1944612_5_500.jpg?t=20220518142840 cut-heavy PIGMENT tshirts(CHARCOAL) SOVERMENT 22summer-PT-03 2022 S/S / 남 53200 [&#39;피그먼트&#39;, &#39;반팔&#39;, &#39;반팔티&#39;, &#39;티셔츠&#39;, &#39;오버핏&#39;, &#39;무지&#39;] 7797 4.9 2603 [&#39;5%&#39;, &#39;26%&#39;, &#39;34%&#39;, &#39;21%&#39;, &#39;8%&#39;, &#39;6%&#39;] 89% 11% -- https://image.msscdn.net/images/goods_img/20220329/2453552/2453552_1_500.jpg?t=20220331173345 TSHIRT FLOWERMARDI_BLACK CREAM MARDI MERCREDI 430767 2022 S/S / 여 42000 [&#39;그래픽&#39;] 20399 4.9 1755 [&#39;29%&#39;, &#39;26%&#39;, &#39;20%&#39;, &#39;9%&#39;, &#39;5%&#39;, &#39;11%&#39;] 14% 86% -- . . . . len(item_list) # json 객체의 길이는 92 -&gt; 사실 90개인데 오류 났지만 담긴 것들도 있을 것이다. . 데이터 프레임 저장 . # # 이제 리스트 딕셔너리(json) 객체를 데이터 프레임으로 변환해보자 df = pd.DataFrame(item_list) df . . .",
            "url": "https://hajunyoo.github.io/Blog/crawling/2022/07/08/%EB%AC%B4%EC%8B%A0%EC%82%AC-%EC%85%80%EB%A0%88%EB%8B%88%EC%9B%80-%EC%88%9C%ED%9A%8C.html",
            "relUrl": "/crawling/2022/07/08/%EB%AC%B4%EC%8B%A0%EC%82%AC-%EC%85%80%EB%A0%88%EB%8B%88%EC%9B%80-%EC%88%9C%ED%9A%8C.html",
            "date": " • Jul 8, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "정렬된 배열에서 특정 수의 개수 구하기",
            "content": "정렬된 배열에서 특정 수의 개수 구하기 . 입력 조건 : 첫째 줄에 N과 x가 정수 형태로 공백으로 구분되어 입력됩니다. (1 ≤ N ≤ 1,000,000), (-10⁹ ≤ x ≤ 10⁹) | 둘째 줄에 N개의 원소가 정수 형태로 공백으로 구분되어 입력됩니다. (-10⁹ ≤ 각 원소의 값 ≤ 10⁹) | . | 출력 조건 수열의 원소 중에서 값이 x인 원소의 개수를 출력합니다. 단, 값이 x인 원소가 하나도 없다면 -1을 출력합니다. | . | . 입력 . 7 2 1 1 2 2 2 2 3 | . 출력 . 4 | . 입력 . 7 4 1 1 2 2 2 2 3 | . 출력 . -1 | . . N개의 원소를 포함하고 있는 수열이 오름차순으로 정렬되어 있습니다. 이때 이 수열에서 x가 등장하는 횟수를 계산하세요. 예를 들어 수열 {1, 1, 2, 2, 2, 2, 3}이 있을 때 x = 2라면, 현재 수열에서 값이 2인 원소가 4개이므로 4를 출력합니다. . 단, 이 문제는 시간 복잡도 O(log N)으로 알고리즘을 설계하지 않으면 ‘시간 초과’ 판정을 받습니다. . 일반적인 선형탐색으로는 시간초과 판정을 받음 데이터가 정렬되어 있기 때문에 이진탐색 을 수행할 수 있음 특정 값이 등장하는 첫번째 위치와 마지막 위치를 찾아 위치 차이를 계산해 문제를 해결 가능 . . bisect 라이브러리를 이용해서 특정 값의 왼쪽과 오른쪽 인덱스를 구하자. . from bisect import bisect_left, bisect_right # 값이 left value, right value인 데이터의 개수를 반환하는 함수 def count_by_range(arr, left_value, right_value): right_index = bisect_right(arr, right_value) left_index = bisect_left(arr, left_value) return right_index - left_index n, x = map(int, input().split()) array = list(map(int, input().split())) # 값이 [x, x] 범위 안에 있는 데이터의 수 계산 count = count_by_range(array, x, x) # 값이 x인 원소가 존재하지 않을 경우 if count == 0 : print(-1) # 값이 존재할 경우 else : print(count) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/08/%ED%8A%B9%EC%A0%95%EC%88%98-%EA%B0%9C%EC%88%98.html",
            "relUrl": "/algorithm/2022/07/08/%ED%8A%B9%EC%A0%95%EC%88%98-%EA%B0%9C%EC%88%98.html",
            "date": " • Jul 8, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "바둑이 승차",
            "content": "바둑이 승차(DFS) . 철수는 그의 바둑이들을 데리고 시장에 가려고 한다. 그런데 그의 트럭은 C킬로그램 넘게 태 울수가 없다. 철수는 C를 넘지 않으면서 그의 바둑이들을 가장 무겁게 태우고 싶다. N마리의 바둑이와 각 바둑이의 무게 W가 주어지면, 철수가 트럭에 태울 수 있는 가장 무거운 무게를 구하는 프로그램을 작성하세요. . ▣ 입력설명 첫 번째 줄에 자연수 C(1&lt;=C&lt;=100,000,000)와 N(1&lt;=N&lt;=30)이 주어집니다. 둘째 줄부터 N마리 바둑이의 무게가 주어진다. . ▣ 출력설명 첫 번째 줄에 가장 무거운 무게를 출력한다. . ▣ 입력예제 1 . 259 5 81 58 42 33 61 . ▣ 출력예제 1 . 242 . 풀이 . c, n = map(int, input().split()) # 강아지의 무게를 받는 리스트 -&gt; memoization 길이와 맞추기 위해 제일 앞에 0을 더해준다. dogs = [0] + [int(input()) for _ in range(n)] # dfs는 1부터 시작하기 때문에 n+1 길이의 memoization 리스트를 생성 dp = [0]*(n+1) weight = list() def dfs(num): if num == n+1 : temp = 0 # memoization의 flag 원소값이 1일 경우 해당 인덱스 에 해당하는 강아지의 무게를 더한다. for idx, flag in enumerate(dp) : if flag == 1 : temp += dogs[idx] # temp가 c를 넘지 않지 않을 경우 if temp &lt;= c : weight.append(temp) else : # 태운다 dp[num] = 1 dfs(num+1) # 안태운다 dp[num] = 0 dfs(num+1) max_val = 0 dfs(1) print(max(weight)) . 정답 문제 풀이 . # c : 무게 제한, n : n마리의 강아지 c, n = map(int, input().split()) # dog weight memoization a = [0]*n # 가장 작은 음수 선언 result = float(&quot;-inf&quot;) for i in range(n): a[i]=int(input()) def DFS(L, sum): # 함수 내부의 result와 외부 전역변수 result를 혼동하지 않게끔 선언 -&gt; 전역변수 result를 사용할 것! global result # 만약 sum이 무게 제한 c를 넘는다면 recurrsion stop if sum &gt; c : return if L==n: if sum &gt; result : result = sum else : # 부분집합에 참여 시키겠다 DFS(L+1, sum+a[L]) # 부분집합에 참여를 시키지 않겠다 DFS(L+1, sum) # 0번째 인덱스부터 탐색 시작, sum = 0부터 시작 DFS(0,0) # 제한을 넘지 않은 최대 무게 출력 print(result) . 위의 코드는 아래의 입력이 주어졌을 경우 . 100000000 21 27 567 999 234 50 567 123 4734 754 84 35 1353 76 464 4634 65 89 3553 59 38 4135 . 위의 입력을 했을 경우 시간초과에 걸린다 . 그렇기 때문에 조금 더 pruning을 통한 시간 복잡도 갱신이 필요하다 . 중간까지의 합을 계산한 tsum이라는 변수가 필요 . tsum이라는 변수는 우리가 강아지들을 포함할지 결정하는 각 노드마다 해당되는 강아지의 무게를 더한 합이다. 부분집합처럼 포함 여부가 있는 것이 아니다. total - tsum = 앞으로 적용할 나머지 가지들에 있는 강아지들의 무게 여태까지 강아지들을 차에 싣은 무게가 sum sum - (total - tsum)은 현재 내가 sum까지의 분기점에서 앞으로 강아지들을 모두 싣었을 때 총 무게 만약 위의 값이 여태까지 기록된 최대 누적 무게(result)보다 작다면 굳이 앞으로의 탐색을 진행할 필요가 없다 . # c : 무게 제한, n : n마리의 강아지 c, n = map(int, input().split()) # dog weight memoization a = [0]*n # 가장 작은 음수 선언 result = float(&quot;-inf&quot;) for i in range(n): a[i]=int(input()) # 바둑이들의 무게 총합 total = sum(a) def DFS(L, sum, tsum): # 함수 내부의 result와 외부 전역변수 result를 혼동하지 않게끔 선언 -&gt; 전역변수 result를 사용할 것! global result # 추가 pruning -&gt; 앞으로의 무게 적재 가능성이 최대값보다 작으면 탐색 종료 if sum + (total-tsum) &lt; result : return # 만약 sum이 무게 제한 c를 넘는다면 recurrsion stop if sum &gt; c : return if L==n: if sum &gt; result : result = sum else : # 참여 여부에 상관 없이 tsum에는 무게를 더한다. # 부분집합에 참여 시키겠다 DFS(L+1, sum+a[L], tsum+a[L]) # 부분집합에 참여를 시키지 않겠다 DFS(L+1, sum, tsum+a[L]) # 0번째 인덱스부터 탐색 시작, sum = 0부터 시작 DFS(0,0,0) # 제한을 넘지 않은 최대 무게 출력 print(result) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/08/%EB%B0%94%EB%91%91%EC%9D%B4%EC%8A%B9%EC%B0%A8.html",
            "relUrl": "/algorithm/2022/07/08/%EB%B0%94%EB%91%91%EC%9D%B4%EC%8A%B9%EC%B0%A8.html",
            "date": " • Jul 8, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "나무 자르기",
            "content": "나무 자르기 . 백준 2805번 문제 . 백준 2805번 문제 . 상근이는 나무 M미터가 필요하다. 근처에 나무를 구입할 곳이 모두 망해버렸기 때문에, 정부에 벌목 허가를 요청했다. 정부는 상근이네 집 근처의 나무 한 줄에 대한 벌목 허가를 내주었고, 상근이는 새로 구입한 목재절단기를 이용해서 나무를 구할것이다. . 목재절단기는 다음과 같이 동작한다. 먼저, 상근이는 절단기에 높이 H를 지정해야 한다. 높이를 지정하면 톱날이 땅으로부터 H미터 위로 올라간다. 그 다음, 한 줄에 연속해있는 나무를 모두 절단해버린다. 따라서, 높이가 H보다 큰 나무는 H 위의 부분이 잘릴 것이고, 낮은 나무는 잘리지 않을 것이다. 예를 들어, 한 줄에 연속해있는 나무의 높이가 20, 15, 10, 17이라고 하자. 상근이가 높이를 15로 지정했다면, 나무를 자른 뒤의 높이는 15, 15, 10, 15가 될 것이고, 상근이는 길이가 5인 나무와 2인 나무를 들고 집에 갈 것이다. (총 7미터를 집에 들고 간다) 절단기에 설정할 수 있는 높이는 양의 정수 또는 0이다. . 상근이는 환경에 매우 관심이 많기 때문에, 나무를 필요한 만큼만 집으로 가져가려고 한다. 이때, 적어도 M미터의 나무를 집에 가져가기 위해서 절단기에 설정할 수 있는 높이의 최댓값을 구하는 프로그램을 작성하시오. . 입력 . 첫째 줄에 나무의 수 N과 상근이가 집으로 가져가려고 하는 나무의 길이 M이 주어진다. (1 ≤ N ≤ 1,000,000, 1 ≤ M ≤ 2,000,000,000) . 둘째 줄에는 나무의 높이가 주어진다. 나무의 높이의 합은 항상 M보다 크거나 같기 때문에, 상근이는 집에 필요한 나무를 항상 가져갈 수 있다. 높이는 1,000,000,000보다 작거나 같은 양의 정수 또는 0이다. . 출력 . 적어도 M미터의 나무를 집에 가져가기 위해서 절단기에 설정할 수 있는 높이의 최댓값을 출력한다. . 예제 입력 . 4 7 20 15 10 17 . 예제 출력 . 15 . 힌트 . 이 문제는 이진탐색을 이용한 파라메트릭 서치 문제입니다. | 적절한 높이를 찾을 때까지 이진 탐색을 수행하여 높이 H를 반복해서 조정하면 됩니다. | 현재 이 높이로 자르면 조건을 만족할 수 있는가? 조건의 만족 여부 : 예 or 아니오 | 에 따라서 탐색 범위를 좁혀서 해결할 수 있습니다 | . | 절단기의 높이는 0부터 10억까지의 정수 중 하나입니다 이렇게 큰 탐색 번위를 보면 가장 먼저 이진 탐색을 떠올리길 바랍니다 | . | . n, m = map(int, input().split()) trees = list(map(int, input().split())) # 시작점과 끝점 start = 0 end = max(trees) # 최소값 저장 변수 result = 0 # 이진 탐색 수행 while (start &lt;= end): total = 0 mid = (start+end)//2 for x in trees : if x &gt; mid : total += x - mid if total &lt; m : end = mid-1 else : result = mid start = mid+1 print(result) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/08/bj2805.html",
            "relUrl": "/algorithm/2022/07/08/bj2805.html",
            "date": " • Jul 8, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "무신사 selenium",
            "content": "import requests import time import warnings import re from bs4 import BeautifulSoup from selenium import webdriver . warnings.filterwarnings(action=&#39;ignore&#39;) . 무신사 접속 -&gt; 상의(전체보기) 카테고리 페이지로 접속을 한다. . # 무신사 접속 -&gt; 상의(전체보기) 카테고리 페이지로 접속을 한다. url = &#39;https://www.musinsa.com/category/001&#39; # 본인의 크롬 드라이버 절대주소 driver = webdriver.Chrome(&#39;본인의 크롬 드라이버 절대주소 혹은 상대주소&#39;) driver.implicitly_wait(5) # 웹 페이지 로딩 최대 5초 대기 driver.get(url) . # 가장 첫번째 옷 아이템의 사진을 클릭 driver.find_element_by_css_selector(&#39;#searchList &gt; li:nth-child(1) &gt; div.li_inner &gt; div.list_img &gt; a &gt; img&#39;).click() . # 상세 페이지 옷 사진 가져오기 # img css selector을 가져온 후 image = driver.find_element_by_css_selector(&#39;#detail_bigimg &gt; div.product-img &gt; img&#39;) # src 태그를 가져온다 src = image.get_attribute(&#39;src&#39;) print(src) . https://image.msscdn.net/images/goods_img/20210316/1848166/1848166_11_500.jpg?t=20220404173105 . # 상품 이름 product_name = driver.find_element_by_css_selector(&#39;#page_product_detail &gt; div.right_area.page_detail_product &gt; div.right_contents.section_product_summary &gt; span &gt; em&#39;).text print(product_name) . 에센셜 쿨 코튼 2-PACK 티셔츠 . # 브랜드 이름 # brand_name = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(1) &gt; p.product_article_contents &gt; strong &gt; a&#39;).text # brand_name . # #product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(1) &gt; p.product_article_contents &gt; strong # 브랜드 이름과 품번 가져오기 product_brand = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(1) &gt; p.product_article_contents &gt; strong&#39;).text brand_id = product_brand.split(&#39;/&#39;) brand_id = [elem.strip() for elem in brand_id] brand_name = brand_id[0] product_id = brand_id[1] print(brand_name) print(product_id) . COVERNAT CO0000STE1BK . # 시즌 정보와 성별 가져오기 season_gender = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(2) &gt; p.product_article_contents&#39;).text season_gender = season_gender.split(&#39;/&#39;) season_gender = [elem.strip() for elem in season_gender] season = season_gender[0] +&#39;/&#39;+ season_gender[1] gender = season_gender[2] print(season) print(gender) . 2022 S/S 남 여 . # 상품 가격 가져오기 price = driver.find_element_by_css_selector(&#39;#goods_price &gt; del&#39;).text price = re.sub(&#39;[-=.,#/?:$}원]&#39;, &#39;&#39;, price) price . &#39;49000&#39; . # 해시태그 가져오기 # #product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li.article-tag-list.list &gt; p hashtag = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li.article-tag-list.list &gt; p&#39;).text hashtag = hashtag.split(&#39; n&#39;) # 정규표현식을 통해 #(특수기호) 제거 hashtag = [re.sub(&#39;[-=.#/?:$}]&#39;, &#39;&#39;, elem) for elem in hashtag] hashtag . [&#39;반팔티셔츠&#39;, &#39;티셔츠&#39;, &#39;반팔티&#39;, &#39;오버핏반팔&#39;, &#39;에센셜라인&#39;, &#39;쿨코튼&#39;, &#39;썸머컬렉션&#39;, &#39;로고티셔츠&#39;, &#39;입시덕후&#39;, &#39;깡스타일리스트PICK&#39;] . # 좋야요 개수 # #product-top-like &gt; p.product_article_contents.goods_like_1848166 &gt; span like = driver.find_element_by_css_selector(&#39;#product-top-like &gt; p.product_article_contents.goods_like_1848166 &gt; span&#39;).text like = re.sub(&#39;[^0-9]&#39;, &#39;&#39;, like) like . &#39;126089&#39; . # 평점 rate = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(6) &gt; p.product_article_contents &gt; a &gt; span.prd-score__rating&#39;).text rate . &#39;4.8&#39; . # 구매 후기 개수 -&gt; 평점과 구매 후기 개수를 곱해서 유의미한 feature을 만들어 낼 수 있을 것 같음 rate_num = driver.find_element_by_css_selector(&#39;#product_order_info &gt; div.explan_product.product_info_section &gt; ul &gt; li:nth-child(6) &gt; p.product_article_contents &gt; a &gt; span.prd-score__review-count&#39;).text rate_num = re.sub(&#39;[^0-9]&#39;, &#39;&#39;, rate_num) rate_num . &#39;32374&#39; . # 구매 현황 (purchase status) # ~18세 / 19 ~ 23세 / 24 ~ 28세 / 29 ~ 33세 / 34 ~ 39세 / 40세 ~ purchase_status = driver.find_element_by_css_selector(&#39;#page_product_detail &gt; div.right_area.page_detail_product &gt; div.section_graph_detail &gt; div &gt; div &gt; div.graph_bar_wrap &gt; div &gt; ul&#39;).text purchase_status = purchase_status.split(&#39; n&#39;) cleaned_purchase_status = [elem for elem in purchase_status if &#39;%&#39; in elem] cleaned_purchase_status . [&#39;37%&#39;, &#39;21%&#39;, &#39;19%&#39;, &#39;11%&#39;, &#39;4%&#39;, &#39;8%&#39;] . # 남성 구매 비율 (파이 차트) purchase_men = driver.find_element_by_css_selector(&#39;#graph_doughnut_label &gt; ul &gt; li:nth-child(1) &gt; dl &gt; dd&#39;).text purchase_men . &#39;64%&#39; . # 여성 구매 비율 (파이 차트) purchase_women = driver.find_element_by_css_selector(&#39;#graph_doughnut_label &gt; ul &gt; li:nth-child(2) &gt; dl &gt; dd&#39;).text purchase_women . &#39;36%&#39; .",
            "url": "https://hajunyoo.github.io/Blog/crawling/2022/07/08/%EB%AC%B4%EC%8B%A0%EC%82%AC-%EC%85%80%EB%A0%88%EB%8B%88%EC%9B%80.html",
            "relUrl": "/crawling/2022/07/08/%EB%AC%B4%EC%8B%A0%EC%82%AC-%EC%85%80%EB%A0%88%EB%8B%88%EC%9B%80.html",
            "date": " • Jul 8, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "Scrapy pipeline을 통한 필터링 저장",
            "content": "Scrapy pipeline . Scrapy 프로젝트를 생성하면 pipelines.py가 제공되는데 해당 파일의 역활을 이해해보자. . pipelines.py 역할은 아이템 데이터 후처리하는 것에 포커스가 맞춰져 있다. 일부 아이템은 저장 | 중복되는 아이템을 저장 | 데이터베이스등에 저장 | 특별한 포멧으로 아이템을 저장 | . . | pipelines.py &amp; spider . 간단한 크롤링의 경우, 해당 spider의 parse 함수에서 pipelines.py 역할을 처리할 수 있다. 원하는 데이터만 yield를 호출하면 됩니다. | . | 다만, 복잡하고 방대한 크롤링의 경우, 별도 파일에 작성할 수 있도록 되어 있다. | . | . | . settings.py 주석 처리 해제 . settings.py 아래의 코드 주석 처리를 풀어주자. . ITEM_PIPELINES = { &#39;mycrawler.pipelines.MycrawlerPipeline&#39;: 300, } # 프로젝트가 만들어 지면 알아서 생성이 된다 # 위의 300은 우선 순위 번호로, 1000 이하의 양의 정수 중 임의로 숫자를 부여하면 됩니다, # 여러 클래스가 존재할 때, 숫자가 낮으면 낮을 수록 먼저 실행이 됩니다. . cd 명령어를 통해 mycrawler(크롤링) 폴더로 가서 아래의 명령어를 수행 scrapy crawl test_web -o test_web.json -t json 를 실행해보자. . 2022-07-07 01:47:01 [scrapy.middleware] INFO: Enabled item pipelines: [&#39;mycrawler.pipelines.MycrawlerPipeline&#39;] . 위와 같은 라인을 발견할 수 있다. 즉, 위의 settings.py 주석 처리를 해제하면 크롤링을 돌렸을 때, 파이프라인이 지원이 되는지 터미널에서 확인이 가능하다. . pipelines.py 의 역활? . from scrapy.exceptions import DropItem 을 import 해주자. 아래의 코드 양식은 저장해져 있다. . 각 아이템 생성 시, pipeline.py 에 있는 process_item을 호출 | 필요한 아이템만 return해준 후, 필터링할 아이템은 raise DropItem(&#39;메세지&#39;)을 통해 처리하지 않도록 해줌 | 크롤링 -&gt; 후처리 -&gt; 원하는 아이템만 필터하여 저장이 가능 | from scrapy.exceptions import DropItem class MycrawlerPipeline(object): def process_item(self, item, spider): if item[&#39;product_type&#39;] == &#39;제외하고 싶은 아이템 타입&#39;: raise DropItem(&#39;drop item for hanger door&#39;) else: return item . 다음과 같이 실행 후, 문자열로 지정한 상품은 저장되지 않음을 확인할 수 있습니다. .",
            "url": "https://hajunyoo.github.io/Blog/crawling/2022/07/06/scrapy_pipeline.html",
            "relUrl": "/crawling/2022/07/06/scrapy_pipeline.html",
            "date": " • Jul 6, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "합이 같은 부분집합 (DFS 아마존 인터뷰)",
            "content": "합이 같은 부분집합(DFS : 아마존 인터뷰) . N개의 원소로 구성된 자연수 집합이 주어지면, 이 집합을 두 개의 부분집합으로 나누었을 때, . 두 부분 집합의 원소의 합이 서로 같은 경우가 존재하면 “YES”를 출력하고, 그렇지 않으면 “NO”를 출력하는 프로그램을 작성하시오. . 예를 들어 {1, 3, 5, 6, 7, 10}이 입력되면 {1, 3, 5, 7} = {6, 10} 으로 두 부분 집합의 합이 16으로 같은 경우가 존재하는 것을 알 수 있다. . 입력 설명 첫번째 줄에 자연수 N(1≤ N ≤ 10) 이 주어집니다. | 두번째 줄에 집합의 원소 N개가 주어진다. 각 원소는 중복되지 않는다. | . | 출력 설명 첫번째 줄에 “YES” 또는 “NO”를 출력한다. | . | 입력 예제 1 6 | 1 3 5 6 7 10 | . | 출력 예제 1 YES | . | . . Total은 입력받은 수들의 총합 . DFS(index, sum) . 왼쪽 매개변수(index)는 리스트의 index . 오른쪽 매개변수 sum은 index까지의 부분 집합들의 합이다. . 함수는 매 리스트 인덱스 분기점마다 해당 리스트의 원소를 add or not 을 선택하면서 리스트의 끝까지 탐색한다. . 리스트의 끝까지 탐색을 했고 아래의 조건을 만족하였을 때, . Sum == Total - sum . ⇒ YES . ⇒ 프로그램을 종료 . . import sys n = int(input()) a = list(map(int, input().split())) total = sum(a) def DFS(L, sum): if L == n : # 트리의 끝까지 탐색을 해서 인덱스가 오버되었을 때 if sum == (total - sum) : # 두 부분 집합이 같다면 &quot;Yes&quot;를 출력하고 프로그램 종료 print(&quot;yes&quot;) sys.exit(0) # 아예 프로그램을 종료 -&gt; process finished with exit code 0 else : DFS(L+1, sum+a[L]) # L번 인덱스의 원소를 합하겠다 DFS(L+1, sum) # L번 인덱스의 원소를 합하지 않겠다. DFS(0, 0) print(&quot;No&quot;) # 참이 되는 경우가 없을 때 함수를 재귀적으로 다 실행시키고 와서 해당 &quot;No&quot;를 출력 . 모든 분기를 다 찾아보지 않는 것으로 시간 복잡도를 낮출 수 있다. . import sys n = int(input()) a = list(map(int, input().split())) total = sum(a) def DFS(L, sum): if sum &gt; total//2 : # 가지를 더 뻗어나갈 필요가 없다. -&gt; 시간 복잡도를 더 아낄 수 있다. return if L == n : if sum == (total - sum) : print(&quot;yes&quot;) sys.exit(0) else : DFS(L+1, sum+a[L]) # L번 인덱스의 원소를 합하겠다 DFS(L+1, sum) # L번 인덱스의 원소를 합하지 않겠다. DFS(0, 0) print(&quot;No&quot;) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/06/subsetsum.html",
            "relUrl": "/algorithm/2022/07/06/subsetsum.html",
            "date": " • Jul 6, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "부분집합 구하기(DFS)",
            "content": "부분 집합 구하기(DFS) . 자연수 N이 주어지면 1부터 N까지의 원소를 갖는 집합의 부분집합을 모두 출력하는 프로그램을 작성하세요 . 입력 설명 첫번째 줄에 자연수 N(1≤ N ≤ 10)이 주어집니다. | . | 출력 설명 첫 번째 줄부터 각 줄에 하나씩 아래와 같이 출력한다. 출력순서는 깊이 우선 탐색 전위 순회 방식으로 출력합니다. 단 공집합은 출력하지 않습니다. | . | 입력 예제 3 | . | 출력 예제 1 2 3 | 1 2 | 1 3 | 1 | 2 3 | 2 | 3 | . | . . DFS를 잘하려면 다음과 같은 상태트리를 잘 사용하면 된다. . D(1) → D(2) 를 사용하냐 or 사용하지 않나… 로 나눈다. . . 모든 노드를 왼쪽 자식, 오른쪽 자식이 아닌 번호로 생각한다 . (v, v2 , v2+1) 이 아닌 (v, v+1, v+1) . 빈 리스트를 만들어 해당 인덱스를 사용하면 1로 표시해주고 아닐 시 0으로 표시해준다. . 깊이우선 탐색 방식으로 . v가 4가 되면 (인덱스 초과), 인덱스가 1인 리스트의 원소만 출력을 해준다. . ch_index 1 2 3 . ch | 1 | 1 | 1 | . ⇒ print ⇒ 1 2 3 . 다시 백트래킹해서 D(3)으로 돌아와 인덱스 3을 사용하지 않는다로 표시해준다. . 그리고 v+1 해줘서 v가 4가 되어 인덱스 초과 시 해당 리스트를 출력해준다. . ch_index 1 2 3 . ch | 1 | 1 | 0 | . ⇒ print ⇒ 1 2 . D(1, 2) = 1 일 때, D(3)의 경우의 수가 다 해결되었다면 백트래킹으로 D(2)로 돌아와 아래의 리스트를 해결하고 . 해당 방식을 반복해준다. . ch_index 1 2 3 . ch | 1 | 0 | 1, 0 | . n = int(input()) ch = [0]*(n+1) def DFS(v): if v == n+1 : for idx, elem in enumerate(ch) : # 원소가 1인 경우만 인덱스 출력 if elem != 0 : print(idx, end = &#39; &#39;) print() else : ch[v] = 1 # 사용한다 DFS(v+1) ch[v] = 0 # 시용하지 않는다 DFS(v+1) DFS(1) # 1부터 스택을 쌓아서 입력을 넘지 않을 때까지 깊이 우선 탐색을 진행한다. .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/06/subset-dfs.html",
            "relUrl": "/algorithm/2022/07/06/subset-dfs.html",
            "date": " • Jul 6, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "이진트리 순회(DFS)",
            "content": "이진트리 순회 (DFS) . 아래 그림과 같은 이진트리를 전위순회와 후위순회를 연습하자. . . 트리는 루트를 기반으로 왼쪽 자식부터 오른쪽 자식으로 탐색을 한다. . 해당 탐색을 깊이 우선적으로 탐색한다. . 전위순회는 . ( 루트(부모) - 왼쪽 자식 - 오른쪽 자식) ⇒ 재귀적으로 반복 . 전위순회 출력 : . (1-(2-4-5)-(3-6-7)) . 중위 순회는 . (왼쪽 자식 - 부모 - 오른쪽 자식) ⇒ 재귀적으로 반복한다 . 중위순회 출력 : . ((4- 2- 5) - 1 - (6- 3- 7)) . 후위 순회는 . (왼쪽 자식 - 오른쪽 자식 - 부모 ) ⇒ 재귀적으로 반복한다 . 후위순회 출력 : . ((4-5-2) - (6-7-3) - 1) . . 부모(n)를 기준으로 . 왼쪽 자식은 2n . 오른쪽 자식은 2n+1 . . vertex ⇒ node . def DFS(v) : # v는 부모 vertex(node) if v &gt; 7 : return # 7보다 클 경우 함수 종료 else : DFS(v*2) # 왼쪽 자식 노드 호출 DFS(v*2+1) # 오른쪽 자식 노드 호출 DFS(1) # 1번 노드부터 호출 시작 . 보통 이진 트리의 node들은 리스트의 형식으로 저장해준다. . tree = [1,2,3,4,5,6,7] . 하지만 아래 코드에서는 위의 리스트를 사용하지 않을 것이다. . 전위 순회 코드 . def DFS(v) : # v는 부모 vertex(node) if v &gt; 7 : return # node가 7보다 클 경우 함수 종료 else : print(v, end = &quot; &quot;) # 부모 출력 DFS(v*2) # 왼쪽 자식 노드 호출 DFS(v*2+1) # 오른쪽 자식 노드 호출 DFS(1) # 1번 노드부터 호출 시작 # 1 2 4 5 3 6 7 . 중위 순회 코드 . def DFS(v) : # v는 부모 vertex(node) if v &gt; 7 : return # node가 7보다 클 경우 함수 종료 else : DFS(v*2) # 왼쪽 자식 노드 호출 print(v, end = &quot; &quot;) # 부모 출력 DFS(v*2+1) # 오른쪽 자식 노드 호출 DFS(1) # 1번 노드부터 호출 시작 # 4 2 5 1 6 3 7 . 후위 순회 코드 . def DFS(v) : # v는 부모 vertex(node) if v &gt; 7 : return # node가 7보다 클 경우 함수 종료 else : DFS(v*2) # 왼쪽 자식 노드 호출 DFS(v*2+1) # 오른쪽 자식 노드 호출 print(v, end = &quot; &quot;) # 부모 출력 DFS(1) # 1번 노드부터 호출 시작 # 4 5 2 6 7 3 1 . 후위 순회를 사용하는 DFS 중 대표적인 문제가 병합정렬 . 순회 방식에서 사용 빈도는 전위 » 후위 &gt; 중위 .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/05/binarytree-dfs.html",
            "relUrl": "/algorithm/2022/07/05/binarytree-dfs.html",
            "date": " • Jul 5, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "데이터 웨어하우스(Data Warehouse)란",
            "content": "데이터 웨어하우스(Data Warehouse)란 . 기업 내에 있는 각종 데이터를 적절히 뽑아내고 조합해 다양한 사업목적에 맞는 정보, 또는 지식으로 바꾸어 주는 기술 . ⇒ 기존의 데이터베이스처럼 거래처리를 위한 데이터가 아니고, 의사결정 지원을 위한 데이터 베이스 . 몇 가지 정의들 Inmon (1992) 기업의 의사결정 과정을 지원하기 위한 주제 중심적이고 통합적이며 시간성을 가지는 비휘발성 자료의 집합 | . | Kelly (1994) 기업 내의 의사결정 지원 어플리케이션들을 위한 정보기반을 제공 하는 통합된 데이터 저장공간 | TPS | DSS | . | Poe (1994) 의사결정 지원에 효과적으로 사용될 수 있도록 다양한 운영시스템으로부터 추출, 변환, 통합되고 요약된 읽기 전용 데이터베이스 | . | . | . 데이터베이스 Vs 데이터 웨어하우스 . 데이터베이스 . 거래처리 중심 | 응용프로그램 지원 | . 데이터 웨어하우스 . 지식분석 중심 | 의사결정지원시스템의 데이터베이스 | . 데이터웨어하우스의 특징 . 주제 중심적 . 기존의 데이터베이스가 응용프로그램 중심적이었다면 | 데이터베이스는 재고관리, 영업관리, 회계관리 등 기업 운영에 필요한 업무 프로세스 처리를 지원하기 위해 설계 | 데이터 웨어하우스는 기업의 의사결정을 위한 주요 주제 및 그와 관련된 데이터들이 중심 | . ◼ 통합적 구조 . ◼ 비휘발성 . 각 부서 단위로 운영하고 있는 데이터베이스에서는 추가/삭제/변경과 같은 갱신 작업이 레코드 단위로 지속적으로 발생 | 데이터 웨어하우스에서는 데이터 로드와 활용만이 존재하며, 기존 운영 시스템에서와 같은 갱신은 발생하지 않음 | . ◼ 시간성 . 기존 데이터베이스에서는 매 순간마다 발생되는 사건들을 즉시 처리 하도록 되어 있어 데이터를 접근하는 순간에만 의미가 있으나, 데이터 웨어하우스는 시간이라는 관점을 갖음 | 데이터 포인트가 시간 상의 한 포인트와 연결되어 있어 데이터 포인트들을 시간 축을 따라 비교, 분석 가능 | . 데이터 마트 . 데이터 웨어하우스와 사용자 사이의 중간층에 위치 | 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스 | 데이터 웨어하우스가 도매상이라면 데이터 마트는 소매상 | 데이터 마트의 데이터는 대부분 데이터 웨어하우스로부터 복제 | . 데이터웨어하우스에는 단순히 자료가 저장되어 있을뿐만 아니라, 이러한 자료를 추출, 저장, 분류하는 일련의 과정을 포함 . ◼ 메타 데이터 . 데이터의 데이터 | 데이터웨어하우스의 생성과 유지보수에 관련된 정보를 담고있는 자료 | . ◼ 데이터 마트 . 데이터 웨어하우스에 저장된 자료 중에서 일정한 주제나 특정 부서의 자료를 별도의 장소에 중복 저장하여 사용자들이 사용하도록 하게 한 것 | .",
            "url": "https://hajunyoo.github.io/Blog/database/2022/07/03/data_warehouse.html",
            "relUrl": "/database/2022/07/03/data_warehouse.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "온라인 분석처리(OLAP)란",
            "content": "OLAP . 온라인 분석처리(OLAP)란? . ◼ 다차원 데이터 구조를 이용하여 다차원의 복잡한 질의를 고속으로 처리하는 데이터 분석(analysis) 기술 ◼ 차원(dimensions)과 측정 항목(measure)을 설정하여 관심주제에 대한 분석을 수행 (예: “지역별/분기별/상품별(차원들)” “판매액(측정항목)” 현황분석) . ◼ 일반적으로 최종 사용자가 필요한 정보를 자료원으로부터 직접 가공하여 분석 ◼ 사용자가 분석 도중 대화식으로 여러 차원 또는 분석 기법간에 심층분석(Drill-down), 또는 축약분석(Drill-up) 가능 . . n-dimensional cube . 큐브와 셀 . ⇒ 있는 데이터를 요약하는 것 . 온라인 분석처리의 구성요소 . ◼ 드릴 다운 (Drill down) . 데이터를 어떤 하나의 차원을 기준으로 분석할 때 계층구조상의 가장 상위 수준에 해당하는 집계 데이터부터 먼저 보고, 다음 세부 수준으로 들어가며 데이터를 분석하는 것 | 예를 들어, 년도별 분석 -&gt; 반기별 분석 -&gt; 분기별 분석 -&gt; 월별 분석과 같이 점점 상세 수준으로 데이터를 분석 | . ◼ 드릴 업 (Drill up) . 드릴다운의 반대 과정으로, 상세 수준의 데이터로부터 차츰 상위 단계의 데이터를 분석해 가는 것 | 월별 분석 -&gt; 분기별 분석 -&gt; 반기별 분석 -&gt; 년도별 분석으로 분석의 범위를 넓혀 나가면서 데이터를 분석 | . ◼ 피벗(pivot) . 임의의 다차원 뷰(view)를 만들고 검토 및 분석한 후, 차원들의 다양하게 변경하여 새로운 뷰를 만들어 데이터를 분석하는 것 | 다양한 뷰를 만들기 위해 축과 축을 바꾸는 작업 | . ◼ 다차원화 된 쿼리 ◼ 설정한 디멘션에 따라 측정값(요약값) 산출해서 검토 → 의사결정에 도움을 받음 .",
            "url": "https://hajunyoo.github.io/Blog/database/2022/07/03/OLAP.html",
            "relUrl": "/database/2022/07/03/OLAP.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "NoSQL에 대하여",
            "content": "NoSQL . NoSQL의 개념 . Not-Only SQL 혹은 No SQL을 의미 | 전통적인 관계형 데이터베이스(RDBMS)와 다르게 설계된 비관계형 데이터베이스 | 스키마가 없는 데이터베이스 | 덜 제한적인 데이터 저장 및 검색 메커니즘 제공 | 비정형 데이터베이스를 사용할 때 필요한 DB | 용이한 데이터 규모 확장성 | 즉 데이터를 다수의 하드웨어에 분산해서 저장 | 대용량의 구조적, 반구조적 데이터들을 저장/분석 (웹, 소셜 미디어, 그래픽 등) | . NoSQL의 분류: 데이터 저장 방식에 따라 . 키-값(key - value) : 다이나모, 리악, 레디스, 캐시, 프로젝트 볼드모트 가장 간단한 구조 – eventual consistency 또는 serializability 의 consistency model제공 | 사용 예제 ) session 정보 제공, 사용자 profile 등 | . | 컬럼 : H베이스, 아큐물로 | 도큐먼트 : 몽고DB, 카우치베이스 JSON같은 semi-structured data 인 document를 저장/조회하는 데이터베이스 | Key-value store의 하위 클래스 | Key-value store와 달리 데이터베이스 내부에서 최적화를 지원하기 위해 document가 가진 metadata 정보 이용 mongo db 예시 . db.inventory.insertMany( [ { item: &quot;journal&quot;, qty: 25, size: { h: 14, w: 21, uom: &quot;cm&quot; }, status: &quot;A&quot; }, { item: &quot;notebook&quot;, qty: 50, size: { h: 8.5, w: 11, uom: &quot;in&quot; }, status: &quot;A&quot; }, { item: &quot;paper&quot;, qty: 100, size: { h: 8.5, w: 11, uom: &quot;in&quot; }, status: &quot;D&quot; }, { item: &quot;planner&quot;, qty: 75, size: { h: 22.85, w: 30, uom: &quot;cm&quot; }, status: &quot;D&quot; }, { item: &quot;postcard&quot;, qty: 45, size: { h: 10, w: 15.25, uom: &quot;cm&quot; }, status: &quot;A&quot; } ]); db.inventory.find( { size: { h: 14, w: 21, uom: &quot;cm&quot; } } ) db.inventory.find( { &quot;size.uom&quot;: &quot;in&quot; } ) . | . | . | 그래프: Neo4J, Infinite Graph, 알레그로그래프, 버투오소 . Graph database . 데이터를 node, edge 및 property를 가지고 graph structure를 이용하여 저장하는 데이터베이스 | Graph에서 데이터를 효율적으로 쿼리하기 위한 graph데이터베이스 전용 query language가 존재 Cypher, Gremlin | . | . | . Wide-column store (column family database) . 데이터모델 : &lt;키, 값&gt; 저장 구조 | . . 설명 : 가장 간단한 데이터 모델, 응용 프로그램 모델링이 복잡 | RDBMS 와 비슷하게 wide-column store 도 table ,column, row 개념을 사용 | RDBMS와 차이점 : 각 rows가 다른 column list를 가질 수 있다 | Two-dimensional key-value store로 생각할 수 있음 | 제품 예 : DynamoDB, 아마존 S3 | . NoSQL의 특징 . NoSQL은 CAP이론의 consistency와 availability을 동시에 제공하지 않는다 - 불가능 | 無 스키마 고정된 스키마 없이 키(Key) 값을 이용하여 다양한 형태의 데이터 저장 및 접근 기능 | 데이터 저장 방식은 크게 값(Value), 열(Column), 문서(Document), 그래프(Graph) 등의 네 가지를 기반으로 구분 | . | 탄력성(Elasticity) 시스템 일부에 장애가 발생해도 클라이언트가 시스템에 접근 가능 | 응용 시스템의 다운 타임이 없도록 하는 동시에 대용량 데이터의 생성 및 갱신 | 시스템 규모와 성능 확장이 용이하며, 입출력의 부하를 분산시키는 데 용이한 구조 | . | 쿼리(Query) 기능 수십 대에서 수천 대 규모로 구성된 시스템에서도 데이터의 특성에 맞게 효율적으로 데이터를 검색 ·처리 가능 | . | . RDBMS vs NoSQL . RDBMS는 대용량 데이터 처리 및 다양한 유형의 데이터 처리를 하는데 어려움이 존재하였음 | 강력한 수평적 확장성이 있는 NoSQL을 사용함으로써 데이터 분산 처리 및 다양한 유형의 데이터 관리가 가능해짐 | . 참고 ) ACID는 Atomicity(원자성), Consistency(일관성), Isolation(독립성), Durability(지속성) . 구분 관계형 데이터베이스(RDBMS) NoSQL . 설명 | 일관성(C)과 가용성(A)을 선택 | 일관성이나 가용성중 하나를 포기하고, 지속성(P)를 보장 | . 장점 | 데이터 무결성, 정확성 보장, 정규화된 테이블과 소규모 트랜잭션이 있음 | 웹 환경의 다양한 정보를 검색 및 저장 가능 | . 단점 | 확장성에 한계가 있음, 클라우드 분산 환경에 부적합 | 데이터의 무결성과 정확성을 보장하지 않음 | . ⇒ NoSQL 이 더 좋은 경우 . NoSQL 데이터베이스는 very large semi-structured data 를 처리하는 애플리케이션에 적합 – Log Analysis, Social Networking Feeds, Time-based data.. | 더 큰 데이터 볼륨을 처리하고 대기 시간을 줄이고 처리량을 개선하는 몇 가지 조합을 통해 데이터 액세스 성능을 개선 | 복잡한 relationship이 있는 테이블이 있는 경우 JOIN을 제공하지 않기 때문에 적합하지 않음 | .",
            "url": "https://hajunyoo.github.io/Blog/database/2022/07/03/Nosql.html",
            "relUrl": "/database/2022/07/03/Nosql.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post28": {
            "title": "CAP 이론",
            "content": "정형 데이터 . 고정된 필드에 저장된 데이터 . 데이터베이스를 설계한 사람에 의해 수집되는 정보의 형태가 정해짐 . 예를 들어 관계형 데이터 베이스의 테이블들, 스프레드시트 등 | . . 비정형 데이터란? . 미리 정해져서 고정되어 있는 필드에 저장되어 있지 않은 데이터 | 스마트 기기에서 페이스북, 트위터, 유튜브 등으로 생성되는 소셜 데이터 | IoT 환경에서 생성되는 위치 정보나 센서 데이터와 같은 사물 데이터 등 | 문서, 그림, 영상 등이 이에 해당 | . 빅데이터 시대의 주요 특징 . 데이터의 크기가 엄청나게 크다 | 데이터의 형태가 비정형적이고, 다양하다 | 빠른 처리속도가 요구된다 | . 비정형 데이터베이스 등장 배경 . 기존의 RDB나 DW와 같은 정형 데이터 베이스만으로는 해결이 어려움 | 분산 시스템으로 갈 수 밖에 없다 | 분산환경 하여서 대용량의 데이터를 신속하게 처리할 비정형 데이터베이스의 등장 NoSQL, Hadoop 등 | . | . 분산 시스템이란? . 작업이나 데이터를 여러 대의 컴퓨터에 나누어서 처리, 저장하여 그 내용이나 결과가 통신망을 통해 상호교환 되도록 연결되어 있는 시스템 | . CAP 이론이란? . 2000년경 버클리 대학 에릭 브루어(Eric Brewer)가 주창한 이론으로, 2002년경 증명 | CAP정리(CAP Theorem), 혹은 브루어 정리(Brewer’s theorem) 등 으로 불리움 | . . CAP theorem . 분산 컴퓨터 시스템에서 다음과 같은 세가지 조건을 모두 만족하는 시스템을 존재할 수 없음을 증명한 정리 . C(Consistency) : 모든 노드들이 같은 시점에 같은 값을 볼 수 있다 (또는 에러) → 한쪽이 업데이트 되면 즉시 같은 정보가 보여져야 한다 | A (Availability) : 모든 request는 에러 없이 response를 받을 수 있다. (최신 값을 return함은 보장 안함) 일부 노드가 다운되어도 다른 노드에 영향을 주지 않아야 함 | 특정 노드가 장애가나도 서비스가 가능해야 한다”라는 의미 | . | P (Partition tolerance) : network partition이 나서 message의 분실이 발생해도 시스템이 계속 동작해야 한다 구성된 분산 시스템에 노드 장애가 있어 일부 데이터를 손실 하더라도 사용자는 다른 최신 데이터에 접속하여 정상 동작할 수 있어야 한다 | . | NoSQL은 consistency와 availability을 동시에 제공하지 않는다 - 불가능 | 두 가지 만족이 가능하다 C-A, C-P, A-P | . | 최근 CAP 이론이 주목받는 이유 빅데이터 저장 문제 | . | . 기존의 RDB가 C(일관성)와 A(가용성) 중심이라면 비정형 데이터베이스는 P(지속성)을 중시 | .",
            "url": "https://hajunyoo.github.io/Blog/database/2022/07/03/CAP-theorem.html",
            "relUrl": "/database/2022/07/03/CAP-theorem.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post29": {
            "title": "Data engineer Roadmap",
            "content": "Data Engineer Roadmap 따라가기 . 데이터 엔지니어 로드맵 링크(클릭) . 데이터 엔지니어는 앱 또는 웹에서 발생하는 데이터들을 파이프라인을 통해 저장소(Database, S3,…)에 저장합니다. 대용량 데이터를 수집하고 관리하며 유지하는 일을 담당합니다. . 데이터 엔지니어 자격조건 및 공부 로드맵 . . .",
            "url": "https://hajunyoo.github.io/Blog/backend/2022/07/03/DE-roadmap.html",
            "relUrl": "/backend/2022/07/03/DE-roadmap.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post30": {
            "title": "재귀함수와 스택",
            "content": "재귀함수와 스택 . 반복문을 대체제 : 재귀 함수 . DFS라는 함수로 재귀함수와 스택을 리뷰해보자 . def DFS1(x) : if x&gt;0 : DFS1(x-1) print(x, end=&#39; &#39;) # 1 2 3 def DFS2(x) : if x&gt;0 : print(x, end=&#39; &#39;) DFS2(x-1) # 3 2 1 . 위의 DFS 1,2 는 재귀 호출 위치에 따라서 리턴값이 각 오름차순과 내림차순으로 다르다. . 메모리 영역 스택에 매개변수가 할당이 된다 . DFS(3) → 스택에 매개변수 x = 3, 지역변수, 복귀주소가 할당된다. . 이렇게 재귀로 인해 쌓인 메모리 묶음을 스택 프레임이라고 한다 . . DFS(0)이 가장 위에 쌓이는 순간 DFS(0)은 x &gt;0 조건을 성립하지 못하기 때문에 종료가 된다 . 종료가 되면서 스택 최상단에 있던 DFS(0)은 없어진다. . 지워지면서 복귀주소 ⇒ DFS(1)로 복귀한다. . def DFS(x) : if x&gt;0 : DFS(x-1) -&gt; 각 복귀 주소 print(x, end=&#39; &#39;) # 1 2 3 . 복귀 주소로 복귀한 다음 바로 다음 줄인 print(x, end = ‘ ‘) 를 수행한다. . 수행하고 종료되고 메모리가 할당 해제되면서 스택에서 사라진다.! 그리고 다시 다음 DFS(2)가 호출된다 . 그렇게 DFS(3)까지 호출되고 사라짐을 반복한다. . 그렇게 1 , 2, 3 이 출력되는 것이다. . 그런 후 스택이 텅 비면 이제 다음 코드 라인으로 이동해서 코드를 수행한다. .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/03/recursive.html",
            "relUrl": "/algorithm/2022/07/03/recursive.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post31": {
            "title": "재귀함수를 이용한 이진수 출력",
            "content": "10진수 N이 입력되면 2진수로 변환하여 출력하는 프로그램을 작성하시오. 단 재귀함수를 이용해서 출력해야 합니다. . 입력 설명 . 첫 번째 줄에 10진수 N(1≤ N ≤ 1000)이 주어집니다. . | 출력 설명 . 첫번째 줄에 이진수를 출력하세요 . | 입력 예제 1 . 11 . | 출력 예제 1 . 1011 . | . def DFS(x): if x == 0 : return else : print(x%2, end=&#39; &#39;) # x를 2로 나눈 나머지를 출력하고 DFS(x//2) # 2를 나눈 몫을 다음 재귀로 넘겨준다 . if x = 16 DFS(16) 16 % 2 = 0 &gt; 0 16 // 2 = 8 DFS(8) 8 % 2 = 0 &gt; 0 0 8 // 2 = 4 DFS(4) 4 % 2 = 0 &gt; 0 0 0 4 // 2 = 2 DFS(2) 2 % 2 = 0 &gt; 0 0 0 0 2 // 2 = 1 DFS(1) 1 % 2 = 1 &gt; ** 0 0 0 0 1 ** 1 // 2 = 0 DFS(0) if x == 0 -&gt; break .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/03/recursive-binary.html",
            "relUrl": "/algorithm/2022/07/03/recursive-binary.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post32": {
            "title": "Scrapy 실습",
            "content": "Scrapy &#49892;&#49845; . http꞉// 를 붙여주면, start_urls 에서 http꞉// 를 무조건 붙여주기 때문에, 결과적으로 http꞉// 가 두 번 붙게되어, 강제로 http꞉// 를 start_urls 에서 삭제해야 한다 . // 프로젝트를 생성하고 크롤러 모듈을 생성하자 scrapy startproject ecommerce1 scrapy genspider gmarket_best corners.gmarket.co.kr/Bestsellers . 위의 명령어를 통해 url에 맞는 크롤러 python 모듈을 생성하자 . start_urls는 def parse의 response로 들어가게 된다 . 구조를 한번 살펴보고 내려가자 | 표시를 한 것이 오늘의 핵심 파일이 될 예정이다. | . scrapy.cfg # deploy configuration file ecommerce1/ # project&#39;s Python module, you&#39;ll import your code from her __init__.py **items.py** # project items definition file pipelines.py # project pipelines file settings.py # project settings file spiders/ # a directory where you&#39;ll later put your spiders __init__.py **gmarket.py** . spiders/gmarket_best.py def parse 부분을 아래와 같이 변경 . import scrapy class GmarketBestSpider(scrapy.Spider): name = &#39;gmarket_best&#39; allowed_domains = [&#39;corners.gmarket.co.kr&#39;] start_urls = [&#39;http://corners.gmarket.co.kr/&#39;] def parse(self, response): titles = response.css(&#39;div.best-list li &gt; a::text&#39;).getall() for title in titles: print(title) . !scrapy crawl gmarket_best 을 통해 크롤링 수행할 수 있다 . items.py . 크롤링 데이터 다루기꞉ 저장하기 items.py 파일 확인해보자 &gt; items.py # project items definition file . 어떤 아이템들을 가져올건지 items.py에 선언을 해줘야 한다 . 선언이 된 아이템들을 gmarket_best.py에서 가져와 전달을 해준다. 위의 과정을 거쳐야 scrapy에서 저장을 하는 등의 작업을 수행할 수 있게 된다 . items.py | . # # See documentation in: # https://docs.scrapy.org/en/latest/topics/items.html import scrapy class Ecommerce1Item(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 저장할 데이터 이름 = scrapy.Field() 과 같이 작성 title = scrapy.Field() . items.py에 전달하기 위해서는 gmarket_best.py를 아래와 같이 수정해야 한다 . gmarket_best.py &#49688;&#51221; . import scrapy ### items.py 의 클래스명인 Ecommerce1Item 을 import 하기 from ecommerce1.items import Ecommerce1Item class GmarketSpider(scrapy.Spider): name = &#39;gmarket_best&#39; allowed_domains = [&#39;corners.gmarket.co.kr/Bestsellers&#39;] start_urls = [&#39;http://corners.gmarket.co.kr/Bestsellers/&#39;] def parse(self, response): titles = response.css(&#39;div.best-list li &gt; a::text&#39;).getall() for title in titles: item = Ecommerce1Item() #선언을 통해 item 객체 생성 # items.py 에서 정의한 scrapy.Field() 명을 동일하게 써줘야 함 item[&#39;title&#39;] = title # parsing해서 가져온 데이터를 field 열에 계속 넣어준다. yield item # yield 하는 순간 데이터가 items.py로 쌓인다 . items.py 필드명 선언 -&gt; | crawling python file에서 items.py import를 통해 불러와 item 객체 생성 -&gt; | 미리 생성해놓은 필드에 parsing해온 데이터를 yield를 통해 적재하기 | . &#45796;&#50577;&#54620; &#45936;&#51060;&#53552; format&#51004;&#47196; &#50500;&#51060;&#53596;&#46308;&#51012; &#51200;&#51109;&#54624; &#49688; &#51080;&#45796;. . csv, xml, json 포멧 | 터미널 환경에서, 크롤링을 실행 시켰던 ecommerce1/ecommerce1 폴더에서 다음 명령을 수행하자 | . // scrapy crawl 크롤러명 -o 저장할 파일명 -t 저장포멧 // 예 scrapy crawl gmarket_best -o gmarket_best.csv -t csv scrapy crawl gmarket_best -o gmarket_best.xml -t xml scrapy crawl gmarket_best -o gmarket_best.json -t json &gt; json 파일을 확인하면, 한글문자가 깨져나온다 &gt; settings.py를 수정해줘야 한다 . settings.py에 들어가서 아래의 코드를 추가해주어야 한다 . 해당 파일 안에 utf-8 encoding 설정을 추가해준다 (위치는 상관 없다) . # FEED_EXPORT_ENCODING 추가 FEED_EXPORT_ENCODING = &#39;utf-8&#39; . ! scrapy crawl gmarket_best -o gmarket_best.csv -t csv // 아래와 같이 gmarket_best.csv 파일이 생성된 것을 확인 할 수 있다. ! ls __init__.py **gmarket_best.csv** middlewares.py settings.py __pycache__ items.py pipelines.py spiders .",
            "url": "https://hajunyoo.github.io/Blog/crawling/2022/07/03/Scrapy-%EC%8B%A4%EC%8A%B5.html",
            "relUrl": "/crawling/2022/07/03/Scrapy-%EC%8B%A4%EC%8A%B5.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post33": {
            "title": "두 배열의 원소 교체",
            "content": "범위 : 1&lt;=N &lt; 100,000, 0&lt;= K &lt;=N . 예를 들어 N=5, k=3, 배열 A와 B가 다음과 같다고 해보자. 배열 A = [1, 2, 5, 4, 3] 배열 B = [5, 5, 6, 6, 5] 이 경우, 다음과 같이 세 번의 연산을 수행할 수 있습니다. 1) 배열 A의 원소 1과 배열 B의 원소 6을 바꾸기 2) 배열 A의 원소 2와 배열 B의 원소 6을 바꾸기 3) 배열 A의 원소 3과 배열 B의 원소 5를 바꾸기 세 번의 연산 이후 배열 A와 배열 B의 상태는 다음과 같이 구성될 것입니다. . 배열 A = [6,6,5,4,5] | 배열 B = [3,5,1,2,5] | . 이 때 배열 A의 모든 원소의 값이 합은 26이 되며, 이보다 더 합을 크게 만들 수 없습니다. . 여기서 메인 아이디어는 배열 A를 오름차순으로 정렬하고, 배열 B를 내림차순으로 정렬해서 같은 인덱스끼리 k번만큼 바꾸면 배열 A의 합은 최대가 될 것입니다. . n, k = map(int, input().split()) a = list(map(int, input().split())) b = list(map(int, input().split())) a.sort() # 오름차순 정렬 b.sort(reverse = True) # 내림차순 정렬 for i in range(k): if a[i] &lt; b[i] : a[i], b[i] = b[i], a[i] else : break print(sum(a)) # 배열 a의 합을 출력 . 파이썬의 자체 내장 기본 정렬 알고리즘을 따르기 때문에 시간 복잡도는 최악의 경우 O(NlogN)O(NlogN)O(NlogN)을 따르게 된다. .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/02/sort_2.html",
            "relUrl": "/algorithm/2022/07/02/sort_2.html",
            "date": " • Jul 2, 2022"
        }
        
    
  
    
        ,"post34": {
            "title": "파도반 수열",
            "content": "문제 . 오른쪽 그림과 같이 삼각형이 나선 모양으로 놓여져 있다. 첫 삼각형은 정삼각형으로 변의 길이는 1이다. . 그 다음에는 다음과 같은 과정으로 정삼각형을 계속 추가한다. 나선에서 가장 긴 변의 길이를 k라 했을 때, 그 변에 길이가 k인 정삼각형을 추가한다. . 파도반 수열 P(N)은 나선에 있는 정삼각형의 변의 길이이다. P(1)부터 P(10)까지 첫 10개 숫자는 1, 1, 1, 2, 2, 3, 4, 5, 7, 9이다. . N이 주어졌을 때, P(N)을 구하는 프로그램을 작성하시오. . . 입력 . 첫째 줄에 테스트 케이스의 개수 T가 주어진다. 각 테스트 케이스는 한 줄로 이루어져 있고, N이 주어진다. (1 ≤ N ≤ 100) . 출력 . 각 테스트 케이스마다 P(N)을 출력한다. . 예제 입력 1 | . 2 &gt; T 6 12 . 예제 출력 1 | . 3 16 . . 점화식을 세워야한다면 바로 동적계획법 문제임을 인식하고 풀면 된다. . 점화식은 f(n+3) = f(n) + f(n+1) 과 같다 . 함수를 이용해서 만드는 방법도 있지만 아래 방법은 함수 호출 때마다 반복문을 수행하기 때문에 처음에 반복문을 수행해서 memoization 리스트를 만들어 놓고 필요한 인덱스를 가져오는 것보다 시간 복잡도 측면에서 조금 더 불리하다. 조금이라도 아껴야하지 않을까. . # 함수 호출용 t = int(input()) # t 개수를 입력받는다 dp = [0]*101 dp[1:4] = [1, 1, 1] def wave(number): for index in range(1, number+1) : dp[index+3] = dp[index] + dp[index+1] return dp[number] for n in range(t): # 문제에서는 enter을 기준으로 입력을 받는다 n = int(input()) print(wave(n)) . print(dp) # memoization 리스트 [0, 1, 1, 1, 2, 2, 3, 4, 5, 7, 9, 12, 16, 21, 28, 37, 49, 65, 86, 114, 151, 200, 265, 351, 465, 616, 816, 1081, 1432, 1897, 2513, 3329, 4410, 5842, 7739, 10252, 13581, 17991, 23833, 31572, 41824, 55405, 73396, 97229, 128801, 170625, 226030, 299426, 396655, 525456, 696081, 922111, 1221537, 1618192, 2143648, 2839729, 3761840, 4983377, 6601569, 8745217, 11584946, 15346786, 20330163, 26931732, 35676949, 47261895, 62608681, 82938844, 109870576, 145547525, 192809420, 255418101, 338356945, 448227521, 593775046, 786584466, 1042002567, 1380359512, 1828587033, 2422362079, 3208946545, 4250949112, 5631308624, 7459895657, 9882257736, 13091204281, 17342153393, 22973462017, 30433357674, 40315615410, 53406819691, 70748973084, 93722435101, 124155792775, 164471408185, 217878227876, 288627200960, 382349636061, 506505428836, 670976837021, 888855064897] . # 문제 제출용 t = int(input()) # t 개수를 입력받는다 dp = [0]*101 # 100번째 dp[1] = 1 dp[2] = 1 dp[3] = 1 for index in range(1, 98) : dp[index+3] = dp[index] + dp[index+1] for n in range(t): # 문제에서는 enter을 기준으로 입력을 받는다 n = int(input()) print(dp[n]) .",
            "url": "https://hajunyoo.github.io/Blog/algorithm/2022/07/02/bj_9461.html",
            "relUrl": "/algorithm/2022/07/02/bj_9461.html",
            "date": " • Jul 2, 2022"
        }
        
    
  
    
        ,"post35": {
            "title": "Neural Collaborative Filtering",
            "content": "Neural Collaborative Filtering &#44060;&#50836; . 본 실습은 패스트캠퍼스의 &#39;딥러닝을 활용한 추천시스템 구현 올인원 패키지 Online&#39; 을 듣고 작성하였다는 점을 명시합니다. . user과 item의 latent features를 모델링하기 위한 신경망 구조를 제안한다 ⇒ user과 item의 관계를 보다 복잡하게 모델링할 수 있다는 점. | Multi layer Perceptron을 사용 | Neural net 기반의 Collaborative filtering으로 non linear한 부분을 커버했다. ⇒ 기존의 Linear Matrix Factorization의 한계점을 지적하였다. | . . 논문 | Keras로 작성된 저자 코드 | 논문은 0과 1로 user-item interaction으로 matrix을 나타내고 학습했으나, 이번 실습에서는 rating을 직접 예측하고, loss를 구해보는 것을 진행한다 | Configuration . import os import pandas as pd import numpy as np from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split import math from torch import nn, optim import torch import torch.nn.functional as F from torch.autograd import Variable from tqdm import tqdm import warnings warnings.filterwarnings(&quot;ignore&quot;) . Load Dataset . KMRD 데이터셋 활용 | google colab의 경우 data path 다시 확인하기 | . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . data_path = &#39;/content/drive/MyDrive/추천 시스템/fastcampus-RecSys/data/kmrd/kmr_dataset/datafile/kmrd-small&#39; . def read_data(data_path): df = pd.read_csv(os.path.join(data_path,&#39;rates.csv&#39;)) train_df, val_df = train_test_split(df, test_size=0.2, random_state=1234, shuffle=True) return train_df, val_df . !nvidia-smi . Fri Jul 1 16:21:37 2022 +--+ | NVIDIA-SMI 460.32.03 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:04.0 Off | 0 | | N/A 37C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . train_df, val_df = read_data(data_path) . print(train_df.shape) print(train_df.head()) . (112568, 4) user movie rate time 137023 48423 10764 10 1212241560 92868 17307 10170 10 1122185220 94390 18180 10048 10 1573403460 22289 1498 10001 9 1432684500 80155 12541 10022 10 1370458140 . val_df.shape . (28142, 4) . fig, ax = plt.subplots(1, 2, sharex=&#39;col&#39;, sharey=&#39;row&#39;, figsize=(12,7)) ax = ax.ravel() train_df[&#39;rate&#39;].hist(ax=ax[0]) val_df[&#39;rate&#39;].hist(ax=ax[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbf78199950&gt; . 대부분 10점에 수렴한 상태이다. . train_df[&#39;rate&#39;].describe() . count 112568.000000 mean 8.948369 std 2.114602 min 1.000000 25% 9.000000 50% 10.000000 75% 10.000000 max 10.000000 Name: rate, dtype: float64 . Load movie dataframe . movies_df = pd.read_csv(os.path.join(data_path, &#39;movies.txt&#39;), sep=&#39; t&#39;, encoding=&#39;utf-8&#39;) movies_df = movies_df.set_index(&#39;movie&#39;) castings_df = pd.read_csv(os.path.join(data_path, &#39;castings.csv&#39;), encoding=&#39;utf-8&#39;) countries_df = pd.read_csv(os.path.join(data_path, &#39;countries.csv&#39;), encoding=&#39;utf-8&#39;) genres_df = pd.read_csv(os.path.join(data_path, &#39;genres.csv&#39;), encoding=&#39;utf-8&#39;) # Get genre information genres = [(list(set(x[&#39;movie&#39;].values))[0], &#39;/&#39;.join(x[&#39;genre&#39;].values)) for index, x in genres_df.groupby(&#39;movie&#39;)] combined_genres_df = pd.DataFrame(data=genres, columns=[&#39;movie&#39;, &#39;genres&#39;]) combined_genres_df = combined_genres_df.set_index(&#39;movie&#39;) # Get castings information castings = [(list(set(x[&#39;movie&#39;].values))[0], x[&#39;people&#39;].values) for index, x in castings_df.groupby(&#39;movie&#39;)] combined_castings_df = pd.DataFrame(data=castings, columns=[&#39;movie&#39;,&#39;people&#39;]) combined_castings_df = combined_castings_df.set_index(&#39;movie&#39;) # Get countries for movie information countries = [(list(set(x[&#39;movie&#39;].values))[0], &#39;,&#39;.join(x[&#39;country&#39;].values)) for index, x in countries_df.groupby(&#39;movie&#39;)] combined_countries_df = pd.DataFrame(data=countries, columns=[&#39;movie&#39;, &#39;country&#39;]) combined_countries_df = combined_countries_df.set_index(&#39;movie&#39;) movies_df = pd.concat([movies_df, combined_genres_df, combined_castings_df, combined_countries_df], axis=1) . movies_df.head() . title title_eng year grade genres people country . movie . 10001 시네마 천국 | Cinema Paradiso , 1988 | 2013.0 | 전체 관람가 | 드라마/멜로/로맨스 | [4374, 178, 3241, 47952, 47953, 19538, 18991, ... | 이탈리아,프랑스 | . 10002 빽 투 더 퓨쳐 | Back To The Future , 1985 | 2015.0 | 12세 관람가 | SF/코미디 | [1076, 4603, 917, 8637, 5104, 9986, 7470, 9987] | 미국 | . 10003 빽 투 더 퓨쳐 2 | Back To The Future Part 2 , 1989 | 2015.0 | 12세 관람가 | SF/코미디 | [1076, 4603, 917, 5104, 391, 5106, 5105, 5107,... | 미국 | . 10004 빽 투 더 퓨쳐 3 | Back To The Future Part III , 1990 | 1990.0 | 전체 관람가 | 서부/SF/판타지/코미디 | [1076, 4603, 1031, 5104, 10001, 5984, 10002, 1... | 미국 | . 10005 스타워즈 에피소드 4 - 새로운 희망 | Star Wars , 1977 | 1997.0 | PG | 판타지/모험/SF/액션 | [1007, 535, 215, 1236, 35] | 미국 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 논문: user latent vector + item latent vector | 새롭게 생각할 수 있는 방법: user latent vector + item latent vector + etc vector (예시) meta information . | 논문의 아이디어를 가져와서 내 상황에 맞게끔 적용하는 것이 추천시스템의 본질이다. . | . movieName_dict = movies_df.to_dict()[&#39;title&#39;] genres_dict = movies_df.to_dict()[&#39;genres&#39;] . movies_df[&#39;genres&#39;] . movie 10001 드라마/멜로/로맨스 10002 SF/코미디 10003 SF/코미디 10004 서부/SF/판타지/코미디 10005 판타지/모험/SF/액션 ... 10995 스릴러 10996 코미디 10997 공포 10998 드라마/액션/모험/스릴러 10999 SF/드라마/공포 Name: genres, Length: 999, dtype: object . Dataset Loader . class DatasetLoader: def __init__(self, data_path): self.train_df, val_temp_df = read_data(data_path) # 데이터를 읽어와서 self.min_rating = min(self.train_df.rate) # 최소 평점 self.max_rating = self.train_df.rate.max() # 최대 평점 self.unique_users = self.train_df.user.unique() # 유니크한 유저 얼마나 되는지 확인 self.num_users = len(self.unique_users) self.user_to_index = {original: idx for idx, original in enumerate(self.unique_users)} # 인덱스로 바꿔줌 =&gt; 어느 인덱스가 1이되는지 원핫벡터로 : 0 1 0 0 0 ... 0 self.unique_movies = self.train_df.movie.unique() self.num_movies = len(self.unique_movies) self.movie_to_index = {original: idx for idx, original in enumerate(self.unique_movies)} self.val_df = val_temp_df[val_temp_df.user.isin(self.unique_users) &amp; val_temp_df.movie.isin(self.unique_movies)] def generate_trainset(self): # user 0, 0, 0, 1,2, 3,3, -&gt; movie: 0,0,0,0,0,0, X_train = pd.DataFrame({&#39;user&#39;: self.train_df.user.map(self.user_to_index), &#39;movie&#39;: self.train_df.movie.map(self.movie_to_index)}) y_train = self.train_df[&#39;rate&#39;].astype(np.float32) return X_train, y_train def generate_valset(self): X_val = pd.DataFrame({&#39;user&#39;: self.val_df.user.map(self.user_to_index), &#39;movie&#39;: self.val_df.movie.map(self.movie_to_index)}) y_val = self.val_df[&#39;rate&#39;].astype(np.float32) return X_val, y_val . Model Structure . 논문에서 제시한 모델 구조를 그대로 구현하고 영화 데이터로 실습해본다. | User Vector는 전체 영화 데이터에서 영화를 평가한 userid를 onehot vector로 나타낸 형태 | Item Vector는 전체 영화 데이터에 등장하는 영화의 id를 onehot vector로 나타낸 형태 | . class FeedForwardEmbedNN(nn.Module): def __init__(self, n_users, n_movies, hidden, dropouts, n_factors, embedding_dropout): super().__init__() self.user_emb = nn.Embedding(n_users, n_factors) # number of user만큼 사이즈를 만들어서 원 핫으로 임베딩해야한다 self.movie_emb = nn.Embedding(n_movies, n_factors) self.drop = nn.Dropout(embedding_dropout) self.hidden_layers = nn.Sequential(*list(self.generate_layers(n_factors*2, hidden, dropouts))) self.fc = nn.Linear(hidden[-1], 1) # hidden(은닉층) 개수만큼 리니어(계층) 세트를 만들어내서 계속 붙여주는 작업 =&gt; 총 3개가 생김 def generate_layers(self, n_factors, hidden, dropouts): assert len(dropouts) == len(hidden) idx = 0 while idx &lt; len(hidden): if idx == 0: yield nn.Linear(n_factors, hidden[idx]) else: yield nn.Linear(hidden[idx-1], hidden[idx]) yield nn.ReLU() yield nn.Dropout(dropouts[idx]) idx += 1 def forward(self, users, movies, min_rating=0.5, max_rating=5): concat_features = torch.cat([self.user_emb(users), self.movie_emb(movies)], dim=1) # 유저와 유저 임베딩을 가지고 concat을 함 x = F.relu(self.hidden_layers(concat_features)) # relu를 씌우고 # 0과 1사이의 숫자로 나타낸다 out = torch.sigmoid(self.fc(x)) # rating으로 변환한다 out = (out * (max_rating - min_rating)) + min_rating # 0~1의 값은 변환을 해서 출력 return out def predict(self, users, movies): # predict score을 내보냄 # return the score output_scores = self.forward(users, movies) return output_scores . class BatchIterator: def __init__(self, X, y, batch_size=32, shuffle=True): X, y = np.asarray(X), np.asarray(y) if shuffle: index = np.random.permutation(X.shape[0]) X, y = X[index], y[index] self.X = X self.y = y self.batch_size = batch_size self.shuffle = shuffle self.n_batches = int(math.ceil(X.shape[0] // batch_size)) self._current = 0 def __iter__(self): return self def __next__(self): return self.next() def next(self): if self._current &gt;= self.n_batches: raise StopIteration() k = self._current self._current += 1 bs = self.batch_size return self.X[k * bs:(k + 1) * bs], self.y[k * bs:(k + 1) * bs] . def batches(X, y, bs=32, shuffle=True): # 배치사이즈 32 for x_batch, y_batch in BatchIterator(X, y, bs, shuffle): x_batch = torch.LongTensor(x_batch) y_batch = torch.FloatTensor(y_batch) yield x_batch, y_batch.view(-1, 1) . Train model . 데이터셋과 모델 학습에 필요한 configuration을 입력하고, 학습을 하는 함수를 만든다 configuration을 바꾸면서 모델의 성능을 측정해볼 수 있다. . def model_train(ds, config): device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) X_train, y_train = ds.generate_trainset() X_valid, y_valid = ds.generate_valset() print(f&#39;TrainSet Info: {ds.num_users} users, {ds.num_movies} movies&#39;) model = FeedForwardEmbedNN( n_users=ds.num_users, n_movies=ds.num_movies, n_factors=config[&#39;num_factors&#39;], hidden=config[&#39;hidden_layers&#39;], embedding_dropout=config[&#39;embedding_dropout&#39;], dropouts=config[&#39;dropouts&#39;] ) # 모델을 정의 model.to(device) batch_size = config[&#39;batch_size&#39;] num_epochs = config[&#39;num_epochs&#39;] max_patience = config[&#39;total_patience&#39;] num_patience = 0 best_loss = np.inf criterion = nn.MSELoss(reduction=&#39;sum&#39;) # MSE LOSS를 사용 &lt;- 논문은 cross entropy를 사용 criterion.to(device) optimizer = optim.Adam(model.parameters(), lr=config[&#39;learning_rate&#39;], weight_decay=config[&#39;weight_decay&#39;]) # 일반적으로 많이 사용하는 아담 옵티마이저 사용 result = dict() for epoch in tqdm(range(num_epochs)): training_loss = 0.0 for batch in batches(X_train, y_train, shuffle=True, bs=batch_size): x_batch, y_batch = [b.to(device) for b in batch] optimizer.zero_grad() # with torch.no_grad() 와 동일한 syntax 입니다 with torch.set_grad_enabled(True): outputs = model(x_batch[:, 0], x_batch[:, 1], ds.min_rating, ds.max_rating) loss = criterion(outputs, y_batch) loss.backward() optimizer.step() training_loss += loss.item() # 로스 값을 계속 더해줌 result[&#39;train&#39;] = training_loss / len(X_train) #로스의 평균 값을 해줌 # Apply Early Stopping criteria and save best model params val_outputs = model(torch.LongTensor(X_valid.user.values).to(device), torch.LongTensor(X_valid.movie.values).to(device), ds.min_rating, ds.max_rating) val_loss = criterion(val_outputs.to(device), torch.FloatTensor(y_valid.values).view(-1, 1).to(device)) result[&#39;val&#39;] = float((val_loss / len(X_valid)).data) if val_loss &lt; best_loss: print(&#39;Save new model on epoch: %d&#39; % (epoch + 1)) best_loss = val_loss result[&#39;best_loss&#39;] = val_loss torch.save(model.state_dict(), config[&#39;save_path&#39;]) num_patience = 0 else: num_patience += 1 print(f&#39;[epoch: {epoch+1}] train: {result[&quot;train&quot;]} - val: {result[&quot;val&quot;]}&#39;) if num_patience &gt;= max_patience: print(f&quot;Early Stopped after epoch {epoch+1}&quot;) break return result . def model_valid(user_id_list, movie_id_list, data_path): dataset = DatasetLoader(data_path) processed_test_input_df = pd.DataFrame({ &#39;user_id&#39;: [dataset.user_to_index[x] for x in user_id_list], &#39;movie_id&#39;: [dataset.movie_to_index[x] for x in movie_id_list] }) # 학습한 모델 load하기 my_model = FeedForwardEmbedNN(dataset.num_users, dataset.num_movies, config[&#39;hidden_layers&#39;], config[&#39;dropouts&#39;], config[&#39;num_factors&#39;], config[&#39;embedding_dropout&#39;]) my_model.load_state_dict(torch.load(&#39;params.data&#39;)) # 모델을 로드 prediction_outputs = my_model.predict(users=torch.LongTensor(processed_test_input_df.user_id.values), movies=torch.LongTensor(processed_test_input_df.movie_id.values)) # 모델로 예측을 수행 return prediction_outputs . dataset = DatasetLoader(data_path) # 데이터 셋을 로드 . config = { &quot;num_factors&quot;: 16, &quot;hidden_layers&quot;: [64, 32, 16], &quot;embedding_dropout&quot;: 0.05, &quot;dropouts&quot;: [0.3, 0.3, 0.3], &quot;learning_rate&quot;: 1e-3, &quot;weight_decay&quot;: 1e-5, &quot;batch_size&quot;: 8, &quot;num_epochs&quot;: 3, &quot;total_patience&quot;: 30, &quot;save_path&quot;: &quot;params.data&quot; } # configuration 정의 . model_train(dataset, config) # epoch 3 . TrainSet Info: 44453 users, 597 movies . 33%|███▎ | 1/3 [00:39&lt;01:19, 39.75s/it] . Save new model on epoch: 1 [epoch: 1] train: 4.342063758400752 - val: 3.8571109771728516 . 67%|██████▋ | 2/3 [01:13&lt;00:36, 36.42s/it] . Save new model on epoch: 2 [epoch: 2] train: 3.7550543531024645 - val: 3.582547426223755 . 100%|██████████| 3/3 [01:48&lt;00:00, 36.10s/it] . Save new model on epoch: 3 [epoch: 3] train: 3.3175223239590426 - val: 3.543619394302368 . . {&#39;best_loss&#39;: tensor(71779.5547, device=&#39;cuda:0&#39;, grad_fn=&lt;MseLossBackward0&gt;), &#39;train&#39;: 3.3175223239590426, &#39;val&#39;: 3.543619394302368} . val_df.head() . user movie rate time . 76196 11242 | 10253 | 10 | 1437788760 | . 109800 26903 | 10102 | 10 | 1322643900 | . 60479 7101 | 10007 | 1 | 1314804000 | . 71460 9705 | 10016 | 10 | 1228825200 | . 73864 10616 | 10106 | 8 | 1425046200 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; movie_id_list = [10253, 10102, 10007] user_id = 11242 user_id_list = [user_id] * len(movie_id_list) pred_results = [float(x) for x in model_valid(user_id_list, movie_id_list, data_path)] # 저장된 모델로 예측을 함 result_df = pd.DataFrame({ &#39;userId&#39;: user_id_list, &#39;movieId&#39;: movie_id_list, # &#39;movieName&#39;: [movieName_dict[x] for x in movie_id_list], # &#39;genres&#39;: [genres_dict[x] for x in movie_id_list], &#39;pred_ratings&#39;: pred_results }) result_df.sort_values(by=&#39;pred_ratings&#39;, ascending=False) . userId movieId pred_ratings . 0 11242 | 10253 | 4.825642 | . 1 11242 | 10102 | 4.768200 | . 2 11242 | 10007 | 4.103816 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt;",
            "url": "https://hajunyoo.github.io/Blog/datascience/recommendation/2022/07/02/Neural-Collaborative-Filtering-%EC%8B%A4%EC%8A%B5.html",
            "relUrl": "/datascience/recommendation/2022/07/02/Neural-Collaborative-Filtering-%EC%8B%A4%EC%8A%B5.html",
            "date": " • Jul 2, 2022"
        }
        
    
  
    
        ,"post36": {
            "title": "맥에서 깃허브 블로그를 만들기 위해 ruby 실행 시 오류 발생",
            "content": "맥에서 깃허브 블로그를 만들기 위해 ruby 실행 시 오류 발생 . 저자는 아래와 같은 스펙의 맥을 사용 중입니다. . brew install rbenv ruby-build . Error: Cannot install under Rosetta 2 in ARM default prefix (/opt/homebrew)! To rerun under ARM use: arch -arm64 brew install ... To install under x86_64, install Homebrew into /usr/local. . 위의 에러 발생 . 아래처럼 arch-arm64 추가 . arch -arm64 brew install rbenv ruby-build . rbenv versions * system . 아직은 시스템을 쓰고 있다고 뜸 . rbenv install 2.6.4 . 이제 시스템 루비가 아닌 다른 루비 rbenv 를 설치해보겠습니다. . 그런 후 버전을 확인하고 시스템 루비에서 방금 설치한 루비로 글로벌 버젼을 교체하겠습니다. . 하지만 build가 fail 한다 . 구글링을 하다 아래의 방식을 참고하게 되었다 링크 . arch -arm64 brew install openssl libffi zlib rbenv readline ruby-build . 그런 후 다른 버전을 설치해본다 . arch -arm64 rbenv install 2.7.5 . rbnev versions rbenv global 2.7.5 system * 2.7.5 (set by /Users/yoohajun/.rbenv/version) . 방금 설치한 것으로 global 하게 바뀐 것을 확인 가능하다 . shell 을 열어서 루비 패스를 추가해주고 저장해준 후 gem install을 해보자 . vi ~/.zshrc &#39;&#39;&#39; [[ -d ~/.rbenv ]] &amp;&amp; export PATH=${HOME}/.rbenv/bin:${PATH} &amp;&amp; eval &quot;$(rbenv init -)&quot; &#39;&#39;&#39; source ~/.zshrc gem install bundler . unicode-display_width, terminal-table, jekyll after 11 seconds 26 gems installed . 설치가 완료되었다! . 깃허브 블로그가 저장된 로컬 주소로 가서 터미널을 열어 번들을 설치해준다. . bundle install . . 그 후 로컬 서버를 실행시켜준다 . bundle exec jekyll serve . 이제 번들 서버를 사용해 로컬에서 업데이트 진행 상황을 확인이 가능해졌다! .",
            "url": "https://hajunyoo.github.io/Blog/etc/2022/07/01/second.html",
            "relUrl": "/etc/2022/07/01/second.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post37": {
            "title": "First Post",
            "content": "오늘 처음 블로그를 만들어보았습니다 . 앞으로 열심히 해보겠습니다 .",
            "url": "https://hajunyoo.github.io/Blog/etc/2022/07/01/first.html",
            "relUrl": "/etc/2022/07/01/first.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post38": {
            "title": "Backend Roadmap",
            "content": "Backend Roadmap 따라가기 . .",
            "url": "https://hajunyoo.github.io/Blog/backend/2022/07/01/roadmap.html",
            "relUrl": "/backend/2022/07/01/roadmap.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post39": {
            "title": "2021 Lpoint 클릭스트림",
            "content": "&#44060;&#50836; . 2021 국민대학교 빅데이터 경영학과 학회 D&amp;A에서 주최한 Lpoint 클릭스트림 데이터를 이용한 성별 예측 대회 제출본입니다. | . %matplotlib inline import random import scipy import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from tqdm import tqdm from datetime import date . df = pd.read_csv(&#39;./L.POINT_train.csv&#39;) df_test = pd.read_csv(&#39;./L.POINT_test.csv&#39;) . /opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) . y = pd.read_csv(&#39;./y_train.csv&#39;) . df.head() . CLNT_ID SESS_ID HITS_SEQ PD_C PD_ADD_NM PD_BRA_NM PD_BUY_AM PD_BUY_CT SESS_SEQ SESS_DT ... TOT_SESS_HR_V DVC_CTG_NM ZON_NM CITY_NM KWD_NM SEARCH_CNT PD_NM CLAC1_NM CLAC2_NM CLAC3_NM . 0 0 | 6771240 | 63 | 578845 | 1개 | 필립스(PHILIPS) | 81,000 | 1 | 17 | 20180609 | ... | 922 | mobile | Gyeonggi-do | Bucheon-si | 에어컨 커버 | 1 | 아방세 프로믹스 핸드블렌더 HR1672/90 | 생활/주방가전 | 주방가전 | 블랜더 | . 1 0 | 6771240 | 63 | 788068 | 선택:버닝 [베이지] / 1개 | 쁘리엘르 | 5,500 | 1 | 17 | 20180609 | ... | 922 | mobile | Gyeonggi-do | Bucheon-si | 에어컨 커버 | 1 | 스판 벽걸이 에어컨커버(트라이앵글_82x27x26) 모음 - 블루가든 [블루] | 침구/수예 | 수예소품 | 거실수예소품 | . 2 1 | 5762174 | 109 | 180447 | 사이즈:L(105) / 1개 | 퀵실버 | 59,000 | 1 | 12 | 20180626 | ... | 1,661 | mobile | Seoul | Seoul | 바비브라운 | 1 | 퀵실버 남성 루즈핏 래쉬가드 QS579KMT - M(100) | 시즌스포츠 | 수영/물놀이 | 남성수영복 | . 3 1 | 5753875 | 94 | 731145 | 1개 | 키엘 | 39,000 | 1 | 13 | 20180626 | ... | 620 | mobile | Seoul | Seoul | 키엘 | 2 | 칼렌듈라 딥 클렌징 포밍 페이스 워시 230ml | 화장품/뷰티케어 | 스킨케어 | 페이셜클렌저 | . 4 1 | 7417570 | 114 | 216947 | 1개 | 키엘 | 49,000 | 1 | 2 | 20180529 | ... | 860 | mobile | Seoul | Seoul | 키엘비타민 | 2 | 키엘 자외선 차단제 점보 세트 | 화장품/뷰티케어 | 선케어 | 선크림류 | . 5 rows × 21 columns . len(df[&#39;KWD_NM&#39;].unique()) . 72089 . len(df[&#39;KWD_NM&#39;].unique()) . 72089 . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1948686 entries, 0 to 1948685 Data columns (total 21 columns): # Column Dtype -- 0 CLNT_ID int64 1 SESS_ID int64 2 HITS_SEQ int64 3 PD_C int64 4 PD_ADD_NM object 5 PD_BRA_NM object 6 PD_BUY_AM object 7 PD_BUY_CT object 8 SESS_SEQ int64 9 SESS_DT int64 10 TOT_PAG_VIEW_CT float64 11 TOT_SESS_HR_V object 12 DVC_CTG_NM object 13 ZON_NM object 14 CITY_NM object 15 KWD_NM object 16 SEARCH_CNT int64 17 PD_NM object 18 CLAC1_NM object 19 CLAC2_NM object 20 CLAC3_NM object dtypes: float64(1), int64(7), object(13) memory usage: 312.2+ MB . y . CLNT_ID LABEL . 0 0 | F20 | . 1 1 | F30 | . 2 6 | F20 | . 3 9 | F30 | . 4 12 | F30 | . ... ... | ... | . 149995 263094 | F30 | . 149996 263095 | F30 | . 149997 263096 | F30 | . 149998 263102 | F30 | . 149999 263103 | F30 | . 150000 rows × 2 columns . y[&#39;LABEL&#39;].unique() . array([&#39;F20&#39;, &#39;F30&#39;, &#39;F40&#39;, &#39;M30&#39;, &#39;M40&#39;, &#39;M20&#39;], dtype=object) . y[&#39;LABEL&#39;].value_counts() . F30 59892 F40 51936 F20 17727 M40 9904 M30 7953 M20 2588 Name: LABEL, dtype: int64 . weight = np.array([59892/17727, 59892/59892, 59892/51936, 59892/2588, 59892/7953, 59892/9904]) weight . array([ 3.37857506, 1. , 1.15318854, 23.14219474, 7.53074312, 6.04725363]) . df = pd.merge(df, y, how=&#39;left&#39;, on=&#39;CLNT_ID&#39;) . len(df[&#39;CLNT_ID&#39;].unique()) . 150000 . 1948686행, 150000 유저 -&gt; 유저당 평균 13회 로그 정보 . len(df[&#39;PD_C&#39;].unique()) . 280867 . 280867개 고유 상품 . df.groupby(by=&#39;LABEL&#39;)[&#39;DVC_CTG_NM&#39;].value_counts() . LABEL DVC_CTG_NM F20 mobile 150178 tablet 519 desktop 18 F30 mobile 794940 tablet 2417 desktop 13 F40 mobile 772145 tablet 5190 desktop 29 M20 mobile 19259 tablet 150 desktop 9 M30 mobile 78795 tablet 109 desktop 10 M40 mobile 123945 tablet 896 desktop 64 Name: DVC_CTG_NM, dtype: int64 . df[&#39;ZON_NM&#39;].unique() . array([&#39;Gyeonggi-do&#39;, &#39;Seoul&#39;, &#39;Jeollabuk-do&#39;, &#39;Gwangju&#39;, &#39;Busan&#39;, &#39;Incheon&#39;, &#39;Gyeongsangnam-do&#39;, &#39;Gyeongsangbuk-do&#39;, &#39;Chungcheongnam-do&#39;, &#39;Jeollanam-do&#39;, &#39;Daegu&#39;, &#39;Gangwon-do&#39;, &#39;Ulsan&#39;, &#39;Daejeon&#39;, &#39;Chungcheongbuk-do&#39;, &#39;Jeju-do&#39;], dtype=object) . df[&#39;CITY_NM&#39;].unique() . array([&#39;Bucheon-si&#39;, &#39;Seoul&#39;, &#39;Wanju-gun&#39;, &#39;Gwangju&#39;, &#39;Namyangju-si&#39;, &#39;Guri-si&#39;, &#39;Busan&#39;, &#39;Imsil-gun&#39;, &#39;Gimpo-si&#39;, &#39;Incheon&#39;, &#39;Gimhae-si&#39;, &#39;Paju-si&#39;, &#39;Sangju-si&#39;, &#39;Yeongi-gun&#39;, &#39;Hwaseong-si&#39;, &#39;Gongju-si&#39;, &#39;Seongnam-si&#39;, &#39;Andong&#39;, &#39;Jeongeup-si&#39;, &#39;Geoje-si&#39;, &#39;Gangjin-gun&#39;, &#39;Gumi-si&#39;, &#39;Yeoju-gun&#39;, &#39;Pyeongtaek-si&#39;, &#39;Daegu&#39;, &#39;Yongin-si&#39;, &#39;Yangsan-si&#39;, &#39;Chuncheon-si&#39;, &#39;Suwon-si&#39;, &#39;Ulsan&#39;, &#39;Tongyeong-si&#39;, &#39;Haman-gun&#39;, &#39;Goyang-si&#39;, &#39;Daejeon&#39;, &#39;Cheonan-si&#39;, &#39;Yeosu-si&#39;, &#39;Jeonju-si&#39;, &#39;Taean-gun&#39;, &#39;Chungju-si&#39;, &#39;Uijeongbu-si&#39;, &#39;Gangneung-si&#39;, &#39;Gyeongju-si&#39;, &#39;Hanam-si&#39;, &#39;Sunchang-gun&#39;, &#39;Mungyeong-si&#39;, &#39;Gunsan-si&#39;, &#39;Wonju-si&#39;, &#39;Uiwang-si&#39;, &#39;Anseong&#39;, &#39;Anyang&#39;, &#39;Ansan-si&#39;, &#39;Gunpo-si&#39;, &#39;Sacheon-si&#39;, &#39;Cheongju-si&#39;, &#39;Miryang-si&#39;, &#39;Gimcheon-si&#39;, &#39;Sokcho-si&#39;, &#39;Yeongju-si&#39;, &#39;Jeju-si&#39;, &#39;Siheung-si&#39;, &#39;Pohang-si&#39;, &#39;Seosan-si&#39;, &#39;Hapcheon-gun&#39;, &#39;Yangju-si&#39;, &#39;Taebaek-si&#39;, &#39;Muan-gun&#39;, &#39;Goesan-gun&#39;, &#39;Jindo-gun&#39;, &#39;Gwangju-si&#39;, &#39;Yangpyeong-gun&#39;, &#39;Yeongyang-gun&#39;, &#39;Hongseong-gun&#39;, &#39;Goryeong-gun&#39;, &#39;Naju-si&#39;, &#39;Icheon-si&#39;, &#39;Yeongdong-gun&#39;, &#39;Gwangmyeong-si&#39;, &#39;Asan-si&#39;, &#39;Gyeongsan-si&#39;, &#39;Gochang-gun&#39;, &#39;Hoengseong-gun&#39;, &#39;Yeongdeok-gun&#39;, &#39;Uljin-gun&#39;, &#39;Seongju-gun&#39;, &#39;Suncheon-si&#39;, &#39;Cheorwon-gun&#39;, &#39;Boseong-gun&#39;, &#39;Yesan-gun&#39;, &#39;Iksan&#39;, &#39;Geumsan-gun&#39;, &#39;Jincheon-gun&#39;, &#39;Dangjin-si&#39;, &#39;Jeungpyeong-gun&#39;, &#39;Jinju-si&#39;, &#39;Gapyeong-gun&#39;, &#39;Mokpo-si&#39;, &#39;Seogwipo-si&#39;, &#39;Namwon-si&#39;, &#39;Jangheung-gun&#39;, &#39;Geochang-gun&#39;, &#39;Gwangyang-si&#39;, &#39;Goseong-gun&#39;, &#39;Chilgok-gun&#39;, &#39;Donghae-si&#39;, &#39;Yeongcheon-si&#39;, &#39;Cheongwon-gun&#39;, &#39;Yangyang-gun&#39;, &#39;Osan-si&#39;, &#39;Haenam-gun&#39;, &#39;Gwacheon-si&#39;, &#39;Yeonggwang-gun&#39;, &#39;Dongducheon-si&#39;, &#39;Yeongam-gun&#39;, &#39;Eumseong-gun&#39;, &#39;Gokseong-gun&#39;, &#39;Cheongyang-gun&#39;, &#39;Yeongwol-gun&#39;, &#39;Nonsan-si&#39;, &#39;Hongcheon-gun&#39;, &#39;Jecheon-si&#39;, &#39;Danyang-gun&#39;, &#39;Pyeongchang-gun&#39;, &#39;Hamyang-gun&#39;, &#39;Hwasun-gun&#39;, &#39;Buyeo-gun&#39;, &#39;Samcheok-si&#39;, &#39;Okcheon-gun&#39;, &#39;Cheongsong-gun&#39;, &#39;Yecheon-gun&#39;, &#39;Buan-gun&#39;, &#39;Boeun-gun&#39;, &#39;Gyeryong-si&#39;, &#39;Yanggu-gun&#39;, &#39;Gurye-gun&#39;, &#39;Wando-gun&#39;, &#39;Sancheong-gun&#39;, &#39;Namhae-gun&#39;, &#39;Hampyeong-gun&#39;, &#39;Hwacheon-gun&#39;, &#39;Gimje-si&#39;, &#39;Cheongdo-gun&#39;, &#39;Jeongseon-gun&#39;, &#39;Pocheon-si&#39;, &#39;Changnyeong-gun&#39;, &#39;Goheung-gun&#39;, &#39;Boryeong-si&#39;, &#39;Jinan-gun&#39;, &#39;Uiseong-gun&#39;, &#39;Damyang-gun&#39;, &#39;Seocheon-gun&#39;, &#39;Uiryeong-gun&#39;, &#39;Yeoncheon-gun&#39;, &#39;Jangsu-gun&#39;, &#39;Jangseong-gun&#39;, &#39;Sinan-gun&#39;, &#39;Inje-gun&#39;, &#39;Gunwi-gun&#39;, &#39;Ulleung-gun&#39;, &#39;Bonghwa-gun&#39;, &#39;Hadong-gun&#39;, &#39;Muju-gun&#39;, &#39;(not set)&#39;], dtype=object) . df[&#39;PD_NM&#39;] . 0 아방세 프로믹스 핸드블렌더 HR1672/90 1 스판 벽걸이 에어컨커버(트라이앵글_82x27x26) 모음 - 블루가든 [블루] 2 퀵실버 남성 루즈핏 래쉬가드 QS579KMT - M(100) 3 칼렌듈라 딥 클렌징 포밍 페이스 워시 230ml 4 키엘 자외선 차단제 점보 세트 ... 1948681 돌핀 배색 올인원 수영복[71SW77831] - 핑크(P) / 110 1948682 돌핀 배색 올인원 수영복[71SW77831] - 핑크(P) / 110 1948683 돌핀 배색 올인원 수영복[71SW77831] - 핑크(P) / 110 1948684 돌핀 배색 올인원 수영복[71SW77831] - 핑크(P) / 110 1948685 여아 돌고래 배색 래쉬가드[71SW50831] - 핑크(P) / 105 Name: PD_NM, Length: 1948686, dtype: object . len(df[&#39;PD_NM&#39;].unique()) . 276002 . num(PD_C) != num(PD_NM) . len(df[&#39;CLAC1_NM&#39;].unique()), len(df[&#39;CLAC2_NM&#39;].unique()), len(df[&#39;CLAC3_NM&#39;].unique()) . (37, 128, 883) . 대분류 37개, 중분류 128개, 소분류 883개 . df[&#39;KWD_NM&#39;] . 0 에어컨 커버 1 에어컨 커버 2 바비브라운 3 키엘 4 키엘비타민 ... 1948681 키즈샌들 1948682 갭키즈 1948683 팁토이조이 1948684 갭키즈 수영복 1948685 갭키즈 여아 Name: KWD_NM, Length: 1948686, dtype: object . df[&#39;KWD_NM&#39;].isna().sum() . 0 . df[&#39;SEARCH_CNT&#39;] . 0 1 1 1 2 1 3 2 4 2 .. 1948681 1 1948682 1 1948683 1 1948684 1 1948685 1 Name: SEARCH_CNT, Length: 1948686, dtype: int64 . clnt_id = y[&#39;CLNT_ID&#39;].values . clnt_id . array([ 0, 1, 6, ..., 263096, 263102, 263103], dtype=int64) . df[&#39;PD_BUY_CT&#39;] = df[&#39;PD_BUY_CT&#39;].astype(&#39;string&#39;) df[&#39;TOT_SESS_HR_V&#39;] = df[&#39;TOT_SESS_HR_V&#39;].astype(&#39;string&#39;) df[&#39;PD_BUY_AM&#39;] = df[&#39;PD_BUY_AM&#39;].map(lambda x: x.replace(&#39;,&#39;, &#39;&#39;)) df[&#39;PD_BUY_CT&#39;] = df[&#39;PD_BUY_CT&#39;].map(lambda x: x.replace(&#39;,&#39;, &#39;&#39;)) df[&#39;TOT_SESS_HR_V&#39;] = df[&#39;TOT_SESS_HR_V&#39;].map(lambda x: x.replace(&#39;,&#39;, &#39;&#39;)) . df[&#39;PD_BUY_AM&#39;] = df[&#39;PD_BUY_AM&#39;].astype(&#39;int&#39;) df[&#39;PD_BUY_CT&#39;] = df[&#39;PD_BUY_CT&#39;].astype(&#39;int&#39;) df[&#39;TOT_SESS_HR_V&#39;] = df[&#39;TOT_SESS_HR_V&#39;].astype(&#39;int&#39;) . df[&#39;SESS_DT&#39;] = df[&#39;SESS_DT&#39;].map(lambda x: date.fromisoformat(str(x)[:4] + &#39;-&#39; + str(x)[4:6] + &#39;-&#39; + str(x)[6:])) . temp_df = df[df[&#39;CLNT_ID&#39;] == clnt_id[4]] temp_df = temp_df.sort_values(by=[&#39;SESS_DT&#39;, &#39;HITS_SEQ&#39;]) temp_df = temp_df[~temp_df.duplicated(subset=[&#39;SESS_ID&#39;, &#39;HITS_SEQ&#39;], keep=&#39;last&#39;)] temp_df . CLNT_ID SESS_ID HITS_SEQ PD_C PD_ADD_NM PD_BRA_NM PD_BUY_AM PD_BUY_CT SESS_SEQ SESS_DT ... DVC_CTG_NM ZON_NM CITY_NM KWD_NM SEARCH_CNT PD_NM CLAC1_NM CLAC2_NM CLAC3_NM LABEL . 48 12 | 4738380 | 47 | 178471 | 사이즈:18M / 1개 | 갭 키즈 | 10000 | 1 | 175 | 2018-07-12 | ... | mobile | Seoul | Seoul | 베베드피노 오버롤 | 1 | 베이비 여아 화이트 블루머 5238234509001 - 24M | 속옷/양말/홈웨어 | 유아동양말류 | 유아동타이즈 | F30 | . 44 12 | 4551726 | 32 | 233182 | 색상:BLUE|사이즈:85 / 1개 | 베베드피노 | 39000 | 1 | 189 | 2018-07-16 | ... | mobile | Gyeonggi-do | Namyangju-si | 뚜아후아 귀걸이 | 4 | 베이비 스트라이프 러플 바디수트BP8216222 - BLUE / 85 | 유아동의류 | 유아의류전신 | 영유아점프수트/오버롤 | F30 | . 57 12 | 4130706 | 104 | 141349 | 모델명:01&gt;MS4388아이보리|사이즈:39 / 1개 | 클립 | 79000 | 1 | 219 | 2018-07-22 | ... | mobile | Gyeonggi-do | Guri-si | 코치 | 1 | 베스트샌들 MS4388 Malou_Color line Webbing 아이보리 외 1... | 패션잡화 | 여성화 | 여성샌들 | F30 | . 55 12 | 1776250 | 36 | 72185 | 색상:코랄|사이즈:S(6~12m) / 1개 | 해피프린스 | 4560 | 1 | 313 | 2018-09-01 | ... | mobile | Gyeonggi-do | Guri-si | 해피프린스 | 4 | 나리앙 니삭스 - 코랄 / M(12~24m) | 속옷/양말/홈웨어 | 유아동양말류 | 유아동일반양말 | F30 | . 60 12 | 1011134 | 50 | 536257 | 색상:베이지|사이즈:S / 1개 | 해피프린스 | 990 | 1 | 345 | 2018-09-13 | ... | mobile | Gyeonggi-do | Namyangju-si | 해피프린스 양말 | 1 | 폴리지 삭스 - 블루 / S | 속옷/양말/홈웨어 | 유아동양말류 | 유아동일반양말 | F30 | . 59 12 | 1011134 | 58 | 102618 | 1개 | 테팔 | 15900 | 1 | 345 | 2018-09-13 | ... | mobile | Gyeonggi-do | Namyangju-si | 해피프린스 양말 | 1 | 그래픽 패스포트 아이 러브 뉴욕 프라이팬 26cm | 식기/조리기구 | 조리기구 | 프라이팬 | F30 | . 6 rows × 22 columns . def calc_avg_shopping_interval(df): mean_shopping_interval = 0 for i in range(len(df)-1): mean_shopping_interval += (df[&#39;SESS_DT&#39;].iloc[i+1] - df[&#39;SESS_DT&#39;].iloc[i]).days if len(df)-1 == 0: mean_shopping_interval = 183 else: mean_shopping_interval = mean_shopping_interval / (len(df)-1) return mean_shopping_interval . def calc_avg_total_price_ct(df): price = 0 cnt = 0 for i in range(len(df)): price += df[&#39;PD_BUY_AM&#39;].iloc[i] * df[&#39;PD_BUY_CT&#39;].iloc[i] cnt += df[&#39;PD_BUY_CT&#39;].iloc[i] return price/cnt, price, cnt/len(df), cnt . &#54588;&#52376; &#49373;&#49457; &#47336;&#54532; &#53076;&#46300; . num_shoppings = [] avg_prices = [] total_prices = [] avg_cts = [] total_cts = [] avg_sess_views = [] total_sess_views = [] avg_sess_hrs = [] total_sess_hrs = [] avg_shopping_intervals = [] main_devices = [] pd_cs = [] clac1_nms = [] clac2_nms = [] clac3_nms = [] for i in tqdm(range(len(clnt_id))): temp_df = df[df[&#39;CLNT_ID&#39;] == clnt_id[i]] temp_df = temp_df.sort_values(by=[&#39;SESS_DT&#39;, &#39;HITS_SEQ&#39;, &#39;PD_C&#39;]) temp_df = temp_df[~temp_df.duplicated(subset=[&#39;SESS_ID&#39;, &#39;HITS_SEQ&#39;, &#39;PD_C&#39;], keep=&#39;last&#39;)] num_shopping = len(temp_df) avg_price, total_price, avg_ct, total_ct = calc_avg_total_price_ct(temp_df) avg_sess_view = temp_df[&#39;TOT_PAG_VIEW_CT&#39;].values.mean() total_sess_view = temp_df[&#39;TOT_PAG_VIEW_CT&#39;].values.sum() avg_sess_hr = temp_df[&#39;TOT_SESS_HR_V&#39;].values.mean() total_sess_hr = temp_df[&#39;TOT_SESS_HR_V&#39;].values.sum() avg_shopping_interval = calc_avg_shopping_interval(temp_df) main_device = scipy.stats.mode(temp_df[&#39;DVC_CTG_NM&#39;].values).mode[0] pd_c = temp_df[&#39;PD_C&#39;].values clac1_nm = temp_df[&#39;CLAC1_NM&#39;].values clac2_nm = temp_df[&#39;CLAC2_NM&#39;].values clac3_nm = temp_df[&#39;CLAC3_NM&#39;].values num_shoppings.append(num_shopping) avg_prices.append(avg_price) total_prices.append(total_price) avg_cts.append(avg_ct) total_cts.append(total_ct) avg_sess_views.append(avg_sess_view) total_sess_views.append(total_sess_view) avg_sess_hrs.append(avg_sess_hr) total_sess_hrs.append(total_sess_hr) avg_shopping_intervals.append(avg_shopping_interval) main_devices.append(main_device) pd_cs.append(pd_c) clac1_nms.append(clac1_nm) clac2_nms.append(clac2_nm) clac3_nms.append(clac3_nm) . 100%|█████████████████████████████████████████████████████████████████████████| 150000/150000 [15:52&lt;00:00, 157.46it/s] . data = pd.DataFrame([clnt_id, num_shoppings, avg_prices, total_prices, avg_cts, total_cts, avg_sess_views, total_sess_views, avg_sess_hrs, total_sess_hrs, avg_shopping_intervals, main_devices, pd_cs, clac1_nms, clac2_nms, clac3_nms, y[&#39;LABEL&#39;]]).T . data.columns = [&#39;clnt_id&#39;, &#39;num_shopping&#39;, &#39;avg_price&#39;, &#39;total_price&#39;, &#39;avg_ct&#39;, &#39;total_ct&#39;, &#39;avg_sess_view&#39;, &#39;total_sess_view&#39;, &#39;avg_sess_hr&#39;, &#39;total_sess_hr&#39;, &#39;avg_shopping_interval&#39;, &#39;main_device&#39;, &#39;pd_c&#39;, &#39;clac1_nm&#39;, &#39;clac2_nm&#39;, &#39;clac3_nm&#39;, &#39;label&#39;] . data . clnt_id num_shopping avg_price total_price avg_ct total_ct avg_sess_view total_sess_view avg_sess_hr total_sess_hr avg_shopping_interval main_device pd_c clac1_nm clac2_nm clac3_nm label . 0 0 | 2 | 43250 | 86500 | 1 | 2 | 59 | 118 | 922 | 1844 | 0 | mobile | [578845, 788068] | [생활/주방가전, 침구/수예] | [주방가전, 수예소품] | [블랜더, 거실수예소품] | F20 | . 1 1 | 9 | 77777.8 | 700000 | 1 | 9 | 132.333 | 1191 | 1311.11 | 11800 | 3.625 | mobile | [216947, 236174, 1965, 190233, 731145, 180447,... | [화장품/뷰티케어, 화장품/뷰티케어, 건강식품, 화장품/뷰티케어, 화장품/뷰티케어,... | [선케어, 스킨케어, 홍삼/인삼가공식품, 스킨케어, 스킨케어, 수영/물놀이, 메이크... | [선크림류, 에센스/세럼, 홍삼액, 에센스/세럼, 페이셜클렌저, 남성수영복, BB/... | F30 | . 2 6 | 4 | 24225 | 96900 | 1 | 4 | 21.75 | 87 | 297.25 | 1189 | 5.66667 | mobile | [554217, 248358, 506337, 506359] | [스포츠패션, 속옷/양말/홈웨어, 속옷/양말/홈웨어, 속옷/양말/홈웨어] | [여성스포츠화, 여성속옷, 여성속옷, 여성속옷] | [여성스포츠샌들/슬리퍼, 여성속옷세트, 여성팬티, 브래지어] | F20 | . 3 9 | 2 | 10550 | 21100 | 1 | 2 | 249 | 498 | 5049 | 10098 | 0 | mobile | [436275, 578537] | [속옷/양말/홈웨어, 속옷/양말/홈웨어] | [유아동속옷, 유아동속옷] | [유아동팬티, 유아동팬티] | F30 | . 4 12 | 16 | 14325.6 | 229210 | 1 | 16 | 144.938 | 2319 | 4187.25 | 66996 | 4.2 | mobile | [178471, 233182, 141349, 64916, 72185, 489942,... | [속옷/양말/홈웨어, 유아동의류, 패션잡화, 패션잡화, 속옷/양말/홈웨어, 속옷/양... | [유아동양말류, 유아의류전신, 여성화, 모자, 유아동양말류, 유아동양말류, 모자, ... | [유아동타이즈, 영유아점프수트/오버롤, 여성샌들, 아동모, 유아동일반양말, 유아동일... | F30 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 149995 263094 | 1 | 10000 | 10000 | 1 | 1 | 66 | 66 | 513 | 513 | 183 | mobile | [536890] | [패션잡화] | [여성화] | [여성플랫] | F30 | . 149996 263095 | 2 | 122000 | 244000 | 1 | 2 | 220.5 | 441 | 1828 | 3656 | 83 | mobile | [411059, 741695] | [유아동의류, 화장품/뷰티케어] | [여아의류아우터, 스킨케어] | [여아점퍼, 스킨케어세트] | F30 | . 149997 263096 | 3 | 28500 | 85500 | 1 | 3 | 256 | 768 | 4237 | 12711 | 0 | mobile | [406269, 592279, 592285] | [스포츠패션, 스포츠패션, 스포츠패션] | [여성스포츠화, 남성일반스포츠의류, 남성일반스포츠의류] | [여성런닝/트레이닝화, 남성일반스포츠바지, 남성일반스포츠바지] | F30 | . 149998 263102 | 1 | 1080 | 1080 | 1 | 1 | 188 | 188 | 1812 | 1812 | 183 | mobile | [785281] | [문구/사무용품] | [필기도구] | [볼펜] | F30 | . 149999 263103 | 7 | 58628.6 | 410400 | 1 | 7 | 280 | 1960 | 3249.43 | 22746 | 4.66667 | mobile | [381544, 576311, 171218, 144818, 307370, 30736... | [패션잡화, 식기/조리기구, 유아동의류, 시즌스포츠, 시즌스포츠, 시즌스포츠, 화장... | [유아동화, 그릇/식기, 여아의류상의, 수영/물놀이, 수영/물놀이, 수영/물놀이, ... | [유아동샌들, 커피잔, 여아티셔츠/탑, 아동수영복, 아동수영복, 아동수영복, 크림/... | F30 | . 150000 rows × 17 columns . pd_c_dic = {} for i, nm in enumerate(pd.concat([df, df_test], axis=0)[&#39;PD_C&#39;].unique()): pd_c_dic[nm] = i . pd_c_dic . {578845: 0, 788068: 1, 180447: 2, 731145: 3, 216947: 4, 491876: 5, 1965: 6, 457181: 7, 190233: 8, 236174: 9, 248358: 10, 506359: 11, 506337: 12, 554217: 13, 436275: 14, 578537: 15, 536257: 16, 141349: 17, 535684: 18, 75955: 19, 29069: 20, 41283: 21, 489942: 22, 760527: 23, 233182: 24, 178471: 25, 64916: 26, 535442: 27, 535601: 28, 64933: 29, 72185: 30, 102618: 31, 809170: 32, 837392: 33, 765673: 34, 660999: 35, 804: 36, 375713: 37, 133407: 38, 742158: 39, 742089: 40, 484590: 41, 641183: 42, 68947: 43, 842488: 44, 225207: 45, 682061: 46, 569802: 47, 832202: 48, 221420: 49, 826313: 50, 276275: 51, 140325: 52, 694453: 53, 811116: 54, 585440: 55, 700995: 56, 7893: 57, 733585: 58, 185099: 59, 722909: 60, 611777: 61, 425921: 62, 499940: 63, 549195: 64, 220714: 65, 218301: 66, 626454: 67, 473112: 68, 739681: 69, 746888: 70, 657620: 71, 478990: 72, 676622: 73, 685372: 74, 186166: 75, 455026: 76, 454153: 77, 839324: 78, 523581: 79, 397797: 80, 594883: 81, 836917: 82, 689380: 83, 714091: 84, 554220: 85, 255735: 86, 255805: 87, 841644: 88, 530357: 89, 550764: 90, 217166: 91, 207163: 92, 783266: 93, 733465: 94, 310728: 95, 841642: 96, 634905: 97, 111788: 98, 798665: 99, 740039: 100, 155106: 101, 793821: 102, 498418: 103, 265906: 104, 717107: 105, 789521: 106, 209960: 107, 369827: 108, 412912: 109, 352717: 110, 772781: 111, 664328: 112, 520714: 113, 448099: 114, 315356: 115, 621557: 116, 242314: 117, 577673: 118, 332489: 119, 221077: 120, 339653: 121, 360023: 122, 164956: 123, 217672: 124, 825162: 125, 210784: 126, 657088: 127, 807682: 128, 812803: 129, 782362: 130, 624221: 131, 750564: 132, 427373: 133, 159530: 134, 600643: 135, 659195: 136, 5609: 137, 809732: 138, 744190: 139, 314843: 140, 117061: 141, 72033: 142, 6353: 143, 500350: 144, 651086: 145, 777470: 146, 261331: 147, 406787: 148, 151256: 149, 323680: 150, 696708: 151, 818129: 152, 377510: 153, 736885: 154, 107273: 155, 274604: 156, 149519: 157, 119734: 158, 814520: 159, 418525: 160, 742656: 161, 176758: 162, 193137: 163, 448598: 164, 665779: 165, 222: 166, 812291: 167, 199507: 168, 595905: 169, 431974: 170, 480855: 171, 234515: 172, 417612: 173, 589529: 174, 635789: 175, 402421: 176, 21386: 177, 59919: 178, 59915: 179, 45864: 180, 42760: 181, 788073: 182, 24286: 183, 237980: 184, 279738: 185, 830988: 186, 831995: 187, 657509: 188, 599806: 189, 520208: 190, 678491: 191, 328975: 192, 225011: 193, 704761: 194, 331305: 195, 236745: 196, 331313: 197, 331315: 198, 325462: 199, 754008: 200, 525734: 201, 316297: 202, 234321: 203, 552345: 204, 809795: 205, 656134: 206, 482240: 207, 224538: 208, 190919: 209, 264638: 210, 705025: 211, 526416: 212, 488965: 213, 353082: 214, 484753: 215, 673029: 216, 668377: 217, 668381: 218, 203849: 219, 847487: 220, 624244: 221, 130018: 222, 391390: 223, 305827: 224, 439597: 225, 844427: 226, 742356: 227, 614387: 228, 347882: 229, 417009: 230, 703034: 231, 684955: 232, 833340: 233, 484185: 234, 384940: 235, 741257: 236, 524284: 237, 230466: 238, 425801: 239, 248687: 240, 465628: 241, 528909: 242, 843603: 243, 801330: 244, 188144: 245, 823839: 246, 672920: 247, 248047: 248, 838123: 249, 773995: 250, 464972: 251, 773992: 252, 846851: 253, 469740: 254, 580208: 255, 778734: 256, 667897: 257, 522054: 258, 742108: 259, 89333: 260, 40972: 261, 214597: 262, 835321: 263, 627769: 264, 771557: 265, 17101: 266, 17095: 267, 17100: 268, 179067: 269, 695935: 270, 836099: 271, 564962: 272, 432361: 273, 742269: 274, 497067: 275, 835143: 276, 590408: 277, 1967: 278, 1974: 279, 415723: 280, 789560: 281, 789582: 282, 273953: 283, 588278: 284, 218470: 285, 164930: 286, 539083: 287, 128079: 288, 182425: 289, 803165: 290, 804957: 291, 350387: 292, 590047: 293, 661770: 294, 337522: 295, 337517: 296, 350336: 297, 340910: 298, 98359: 299, 589994: 300, 215829: 301, 220490: 302, 199326: 303, 70943: 304, 783193: 305, 776237: 306, 481452: 307, 648696: 308, 308641: 309, 379286: 310, 548654: 311, 634550: 312, 298844: 313, 584080: 314, 525226: 315, 377512: 316, 132566: 317, 835090: 318, 345871: 319, 793807: 320, 214124: 321, 589276: 322, 786274: 323, 798040: 324, 840782: 325, 827708: 326, 556576: 327, 556736: 328, 556575: 329, 786573: 330, 839528: 331, 787263: 332, 774268: 333, 793173: 334, 787900: 335, 795003: 336, 240984: 337, 839518: 338, 787901: 339, 774271: 340, 511275: 341, 190234: 342, 732222: 343, 687109: 344, 715428: 345, 282505: 346, 484979: 347, 24736: 348, 75417: 349, 91765: 350, 220251: 351, 737516: 352, 419017: 353, 246686: 354, 365338: 355, 475039: 356, 146717: 357, 542723: 358, 70388: 359, 369894: 360, 710881: 361, 363761: 362, 540267: 363, 607801: 364, 339928: 365, 231953: 366, 433158: 367, 405531: 368, 9622: 369, 362816: 370, 346349: 371, 647567: 372, 820596: 373, 702687: 374, 765348: 375, 430918: 376, 384838: 377, 699485: 378, 700083: 379, 454775: 380, 694510: 381, 106165: 382, 835536: 383, 724248: 384, 477852: 385, 763182: 386, 427529: 387, 1555: 388, 3627: 389, 138127: 390, 499565: 391, 189112: 392, 55505: 393, 818150: 394, 402455: 395, 226541: 396, 499571: 397, 398276: 398, 751647: 399, 422559: 400, 62544: 401, 20778: 402, 80953: 403, 54442: 404, 616222: 405, 398777: 406, 166466: 407, 43168: 408, 164222: 409, 608512: 410, 692709: 411, 667520: 412, 528952: 413, 233088: 414, 685637: 415, 844451: 416, 325784: 417, 233093: 418, 586706: 419, 232364: 420, 808125: 421, 610038: 422, 231869: 423, 471376: 424, 116151: 425, 835793: 426, 224479: 427, 350816: 428, 467676: 429, 593242: 430, 92054: 431, 695295: 432, 106313: 433, 261184: 434, 457338: 435, 392975: 436, 187406: 437, 818235: 438, 788184: 439, 547987: 440, 90726: 441, 587688: 442, 813725: 443, 831581: 444, 119440: 445, 82412: 446, 467056: 447, 801428: 448, 737381: 449, 153479: 450, 542721: 451, 237334: 452, 723410: 453, 732165: 454, 654287: 455, 817724: 456, 606643: 457, 461583: 458, 337679: 459, 178427: 460, 183546: 461, 301060: 462, 179311: 463, 564444: 464, 369907: 465, 704429: 466, 466095: 467, 576935: 468, 648484: 469, 26786: 470, 26781: 471, 170172: 472, 783315: 473, 325027: 474, 427578: 475, 92959: 476, 39798: 477, 10617: 478, 22503: 479, 746106: 480, 448458: 481, 662207: 482, 338225: 483, 358308: 484, 339641: 485, 686956: 486, 746637: 487, 274520: 488, 396880: 489, 805342: 490, 721493: 491, 621954: 492, 743591: 493, 163986: 494, 771158: 495, 685591: 496, 805287: 497, 795994: 498, 219950: 499, 613327: 500, 143001: 501, 221502: 502, 686598: 503, 77784: 504, 70824: 505, 44083: 506, 62448: 507, 831368: 508, 571977: 509, 346353: 510, 427398: 511, 239291: 512, 802617: 513, 1991: 514, 319558: 515, 470374: 516, 802533: 517, 355423: 518, 612430: 519, 285225: 520, 612192: 521, 101236: 522, 285226: 523, 514632: 524, 225375: 525, 397026: 526, 451206: 527, 361266: 528, 180640: 529, 796170: 530, 448112: 531, 707723: 532, 463914: 533, 318514: 534, 678593: 535, 721593: 536, 134109: 537, 782256: 538, 807086: 539, 484066: 540, 484021: 541, 278269: 542, 730248: 543, 549163: 544, 228969: 545, 290962: 546, 451855: 547, 579504: 548, 813584: 549, 531745: 550, 84088: 551, 538938: 552, 673948: 553, 339654: 554, 637249: 555, 236205: 556, 565534: 557, 751437: 558, 462370: 559, 524623: 560, 131621: 561, 166295: 562, 146757: 563, 37591: 564, 37589: 565, 228819: 566, 157438: 567, 737833: 568, 331291: 569, 845920: 570, 766109: 571, 847340: 572, 95376: 573, 283474: 574, 484910: 575, 464142: 576, 484914: 577, 21527: 578, 139318: 579, 68262: 580, 744813: 581, 304719: 582, 455203: 583, 382491: 584, 462218: 585, 812355: 586, 602936: 587, 177621: 588, 633165: 589, 737465: 590, 439241: 591, 638834: 592, 745992: 593, 100581: 594, 600886: 595, 295538: 596, 737019: 597, 649639: 598, 433922: 599, 348994: 600, 325866: 601, 503810: 602, 168943: 603, 793984: 604, 288716: 605, 398993: 606, 602742: 607, 185590: 608, 737456: 609, 185451: 610, 179794: 611, 633205: 612, 507445: 613, 737108: 614, 492100: 615, 489573: 616, 426460: 617, 643719: 618, 381421: 619, 498898: 620, 86035: 621, 8044: 622, 584546: 623, 737770: 624, 457825: 625, 514057: 626, 351544: 627, 189859: 628, 737744: 629, 757621: 630, 812389: 631, 731144: 632, 737753: 633, 807229: 634, 296651: 635, 273453: 636, 588188: 637, 248898: 638, 813189: 639, 838160: 640, 373139: 641, 423749: 642, 775255: 643, 802474: 644, 2574: 645, 701460: 646, 731231: 647, 385727: 648, 405952: 649, 358419: 650, 516983: 651, 666218: 652, 396146: 653, 333489: 654, 462824: 655, 286237: 656, 587449: 657, 803232: 658, 143555: 659, 263449: 660, 141498: 661, 741797: 662, 175170: 663, 742014: 664, 156769: 665, 1959: 666, 130354: 667, 818172: 668, 818170: 669, 333607: 670, 826962: 671, 737200: 672, 125314: 673, 405643: 674, 363078: 675, 222557: 676, 675999: 677, 680819: 678, 8781: 679, 701610: 680, 496043: 681, 421603: 682, 197538: 683, 332857: 684, 574687: 685, 574728: 686, 347557: 687, 196508: 688, 303129: 689, 127472: 690, 563385: 691, 771558: 692, 349862: 693, 710755: 694, 27493: 695, 134810: 696, 59137: 697, 742702: 698, 482531: 699, 339054: 700, 142758: 701, 668157: 702, 699665: 703, 518765: 704, 7010: 705, 792983: 706, 637455: 707, 812091: 708, 736142: 709, 5671: 710, 763472: 711, 791368: 712, 719827: 713, 800276: 714, 156818: 715, 156821: 716, 678668: 717, 334785: 718, 62002: 719, 379743: 720, 63194: 721, 279649: 722, 174418: 723, 799721: 724, 673660: 725, 158936: 726, 147650: 727, 550249: 728, 523074: 729, 732025: 730, 268104: 731, 653293: 732, 255244: 733, 232737: 734, 689564: 735, 343289: 736, 64438: 737, 496201: 738, 295663: 739, 729276: 740, 449614: 741, 810725: 742, 681618: 743, 513766: 744, 688070: 745, 470469: 746, 824246: 747, 98085: 748, 674014: 749, 352123: 750, 467361: 751, 290426: 752, 77621: 753, 8143: 754, 357620: 755, 671211: 756, 807545: 757, 104938: 758, 446901: 759, 462910: 760, 450004: 761, 456907: 762, 539982: 763, 594892: 764, 684241: 765, 705211: 766, 547370: 767, 38078: 768, 661752: 769, 18096: 770, 34901: 771, 243436: 772, 666769: 773, 106113: 774, 337126: 775, 132486: 776, 221243: 777, 150551: 778, 109616: 779, 669572: 780, 411683: 781, 274759: 782, 302954: 783, 591447: 784, 46814: 785, 83872: 786, 224211: 787, 492664: 788, 679395: 789, 73461: 790, 706836: 791, 125551: 792, 601039: 793, 600923: 794, 134168: 795, 456811: 796, 366070: 797, 335108: 798, 226114: 799, 846665: 800, 91249: 801, 340190: 802, 780899: 803, 489294: 804, 489616: 805, 780898: 806, 31803: 807, 32543: 808, 32539: 809, 32546: 810, 305699: 811, 155435: 812, 787614: 813, 31880: 814, 604488: 815, 43292: 816, 140109: 817, 31871: 818, 450598: 819, 791897: 820, 784275: 821, 32541: 822, 95079: 823, 14957: 824, 747812: 825, 23085: 826, 699048: 827, 405748: 828, 847121: 829, 440300: 830, 183258: 831, 810085: 832, 394172: 833, 27852: 834, 447552: 835, 133610: 836, 810083: 837, 23082: 838, 191422: 839, 191427: 840, 191424: 841, 212054: 842, 154999: 843, 222951: 844, 101612: 845, 638673: 846, 578512: 847, 643030: 848, 259725: 849, 679889: 850, 108585: 851, 115889: 852, 113999: 853, 95077: 854, 825163: 855, 625310: 856, 352237: 857, 525279: 858, 58764: 859, 50185: 860, 428396: 861, 631492: 862, 814295: 863, 747182: 864, 343507: 865, 735510: 866, 561804: 867, 343675: 868, 527221: 869, 365009: 870, 727523: 871, 255857: 872, 354836: 873, 83748: 874, 359218: 875, 295051: 876, 465400: 877, 317252: 878, 338161: 879, 349436: 880, 120008: 881, 155128: 882, 111253: 883, 128018: 884, 149651: 885, 391579: 886, 85339: 887, 2310: 888, 178122: 889, 67884: 890, 615582: 891, 726154: 892, 719174: 893, 733764: 894, 134830: 895, 463058: 896, 463062: 897, 655621: 898, 750374: 899, 260824: 900, 125584: 901, 469647: 902, 444736: 903, 53376: 904, 724836: 905, 432644: 906, 128837: 907, 535774: 908, 286948: 909, 570870: 910, 201175: 911, 352958: 912, 128829: 913, 785097: 914, 7514: 915, 835836: 916, 8460: 917, 801491: 918, 754998: 919, 390491: 920, 716500: 921, 605825: 922, 574707: 923, 742651: 924, 530833: 925, 737665: 926, 481765: 927, 742654: 928, 671328: 929, 201595: 930, 722733: 931, 642112: 932, 647750: 933, 742311: 934, 742625: 935, 737808: 936, 190230: 937, 112365: 938, 195072: 939, 195071: 940, 689788: 941, 334695: 942, 167536: 943, 671828: 944, 563784: 945, 27708: 946, 350247: 947, 72324: 948, 72394: 949, 840047: 950, 33795: 951, 2376: 952, 772365: 953, 802906: 954, 19558: 955, 253332: 956, 547977: 957, 742747: 958, 677214: 959, 840541: 960, 826939: 961, 346356: 962, 320963: 963, 690246: 964, 567651: 965, 514547: 966, 193356: 967, 377673: 968, 526080: 969, 350581: 970, 536810: 971, 709293: 972, 710074: 973, 326193: 974, 677217: 975, 730706: 976, 654997: 977, 763454: 978, 313387: 979, 5197: 980, 812280: 981, 118621: 982, 835: 983, 538088: 984, 48019: 985, 677215: 986, 212710: 987, 203160: 988, 481045: 989, 458353: 990, 632139: 991, 295137: 992, 844453: 993, 302856: 994, 244483: 995, 538085: 996, 9991: 997, 465895: 998, 599577: 999, ...} . data[&#39;pd_c&#39;] = data[&#39;pd_c&#39;].map(lambda x: [pd_c_dic[nm] for nm in x]) . 클라이언트 별 각 대 중 소 분류 검색 누적 숫자 =&gt; matrix 변환 | . clac1_nm_dic = {} for i, nm in enumerate(df[&#39;CLAC1_NM&#39;].unique()): clac1_nm_dic[nm] = i . clac1_nm_dic . {&#39;생활/주방가전&#39;: 0, &#39;침구/수예&#39;: 1, &#39;시즌스포츠&#39;: 2, &#39;화장품/뷰티케어&#39;: 3, &#39;건강식품&#39;: 4, &#39;속옷/양말/홈웨어&#39;: 5, &#39;스포츠패션&#39;: 6, &#39;패션잡화&#39;: 7, &#39;주방잡화&#39;: 8, &#39;유아동의류&#39;: 9, &#39;식기/조리기구&#39;: 10, &#39;세제/위생&#39;: 11, &#39;청소/세탁/욕실용품&#39;: 12, &#39;출산/육아용품&#39;: 13, &#39;냉장/세탁가전&#39;: 14, &#39;퍼스널케어&#39;: 15, &#39;구기/필드스포츠&#39;: 16, &#39;원예/애완&#39;: 17, &#39;음료&#39;: 18, &#39;여성의류&#39;: 19, &#39;남성의류&#39;: 20, &#39;계절가전&#39;: 21, &#39;냉장식품&#39;: 22, &#39;아웃도어/레저&#39;: 23, &#39;냉동식품&#39;: 24, &#39;가구&#39;: 25, &#39;완구&#39;: 26, &#39;헬스/피트니스&#39;: 27, &#39;축산물&#39;: 28, &#39;컴퓨터&#39;: 29, &#39;모바일&#39;: 30, &#39;문구/사무용품&#39;: 31, &#39;인테리어/조명&#39;: 32, &#39;상품권&#39;: 33, &#39;과일&#39;: 34, &#39;자동차용품&#39;: 35, &#39;영상/음향가전&#39;: 36} . data[&#39;clac1_nm&#39;] = data[&#39;clac1_nm&#39;].map(lambda x: [clac1_nm_dic[nm] for nm in x]) . clac1_matrix = np.zeros((len(data), len(clac1_nm_dic))) for i in range(len(data)): for j in data[&#39;clac1_nm&#39;][i]: clac1_matrix[i, j] += 1 . 분류별 구매 횟수 계산 행렬 | . clac1_matrix . array([[1., 1., 0., ..., 0., 0., 0.], [0., 0., 2., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 3., ..., 0., 0., 0.]]) . cols = [&#39;clac1_nm_&#39; + str(i) for i in range(len(clac1_nm_dic))] . clac1_df = pd.DataFrame(clac1_matrix) clac1_df.columns = cols . clac1_df . clac1_nm_0 clac1_nm_1 clac1_nm_2 clac1_nm_3 clac1_nm_4 clac1_nm_5 clac1_nm_6 clac1_nm_7 clac1_nm_8 clac1_nm_9 ... clac1_nm_27 clac1_nm_28 clac1_nm_29 clac1_nm_30 clac1_nm_31 clac1_nm_32 clac1_nm_33 clac1_nm_34 clac1_nm_35 clac1_nm_36 . 0 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 0.0 | 0.0 | 2.0 | 6.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 10.0 | 0.0 | 3.0 | 1.0 | 1.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 149995 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149996 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149997 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149998 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149999 0.0 | 0.0 | 3.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 150000 rows × 37 columns . clac2_nm_dic = {} for i, nm in enumerate(df[&#39;CLAC2_NM&#39;].unique()): clac2_nm_dic[nm] = i . clac2_nm_dic . {&#39;주방가전&#39;: 0, &#39;수예소품&#39;: 1, &#39;수영/물놀이&#39;: 2, &#39;스킨케어&#39;: 3, &#39;선케어&#39;: 4, &#39;홍삼/인삼가공식품&#39;: 5, &#39;메이크업&#39;: 6, &#39;여성속옷&#39;: 7, &#39;여성스포츠화&#39;: 8, &#39;유아동속옷&#39;: 9, &#39;유아동양말류&#39;: 10, &#39;여성화&#39;: 11, &#39;조리도구&#39;: 12, &#39;유아의류전신&#39;: 13, &#39;모자&#39;: 14, &#39;조리기구&#39;: 15, &#39;화장지/티슈&#39;: 16, &#39;정리용품&#39;: 17, &#39;유아스킨/바디케어&#39;: 18, &#39;냉장/냉동고&#39;: 19, &#39;핸드/풋케어&#39;: 20, &#39;남성일반스포츠의류&#39;: 21, &#39;골프&#39;: 22, &#39;남성지갑&#39;: 23, &#39;남성스포츠화&#39;: 24, &#39;애견용품&#39;: 25, &#39;생수&#39;: 26, &#39;여성일반스포츠의류&#39;: 27, &#39;건강진액&#39;: 28, &#39;남성케어&#39;: 29, &#39;우산/양산류&#39;: 30, &#39;스포츠잡화&#39;: 31, &#39;남성속옷&#39;: 32, &#39;여성의류상의&#39;: 33, &#39;여성의류전신&#39;: 34, &#39;유아동침구&#39;: 35, &#39;남성의류상의&#39;: 36, &#39;남성의류하의&#39;: 37, &#39;여성의류아우터&#39;: 38, &#39;냉방가전&#39;: 39, &#39;유아의류상의&#39;: 40, &#39;여아의류상의&#39;: 41, &#39;여성위생용품&#39;: 42, &#39;유아위생용품&#39;: 43, &#39;영양제&#39;: 44, &#39;포장반찬&#39;: 45, &#39;남성등산/아웃도어의류&#39;: 46, &#39;유아동스포츠화&#39;: 47, &#39;헤어케어&#39;: 48, &#39;캠핑&#39;: 49, &#39;등산&#39;: 50, &#39;여성가방&#39;: 51, &#39;유아의류하의&#39;: 52, &#39;고양이용품&#39;: 53, &#39;구강케어&#39;: 54, &#39;남성의류아우터&#39;: 55, &#39;냉동간편식&#39;: 56, &#39;수납가구&#39;: 57, &#39;사무용/학생용가구&#39;: 58, &#39;수유/이유용품&#39;: 59, &#39;유아발육용품&#39;: 60, &#39;유아동화&#39;: 61, &#39;여성의류하의&#39;: 62, &#39;교육완구&#39;: 63, &#39;피트니스&#39;: 64, &#39;그릇/식기&#39;: 65, &#39;남성골프의류&#39;: 66, &#39;여성골프의류&#39;: 67, &#39;거실가구&#39;: 68, &#39;닭고기류&#39;: 69, &#39;남아의류상의&#39;: 70, &#39;홈웨어&#39;: 71, &#39;주방가구&#39;: 72, &#39;밀폐/보관용기&#39;: 73, &#39;시계&#39;: 74, &#39;바디케어&#39;: 75, &#39;여행용가방류&#39;: 76, &#39;기능성음료&#39;: 77, &#39;캐쥬얼가방&#39;: 78, &#39;컴퓨터/노트북&#39;: 79, &#39;침실가구&#39;: 80, &#39;모바일액세서리&#39;: 81, &#39;성인침구&#39;: 82, &#39;일반문구/사무용품&#39;: 83, &#39;필기도구&#39;: 84, &#39;안경/선글라스&#39;: 85, &#39;여아의류하의&#39;: 86, &#39;남아의류하의&#39;: 87, &#39;커튼/블라인드류&#39;: 88, &#39;여성지갑&#39;: 89, &#39;욕실용품&#39;: 90, &#39;남아완구&#39;: 91, &#39;세탁세제&#39;: 92, &#39;여성등산/아웃도어의류&#39;: 93, &#39;패션액세서리&#39;: 94, &#39;컴퓨터주변기기&#39;: 95, &#39;남아의류세트&#39;: 96, &#39;청소기&#39;: 97, &#39;여성양말류&#39;: 98, &#39;미용소품&#39;: 99, &#39;여아완구&#39;: 100, &#39;유아안전용품&#39;: 101, &#39;모바일상품권&#39;: 102, &#39;남성화&#39;: 103, &#39;이미용가전&#39;: 104, &#39;향수&#39;: 105, &#39;주방정리용품/소모품&#39;: 106, &#39;보석&#39;: 107, &#39;남성가방&#39;: 108, &#39;남성양말류&#39;: 109, &#39;공기청정/가습/제습&#39;: 110, &#39;여아의류아우터&#39;: 111, &#39;건강보조식품&#39;: 112, &#39;유아동가구&#39;: 113, &#39;인라인/스케이트보드/킥보드&#39;: 114, &#39;두유&#39;: 115, &#39;견과류&#39;: 116, &#39;자동차음향/가전기기&#39;: 117, &#39;유아동일반스포츠의류&#39;: 118, &#39;국산과일&#39;: 119, &#39;세탁기&#39;: 120, &#39;모바일기기&#39;: 121, &#39;유아의류아우터&#39;: 122, &#39;남성의류세트&#39;: 123, &#39;카메라/캠코더&#39;: 124, &#39;TV&#39;: 125, &#39;축산선물세트&#39;: 126, &#39;시공/DIY가구&#39;: 127} . data[&#39;clac2_nm&#39;] = data[&#39;clac2_nm&#39;].map(lambda x: [clac2_nm_dic[nm] for nm in x]) . clac2_matrix = np.zeros((len(data), len(clac2_nm_dic))) for i in range(len(data)): for j in data[&#39;clac2_nm&#39;][i]: clac2_matrix[i, j] += 1 . clac2_matrix . array([[1., 1., 0., ..., 0., 0., 0.], [0., 0., 2., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 3., ..., 0., 0., 0.]]) . cols = [&#39;clac2_nm_&#39; + str(i) for i in range(len(clac2_nm_dic))] . clac2_df = pd.DataFrame(clac2_matrix) clac2_df.columns = cols . clac2_df . clac2_nm_0 clac2_nm_1 clac2_nm_2 clac2_nm_3 clac2_nm_4 clac2_nm_5 clac2_nm_6 clac2_nm_7 clac2_nm_8 clac2_nm_9 ... clac2_nm_118 clac2_nm_119 clac2_nm_120 clac2_nm_121 clac2_nm_122 clac2_nm_123 clac2_nm_124 clac2_nm_125 clac2_nm_126 clac2_nm_127 . 0 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 0.0 | 0.0 | 2.0 | 4.0 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 149995 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149996 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149997 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149998 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149999 0.0 | 0.0 | 3.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 150000 rows × 128 columns . clac3_nm_dic = {} for i, nm in enumerate(df[&#39;CLAC3_NM&#39;].unique()): clac3_nm_dic[nm] = i . clac3_nm_dic . {&#39;블랜더&#39;: 0, &#39;거실수예소품&#39;: 1, &#39;남성수영복&#39;: 2, &#39;페이셜클렌저&#39;: 3, &#39;선크림류&#39;: 4, &#39;여성비치웨어&#39;: 5, &#39;홍삼액&#39;: 6, &#39;BB/파운데이션/컴팩트류&#39;: 7, &#39;에센스/세럼&#39;: 8, &#39;여성속옷세트&#39;: 9, &#39;브래지어&#39;: 10, &#39;여성팬티&#39;: 11, &#39;여성스포츠샌들/슬리퍼&#39;: 12, &#39;유아동팬티&#39;: 13, &#39;유아동일반양말&#39;: 14, &#39;여성샌들&#39;: 15, &#39;유아동타이즈&#39;: 16, &#39;주방칼/가위&#39;: 17, &#39;영유아점프수트/오버롤&#39;: 18, &#39;아동모&#39;: 19, &#39;프라이팬&#39;: 20, &#39;롤티슈&#39;: 21, &#39;플라스틱서랍장&#39;: 22, &#39;유아용화장품&#39;: 23, &#39;일반형냉장고&#39;: 24, &#39;핸드로션/크림&#39;: 25, &#39;남성스포츠티셔츠&#39;: 26, &#39;크림/밤/오일&#39;: 27, &#39;골프공&#39;: 28, &#39;골프연습장비&#39;: 29, &#39;립글로즈/틴트&#39;: 30, &#39;남성일반지갑&#39;: 31, &#39;남성런닝/트레이닝화&#39;: 32, &#39;애견주거/실내용품&#39;: 33, &#39;생수&#39;: 34, &#39;여성트레이닝복&#39;: 35, &#39;여성런닝셔츠/캐미솔&#39;: 36, &#39;채소즙&#39;: 37, &#39;남성용스킨케어류&#39;: 38, &#39;3단우산&#39;: 39, &#39;스포츠가방&#39;: 40, &#39;남성팬티&#39;: 41, &#39;여성남방셔츠&#39;: 42, &#39;여성원피스&#39;: 43, &#39;유아동이불/이불커버&#39;: 44, &#39;남성티셔츠&#39;: 45, &#39;남성캐주얼바지&#39;: 46, &#39;여성코트&#39;: 47, &#39;남성청바지&#39;: 48, &#39;기타여성속옷&#39;: 49, &#39;기타냉방가전&#39;: 50, &#39;영유아티셔츠/탑&#39;: 51, &#39;여아티셔츠/탑&#39;: 52, &#39;생리대&#39;: 53, &#39;유아용기저귀&#39;: 54, &#39;유산균/프로바이오틱스&#39;: 55, &#39;김치류&#39;: 56, &#39;남성등산바지&#39;: 57, &#39;유아동스포츠샌들/슬리퍼&#39;: 58, &#39;골프패션잡화&#39;: 59, &#39;염모제&#39;: 60, &#39;장우산&#39;: 61, &#39;골프필드용품&#39;: 62, &#39;텐트&#39;: 63, &#39;기타에어컨&#39;: 64, &#39;여성스니커즈&#39;: 65, &#39;남성정장셔츠&#39;: 66, &#39;토스터/제빵기&#39;: 67, &#39;선풍기&#39;: 68, &#39;유아동런닝셔츠&#39;: 69, &#39;배낭&#39;: 70, &#39;여성크로스백&#39;: 71, &#39;남성런닝셔츠&#39;: 72, &#39;남성남방셔츠&#39;: 73, &#39;여성스웨터/풀오버&#39;: 74, &#39;영유아스커트&#39;: 75, &#39;여아가디건&#39;: 76, &#39;아동수영복&#39;: 77, &#39;미스트&#39;: 78, &#39;스킨/토너&#39;: 79, &#39;애견장난감/훈련&#39;: 80, &#39;고양이캣타워/실내용품&#39;: 81, &#39;기타구강관리용품&#39;: 82, &#39;남성등산티셔츠&#39;: 83, &#39;남성점퍼&#39;: 84, &#39;팬티라이너&#39;: 85, &#39;여성로퍼&#39;: 86, &#39;여성티셔츠/탑&#39;: 87, &#39;냉동국탕류&#39;: 88, &#39;서랍장/수납장&#39;: 89, &#39;책상의자&#39;: 90, &#39;스킨케어세트&#39;: 91, &#39;페이셜팩류&#39;: 92, &#39;출산/신생아용품세트&#39;: 93, &#39;아기띠/캐리어&#39;: 94, &#39;여성스포츠티셔츠/탑&#39;: 95, &#39;유아동샌들&#39;: 96, &#39;여성바지&#39;: 97, &#39;영유아원피스&#39;: 98, &#39;남성정장바지&#39;: 99, &#39;남성일반스포츠바지&#39;: 100, &#39;미술/창작완구&#39;: 101, &#39;기타요가/필라테스소품&#39;: 102, &#39;숟가락/젓가락&#39;: 103, &#39;아쿠아슈즈&#39;: 104, &#39;여성재킷&#39;: 105, &#39;영유아바지&#39;: 106, &#39;남성골프바지&#39;: 107, &#39;유아동침구세트&#39;: 108, &#39;유아/아동용치약&#39;: 109, &#39;스포츠모자&#39;: 110, &#39;유아동슬리퍼&#39;: 111, &#39;여성골프패딩&#39;: 112, &#39;여성숄더백&#39;: 113, &#39;야구모자&#39;: 114, &#39;유아동스니커즈&#39;: 115, &#39;헤어에센스&#39;: 116, &#39;탁자&#39;: 117, &#39;닭가슴살&#39;: 118, &#39;유아동내의&#39;: 119, &#39;남아티셔츠/탑&#39;: 120, &#39;오리발/스노클링&#39;: 121, &#39;남아잠옷&#39;: 122, &#39;식탁의자&#39;: 123, &#39;유아용물티슈&#39;: 124, &#39;애견간식&#39;: 125, &#39;여성플랫&#39;: 126, &#39;애견사료&#39;: 127, &#39;반찬통/밀폐용기&#39;: 128, &#39;어린이홍삼&#39;: 129, &#39;스포츠시계&#39;: 130, &#39;고양이모래/배변용품&#39;: 131, &#39;남성스포츠샌들/슬리퍼&#39;: 132, &#39;혼합즙&#39;: 133, &#39;아이브로우&#39;: 134, &#39;아이케어&#39;: 135, &#39;바디워시&#39;: 136, &#39;유아용샴푸/바디워시&#39;: 137, &#39;샴푸&#39;: 138, &#39;남성가디건&#39;: 139, &#39;여성가디건&#39;: 140, &#39;캐리어&#39;: 141, &#39;한방음료&#39;: 142, &#39;아동용가방&#39;: 143, &#39;여성일반스포츠바지&#39;: 144, &#39;노트북&#39;: 145, &#39;메이크업세트&#39;: 146, &#39;여성임부속옷&#39;: 147, &#39;접시&#39;: 148, &#39;반상기세트/홈세트&#39;: 149, &#39;장롱&#39;: 150, &#39;매트리스&#39;: 151, &#39;여성점퍼&#39;: 152, &#39;홈웨어세트&#39;: 153, &#39;남성골프티셔츠&#39;: 154, &#39;여성스웨트셔츠/후드/집업&#39;: 155, &#39;남성비치웨어&#39;: 156, &#39;기타모바일액세서리&#39;: 157, &#39;성인침구속통/솜&#39;: 158, &#39;바구니&#39;: 159, &#39;테이프&#39;: 160, &#39;칼/가위&#39;: 161, &#39;수정용품&#39;: 162, &#39;유아동선글라스&#39;: 163, &#39;여성클러치백&#39;: 164, &#39;영유아청바지&#39;: 165, &#39;캠핑테이블/의자&#39;: 166, &#39;여아바지&#39;: 167, &#39;남아청바지&#39;: 168, &#39;고양이사료&#39;: 169, &#39;커튼&#39;: 170, &#39;기타물놀이용품&#39;: 171, &#39;여성신발부속품&#39;: 172, &#39;노트북가방&#39;: 173, &#39;여성카드/명함지갑&#39;: 174, &#39;책상&#39;: 175, &#39;여성스커트&#39;: 176, &#39;욕실발판&#39;: 177, &#39;마스카라&#39;: 178, &#39;남성시계&#39;: 179, &#39;남성패딩&#39;: 180, &#39;식탁세트&#39;: 181, &#39;피규어&#39;: 182, &#39;욕실소품&#39;: 183, &#39;롤플레잉완구&#39;: 184, &#39;기타캠핑용품&#39;: 185, &#39;분말표백제&#39;: 186, &#39;풀&#39;: 187, &#39;종합영양제&#39;: 188, &#39;피트니스용품&#39;: 189, &#39;스툴/리빙의자&#39;: 190, &#39;책장&#39;: 191, &#39;칫솔&#39;: 192, &#39;수건&#39;: 193, &#39;액상세탁세제&#39;: 194, &#39;영유아블라우스&#39;: 195, &#39;남아바지&#39;: 196, &#39;헤어케어선물세트&#39;: 197, &#39;여성등산티셔츠/탑&#39;: 198, &#39;남성베스트&#39;: 199, &#39;썬캡&#39;: 200, &#39;스카프&#39;: 201, &#39;블러셔/쉐이딩/하이라이터&#39;: 202, &#39;여아레깅스&#39;: 203, &#39;애견목욕/위생용품&#39;: 204, &#39;기타일반문구/사무용품&#39;: 205, &#39;린스/컨디셔너&#39;: 206, &#39;샴푸/린스세트&#39;: 207, &#39;미용비누&#39;: 208, &#39;성인매트리스커버&#39;: 209, &#39;스냅백&#39;: 210, &#39;기타컴퓨터액세서리&#39;: 211, &#39;유아동플랫&#39;: 212, &#39;기타영양제&#39;: 213, &#39;여아남방셔츠&#39;: 214, &#39;남아의류세트&#39;: 215, &#39;탄산수&#39;: 216, &#39;일반청소기&#39;: 217, &#39;손싸개/발싸개&#39;: 218, &#39;유아동런닝/트레이닝화&#39;: 219, &#39;여성토트백&#39;: 220, &#39;여성일반양말&#39;: 221, &#39;여성런닝/트레이닝화&#39;: 222, &#39;치약&#39;: 223, &#39;샤워/목욕도구/목욕헤어밴드&#39;: 224, &#39;봉제인형&#39;: 225, &#39;유아동베개/베개커버&#39;: 226, &#39;놀이방매트&#39;: 227, &#39;식음료모바일상품권&#39;: 228, &#39;여아베스트&#39;: 229, &#39;성인베개/베개커버&#39;: 230, &#39;이유식용품&#39;: 231, &#39;아이섀도우&#39;: 232, &#39;남성샌들&#39;: 233, &#39;기타이미용가전&#39;: 234, &#39;요가/필라테스복&#39;: 235, &#39;여성향수&#39;: 236, &#39;기타정리용품&#39;: 237, &#39;트리트먼트/팩&#39;: 238, &#39;욕실청소용품&#39;: 239, &#39;기타주방정리용품/소모품&#39;: 240, &#39;여아청바지&#39;: 241, &#39;기름종이&#39;: 242, &#39;아이라이너&#39;: 243, &#39;홍삼/인삼혼합세트&#39;: 244, &#39;여성시계&#39;: 245, &#39;손수건&#39;: 246, &#39;팔찌&#39;: 247, &#39;남성숄더/크로스백&#39;: 248, &#39;지퍼백/비닐백&#39;: 249, &#39;성인패드/스프레드&#39;: 250, &#39;여성펌프스&#39;: 251, &#39;구강청정제&#39;: 252, &#39;남성일반양말&#39;: 253, &#39;여성오픈토&#39;: 254, &#39;공기청정기&#39;: 255, &#39;여성등산바지&#39;: 256, &#39;커튼링/커튼봉/부속품&#39;: 257, &#39;메이크업베이스/프라이머&#39;: 258, &#39;여아점퍼&#39;: 259, &#39;남성등산점퍼/재킷&#39;: 260, &#39;유아동슬립온&#39;: 261, &#39;인삼가공식품&#39;: 262, &#39;여성덧신류&#39;: 263, &#39;냉동핫도그&#39;: 264, &#39;여성블라우스&#39;: 265, &#39;식기건조대/수저통&#39;: 266, &#39;국자/뒤지개/주걱&#39;: 267, &#39;여성부츠&#39;: 268, &#39;여성베스트&#39;: 269, &#39;에멀젼/로션&#39;: 270, &#39;기타조리도구&#39;: 271, &#39;바디보습&#39;: 272, &#39;여성패딩&#39;: 273, &#39;성인이불/이불커버&#39;: 274, &#39;솥&#39;: 275, &#39;이불/옷압축팩&#39;: 276, &#39;섬유유연제/향기지속제&#39;: 277, &#39;양산&#39;: 278, &#39;여성수영복&#39;: 279, &#39;남성내의&#39;: 280, &#39;기타등산용품&#39;: 281, &#39;유아용세척용품&#39;: 282, &#39;도마&#39;: 283, &#39;생활모바일상품권&#39;: 284, &#39;젖병/젖꼭지&#39;: 285, &#39;아동우산&#39;: 286, &#39;케이스/보호필름&#39;: 287, &#39;남성스킨케어세트&#39;: 288, &#39;여성백팩&#39;: 289, &#39;커피머신&#39;: 290, &#39;방석/방석커버&#39;: 291, &#39;장식장/진열장&#39;: 292, &#39;다이어트보조식품&#39;: 293, &#39;남성스니커즈&#39;: 294, &#39;남성용클렌저&#39;: 295, &#39;소품가방&#39;: 296, &#39;변기시트/커버&#39;: 297, &#39;헤드웨어&#39;: 298, &#39;여행용소품&#39;: 299, &#39;목걸이&#39;: 300, &#39;미용보조식품&#39;: 301, &#39;제빵용품&#39;: 302, &#39;PC부품&#39;: 303, &#39;등산화&#39;: 304, &#39;젤네일/케어류&#39;: 305, &#39;핸디형청소기&#39;: 306, &#39;주방선반/걸이대&#39;: 307, &#39;옷걸이&#39;: 308, &#39;유아동침대&#39;: 309, &#39;로봇청소기&#39;: 310, &#39;헤어드라이어&#39;: 311, &#39;레저모바일상품권&#39;: 312, &#39;스폰지/퍼프&#39;: 313, &#39;커피용품&#39;: 314, &#39;커피잔&#39;: 315, &#39;저장장치&#39;: 316, &#39;우주복&#39;: 317, &#39;립스틱/립라이너&#39;: 318, &#39;조립/프라모델&#39;: 319, &#39;스케이트보드/킥보드&#39;: 320, &#39;남성로퍼&#39;: 321, &#39;여성쪼리&#39;: 322, &#39;여성트렌치코트&#39;: 323, &#39;남성정장재킷&#39;: 324, &#39;골프화&#39;: 325, &#39;남성트레이닝복&#39;: 326, &#39;전기찜기&#39;: 327, &#39;남성향수&#39;: 328, &#39;일반비타민&#39;: 329, &#39;여성슬링백&#39;: 330, &#39;촉각놀이/오뚝이&#39;: 331, &#39;기타패션잡화&#39;: 332, &#39;데오도란트&#39;: 333, &#39;수영모자&#39;: 334, &#39;여성슬리퍼&#39;: 335, &#39;고무장갑&#39;: 336, &#39;일반두유&#39;: 337, &#39;속눈썹/쌍꺼풀&#39;: 338, &#39;물안경&#39;: 339, &#39;기타여행용가방&#39;: 340, &#39;남성트렌치코트&#39;: 341, &#39;발찌&#39;: 342, &#39;남성스웨터/풀오버&#39;: 343, &#39;호두&#39;: 344, &#39;캠핑침구&#39;: 345, &#39;아동비치웨어&#39;: 346, &#39;애견의류/악세서리&#39;: 347, &#39;성인침구세트&#39;: 348, &#39;남성등산패딩&#39;: 349, &#39;냉동만두&#39;: 350, &#39;냄비&#39;: 351, &#39;남성캐주얼재킷&#39;: 352, &#39;승마운동기&#39;: 353, &#39;바디케어세트&#39;: 354, &#39;거들&#39;: 355, &#39;성인요/요커버&#39;: 356, &#39;여성내의&#39;: 357, &#39;루테인&#39;: 358, &#39;여성청바지&#39;: 359, &#39;여성잠옷&#39;: 360, &#39;인덕션/가스레인지&#39;: 361, &#39;캠핑취사&#39;: 362, &#39;마우스&#39;: 363, &#39;블랙박스&#39;: 364, &#39;포크/나이프&#39;: 365, &#39;남성속옷세트&#39;: 366, &#39;여성슬립온&#39;: 367, &#39;오메가3/기타추출오일&#39;: 368, &#39;헤어세팅기&#39;: 369, &#39;키보드&#39;: 370, &#39;모바일배터리/충전기&#39;: 371, &#39;채반/바구니/쟁반&#39;: 372, &#39;선반장/행거&#39;: 373, &#39;안경테&#39;: 374, &#39;여성골프바지&#39;: 375, &#39;커피메이커/포트&#39;: 376, &#39;유아동스포츠티셔츠/탑&#39;: 377, &#39;유아동스포츠스웨트셔츠/후드/집업&#39;: 378, &#39;고양이간식&#39;: 379, &#39;사무용/학생용가구세트&#39;: 380, &#39;벽걸이형에어컨&#39;: 381, &#39;여성등산점퍼/재킷&#39;: 382, &#39;홍삼정/분말/환&#39;: 383, &#39;토마토&#39;: 384, &#39;스킨케어디바이스&#39;: 385, &#39;입욕제/스파제품&#39;: 386, &#39;과일즙&#39;: 387, &#39;전기튀김기&#39;: 388, &#39;성인담요&#39;: 389, &#39;귀걸이&#39;: 390, &#39;소파&#39;: 391, &#39;행주&#39;: 392, &#39;여성골프남방셔츠&#39;: 393, &#39;영유아남방셔츠&#39;: 394, &#39;스텝퍼/트위스트&#39;: 395, &#39;여성등산패딩&#39;: 396, &#39;식탁&#39;: 397, &#39;전기밥솥&#39;: 398, &#39;대접/볼&#39;: 399, &#39;밥공기&#39;: 400, &#39;찬기/종지&#39;: 401, &#39;여성발가락양말&#39;: 402, &#39;역할놀이&#39;: 403, &#39;유아용카시트/매트&#39;: 404, &#39;젖병소독/건조용품&#39;: 405, &#39;유아/아동용칫솔&#39;: 406, &#39;기타냉동간편식&#39;: 407, &#39;유아목욕용품&#39;: 408, &#39;유아동트레이닝복&#39;: 409, &#39;여아잠옷&#39;: 410, &#39;선반/걸이&#39;: 411, &#39;여성양말선물세트&#39;: 412, &#39;물티슈&#39;: 413, &#39;남성코트&#39;: 414, &#39;분말세탁세제&#39;: 415, &#39;수유패드/보조용품&#39;: 416, &#39;유아동의자&#39;: 417, &#39;학생용가방&#39;: 418, &#39;남성스포츠점퍼/재킷&#39;: 419, &#39;유아공부상/디딤대&#39;: 420, &#39;잉크/토너&#39;: 421, &#39;여성향수세트&#39;: 422, &#39;펜던트&#39;: 423, &#39;여성일반지갑&#39;: 424, &#39;보드게임&#39;: 425, &#39;레고&#39;: 426, &#39;유아동스포츠점퍼/재킷&#39;: 427, &#39;치약/칫솔세트&#39;: 428, &#39;수영가방&#39;: 429, &#39;패션인형&#39;: 430, &#39;도시락/찬합&#39;: 431, &#39;발효원액&#39;: 432, &#39;남성등산베스트&#39;: 433, &#39;여성선글라스&#39;: 434, &#39;여성점프수트/오버롤&#39;: 435, &#39;치아발육기/딸랑이&#39;: 436, &#39;남성카드/명함지갑&#39;: 437, &#39;남성용선크림/메이크업류&#39;: 438, &#39;남성골프점퍼/재킷&#39;: 439, &#39;여성골프티셔츠/탑&#39;: 440, &#39;기타국산과일류&#39;: 441, &#39;여아스커트&#39;: 442, &#39;건조기&#39;: 443, &#39;기타기능성음료&#39;: 444, &#39;스피커&#39;: 445, &#39;얼음/빙수용품&#39;: 446, &#39;스타킹&#39;: 447, &#39;보온병/텀블러&#39;: 448, &#39;전동칫솔/칫솔모&#39;: 449, &#39;여아스웨트셔츠/후드/집업&#39;: 450, &#39;혼합견과&#39;: 451, &#39;볼펜&#39;: 452, &#39;필통&#39;: 453, &#39;샤프/샤프심&#39;: 454, &#39;필기구세트&#39;: 455, &#39;엽산/철분&#39;: 456, &#39;유아패션잡화&#39;: 457, &#39;휴대폰&#39;: 458, &#39;각티슈/미용티슈&#39;: 459, &#39;골프가방&#39;: 460, &#39;여성타이즈&#39;: 461, &#39;요가/스포츠매트&#39;: 462, &#39;여성골프스커트&#39;: 463, &#39;골프장갑&#39;: 464, &#39;여성골프니트/가디건&#39;: 465, &#39;여성골프베스트&#39;: 466, &#39;영유아점퍼&#39;: 467, &#39;영유아가디건&#39;: 468, &#39;남성스웨트셔츠/후드/집업&#39;: 469, &#39;사인펜&#39;: 470, &#39;남성선글라스&#39;: 471, &#39;반지&#39;: 472, &#39;이어폰/헤드폰&#39;: 473, &#39;조리도구세트&#39;: 474, &#39;키친타올&#39;: 475, &#39;남녀공용향수&#39;: 476, &#39;유아동레인부츠/슈즈&#39;: 477, &#39;유아건강보조제&#39;: 478, &#39;여성가운&#39;: 479, &#39;남성잠옷&#39;: 480, &#39;유아동스포츠패딩&#39;: 481, &#39;여아패딩&#39;: 482, &#39;전통/종교장신구&#39;: 483, &#39;복근/벨트마사지기구&#39;: 484, &#39;벙거지&#39;: 485, &#39;슬립&#39;: 486, &#39;만년필&#39;: 487, &#39;공병/모델링팩전용도구&#39;: 488, &#39;화장대&#39;: 489, &#39;핸드카트&#39;: 490, &#39;여성레인부츠/슈즈&#39;: 491, &#39;퍼즐&#39;: 492, &#39;캐쥬얼크로스백&#39;: 493, &#39;음악/악기완구&#39;: 494, &#39;아기체육관/러닝홈&#39;: 495, &#39;면봉/화장솜&#39;: 496, &#39;남성골프남방셔츠&#39;: 497, &#39;수예소품속통/솜&#39;: 498, &#39;쿠션/쿠션커버&#39;: 499, &#39;붙박이장&#39;: 500, &#39;기타견과류&#39;: 501, &#39;아몬드&#39;: 502, &#39;여성등산베스트&#39;: 503, &#39;영유아레깅스&#39;: 504, &#39;여성컴포트화&#39;: 505, &#39;남성정장화&#39;: 506, &#39;블라인드/버티컬&#39;: 507, &#39;스포츠두건/머플러/마스크&#39;: 508, &#39;남성골프패딩&#39;: 509, &#39;스포츠양말&#39;: 510, &#39;남성정장세트&#39;: 511, &#39;음료용컵&#39;: 512, &#39;인라인/스케이트보드/킥보드안전용품&#39;: 513, &#39;애견식기/물병&#39;: 514, &#39;복숭아&#39;: 515, &#39;글루코사민&#39;: 516, &#39;헤어브러쉬/롤&#39;: 517, &#39;거실화/실내화&#39;: 518, &#39;유아동담요&#39;: 519, &#39;고데기&#39;: 520, &#39;칼슘/미네랄&#39;: 521, &#39;주방수예소품&#39;: 522, &#39;무선조종&#39;: 523, &#39;수세미/솔&#39;: 524, &#39;유모차&#39;: 525, &#39;남성덧신류&#39;: 526, &#39;참외&#39;: 527, &#39;사과&#39;: 528, &#39;제습기&#39;: 529, &#39;양문형냉장고&#39;: 530, &#39;메이크업브러쉬&#39;: 531, &#39;가습기&#39;: 532, &#39;유아동요/요커버&#39;: 533, &#39;구명조끼/안전용품&#39;: 534, &#39;네일케어도구&#39;: 535, &#39;애견건강용품&#39;: 536, &#39;오븐/전자레인지&#39;: 537, &#39;압력솥&#39;: 538, &#39;기타유아동화&#39;: 539, &#39;유아동일반스포츠바지&#39;: 540, &#39;여성세정제&#39;: 541, &#39;비닐장갑&#39;: 542, &#39;스포츠선글라스&#39;: 543, &#39;미니자동차&#39;: 544, &#39;전자교육완구&#39;: 545, &#39;수박&#39;: 546, &#39;바디슬리밍/리프팅&#39;: 547, &#39;남아셔츠&#39;: 548, &#39;헤어무스/젤&#39;: 549, &#39;남아실내복&#39;: 550, &#39;풋케어&#39;: 551, &#39;여아재킷&#39;: 552, &#39;제기&#39;: 553, &#39;일반교육완구&#39;: 554, &#39;올인원&#39;: 555, &#39;유축기&#39;: 556, &#39;욕실화&#39;: 557, &#39;필기도구소모품&#39;: 558, &#39;남성머니클립&#39;: 559, &#39;헤어스프레이&#39;: 560, &#39;스팀청소기&#39;: 561, &#39;여성스포츠점퍼/재킷&#39;: 562, &#39;기타보석류&#39;: 563, &#39;자두&#39;: 564, &#39;메론&#39;: 565, &#39;멀티형에어컨&#39;: 566, &#39;기타모자&#39;: 567, &#39;유아동패드/스프레드&#39;: 568, &#39;카메라액세서리&#39;: 569, &#39;전기면도기&#39;: 570, &#39;유아동방한화&#39;: 571, &#39;남성서류가방&#39;: 572, &#39;책상정리용품&#39;: 573, &#39;비니&#39;: 574, &#39;남성슬립온&#39;: 575, &#39;디저트포크/스푼&#39;: 576, &#39;목욕용장난감&#39;: 577, &#39;우비&#39;: 578, &#39;주방수납장&#39;: 579, &#39;유아동수납장&#39;: 580, &#39;냉동떡볶이&#39;: 581, &#39;건강보조식품세트&#39;: 582, &#39;특수용세탁세제&#39;: 583, &#39;여성레깅스&#39;: 584, &#39;집게/클립&#39;: 585, &#39;캐노피&#39;: 586, &#39;핸드워시/손세정제&#39;: 587, &#39;유아용욕조&#39;: 588, &#39;모유보관용품&#39;: 589, &#39;바운서/쏘서/보행기&#39;: 590, &#39;발포비타민&#39;: 591, &#39;여성스포츠베스트&#39;: 592, &#39;드럼세탁기&#39;: 593, &#39;고양이건강용품&#39;: 594, &#39;스탠드형에어컨&#39;: 595, &#39;남성신발부속품&#39;: 596, &#39;유아동속옷세트&#39;: 597, &#39;자연유래영양제&#39;: 598, &#39;조리기구세트&#39;: 599, &#39;남성골프스웨트셔츠/후드/집업&#39;: 600, &#39;여성방한화&#39;: 601, &#39;LED&#39;: 602, &#39;UHD&#39;: 603, &#39;냉동튀김&#39;: 604, &#39;캐쥬얼백팩&#39;: 605, &#39;욕실수납용품&#39;: 606, &#39;연필깎이&#39;: 607, &#39;연필&#39;: 608, &#39;탐폰&#39;: 609, &#39;운동보조식품&#39;: 610, &#39;남성백팩&#39;: 611, &#39;유아동로퍼&#39;: 612, &#39;헤어왁스&#39;: 613, &#39;배냇저고리&#39;: 614, &#39;영유아베스트&#39;: 615, &#39;고양이장난감&#39;: 616, &#39;고양이목욕/위생용품&#39;: 617, &#39;여성스포츠스웨트셔츠/후드/집업&#39;: 618, &#39;국그릇&#39;: 619, &#39;남성힙색&#39;: 620, &#39;남성스포츠화부속품&#39;: 621, &#39;여아블라우스&#39;: 622, &#39;기타주방가전&#39;: 623, &#39;남성스포츠스웨트셔츠/후드/집업&#39;: 624, &#39;서류정리용품&#39;: 625, &#39;숙취해소음료&#39;: 626, &#39;배&#39;: 627, &#39;영화/문화모바일상품권&#39;: 628, &#39;남성스포츠속옷&#39;: 629, &#39;보온도시락&#39;: 630, &#39;남아스웨트셔츠/후드/집업&#39;: 631, &#39;홍삼절편&#39;: 632, &#39;침실가구세트&#39;: 633, &#39;일반네일/케어류&#39;: 634, &#39;남성슬리퍼&#39;: 635, &#39;시계세트&#39;: 636, &#39;한우선물세트&#39;: 637, &#39;남성클러치백&#39;: 638, &#39;붕붕카/스프링카/흔들말&#39;: 639, &#39;이발기&#39;: 640, &#39;삼계탕용닭&#39;: 641, &#39;남아레깅스&#39;: 642, &#39;블록&#39;: 643, &#39;기타청소기&#39;: 644, &#39;남성양말선물세트&#39;: 645, &#39;남성캐쥬얼스포츠양말&#39;: 646, &#39;목욕타올&#39;: 647, &#39;여성골프스웨트셔츠/후드/집업&#39;: 648, &#39;호일/랩/기름종이&#39;: 649, &#39;파일/바인더&#39;: 650, &#39;기타피트니스기구&#39;: 651, &#39;영양제세트&#39;: 652, &#39;튜브/보트&#39;: 653, &#39;에어로빅복&#39;: 654, &#39;여성사파리&#39;: 655, &#39;러닝/워킹머신&#39;: 656, &#39;롤스크린&#39;: 657, &#39;애견이동장&#39;: 658, &#39;액상표백제&#39;: 659, &#39;야외용돗자리&#39;: 660, &#39;침대&#39;: 661, &#39;여성등산전신/원피스&#39;: 662, &#39;메탈미용소도구&#39;: 663, &#39;모빌&#39;: 664, &#39;주전자&#39;: 665, &#39;주류잔&#39;: 666, &#39;오븐팬/피자팬&#39;: 667, &#39;홍삼근&#39;: 668, &#39;남성사파리&#39;: 669, &#39;여성골프점퍼/재킷&#39;: 670, &#39;형광펜&#39;: 671, &#39;독서대&#39;: 672, &#39;보석세트&#39;: 673, &#39;열쇠고리&#39;: 674, &#39;기타여성의류아우터&#39;: 675, &#39;영유아코트&#39;: 676, &#39;군모&#39;: 677, &#39;헬스바이크&#39;: 678, &#39;남성등산/아웃도어세트&#39;: 679, &#39;남성등산전신&#39;: 680, &#39;유아동부츠&#39;: 681, &#39;남성컴포트화&#39;: 682, &#39;영유아스웨터/풀오버&#39;: 683, &#39;스탠드형김치냉장고&#39;: 684, &#39;부분세탁제&#39;: 685, &#39;닭윗날개(봉)&#39;: 686, &#39;샤워커튼&#39;: 687, &#39;골프채&#39;: 688, &#39;미러리스&#39;: 689, &#39;기타유아안전용품&#39;: 690, &#39;영유아재킷&#39;: 691, &#39;등산지팡이/스틱&#39;: 692, &#39;땅콩&#39;: 693, &#39;냉동밥&#39;: 694, &#39;스포츠아대/헤어밴드&#39;: 695, &#39;순금/순은/장식품&#39;: 696, &#39;가발/부분가발&#39;: 697, &#39;여성등산/아웃도어세트&#39;: 698, &#39;전기그릴&#39;: 699, &#39;그릴/구이불판&#39;: 700, &#39;컵/행주살균기&#39;: 701, &#39;뚝배기&#39;: 702, &#39;기타유아동양말류&#39;: 703, &#39;유아변기/배변훈련기&#39;: 704, &#39;여성골프전신/원피스&#39;: 705, &#39;무릎담요&#39;: 706, &#39;태블릿PC&#39;: 707, &#39;캐슈넛&#39;: 708, &#39;남성스포츠패딩&#39;: 709, &#39;싱크대/배수구용품&#39;: 710, &#39;여성스포츠속옷&#39;: 711, &#39;교자상/다용도상&#39;: 712, &#39;캐쥬얼힙색&#39;: 713, &#39;매직/보드마카&#39;: 714, &#39;영유아스웨트셔츠/후드/집업&#39;: 715, &#39;남성골프베스트&#39;: 716, &#39;여성스포츠스커트&#39;: 717, &#39;자/제도용품&#39;: 718, &#39;문구세트&#39;: 719, &#39;남성수면양말&#39;: 720, &#39;남아스웨터/풀오버&#39;: 721, &#39;일반세탁기&#39;: 722, &#39;캐쥬얼숄더백&#39;: 723, &#39;이불/옷커버류&#39;: 724, &#39;전기냄비/뚝배기&#39;: 725, &#39;피스타치오&#39;: 726, &#39;돼지고기선물세트&#39;: 727, &#39;전자계산기&#39;: 728, &#39;힙색/사이드백&#39;: 729, &#39;머플러&#39;: 730, &#39;넥워머&#39;: 731, &#39;젓갈&#39;: 732, &#39;세탁비누&#39;: 733, &#39;목욕가운&#39;: 734, &#39;남성발가락양말&#39;: 735, &#39;공유기&#39;: 736, &#39;협탁&#39;: 737, &#39;성인침대커버/스커트&#39;: 738, &#39;명함정리용품&#39;: 739, &#39;절임반찬&#39;: 740, &#39;과실주병&#39;: 741, &#39;립밤/립스크럽&#39;: 742, &#39;제모용품&#39;: 743, &#39;제모기&#39;: 744, &#39;계량도구&#39;: 745, &#39;보드류&#39;: 746, &#39;프린터/복합기/스캐너&#39;: 747, &#39;양념통&#39;: 748, &#39;방울토마토&#39;: 749, &#39;밤&#39;: 750, &#39;스포츠목걸이/팔찌&#39;: 751, &#39;고양이식기/급수&#39;: 752, &#39;그늘막/타프&#39;: 753, &#39;기타모바일기기&#39;: 754, &#39;유아동옷장&#39;: 755, &#39;남아가디건&#39;: 756, &#39;테이블데코&#39;: 757, &#39;여성스포츠전신/원피스&#39;: 758, &#39;냅킨&#39;: 759, &#39;바란스&#39;: 760, &#39;뚜껑형김치냉장고&#39;: 761, &#39;남성골프니트/가디건&#39;: 762, &#39;여아코트&#39;: 763, &#39;유아동매트리스커버&#39;: 764, &#39;여성스포츠패딩&#39;: 765, &#39;다기류&#39;: 766, &#39;컴팩트&#39;: 767, &#39;2단우산&#39;: 768, &#39;기타냉장고&#39;: 769, &#39;물병&#39;: 770, &#39;데스크탑/올인원PC&#39;: 771, &#39;기타카메라&#39;: 772, &#39;마카다미아&#39;: 773, &#39;냉동부침&#39;: 774, &#39;브로치&#39;: 775, &#39;남아베스트&#39;: 776, &#39;여행용세트&#39;: 777, &#39;귤류&#39;: 778, &#39;하이앤드&#39;: 779, &#39;항아리/쌀독류&#39;: 780, &#39;헤어롤&#39;: 781, &#39;여아스웨터/풀오버&#39;: 782, &#39;냉동고&#39;: 783, &#39;하이브리드&#39;: 784, &#39;기타여성양말류&#39;: 785, &#39;패션액세서리세트&#39;: 786, &#39;기타영유아아우터&#39;: 787, &#39;잣&#39;: 788, &#39;스포츠음료&#39;: 789, &#39;스테이플러&#39;: 790, &#39;영유아패딩&#39;: 791, &#39;기타배낭소품&#39;: 792, &#39;시공가구&#39;: 793, &#39;DIY가구&#39;: 794, &#39;살구&#39;: 795, &#39;여성수면양말&#39;: 796, &#39;유아동침구매트&#39;: 797, &#39;기타남성양말류&#39;: 798, &#39;정수기&#39;: 799, &#39;여아실내복&#39;: 800, &#39;모니터&#39;: 801, &#39;유아동침구속통/솜&#39;: 802, &#39;전동보드/전동킥보드&#39;: 803, &#39;공간박스&#39;: 804, &#39;하이패스&#39;: 805, &#39;신발장&#39;: 806, &#39;DSLR&#39;: 807, &#39;남성실내복&#39;: 808, &#39;기타남성화&#39;: 809, &#39;네비게이션&#39;: 810, &#39;전기프라이팬&#39;: 811, &#39;여성실내복&#39;: 812, &#39;오프너/와인스크류&#39;: 813, &#39;유아동시계&#39;: 814, &#39;환풍기&#39;: 815, &#39;볶음반찬&#39;: 816, &#39;파티션&#39;: 817, &#39;냉온풍기&#39;: 818, &#39;펀치류&#39;: 819, &#39;냉동피자&#39;: 820, &#39;기차/레일완구&#39;: 821, &#39;인라인/롤러스케이트&#39;: 822, &#39;매실&#39;: 823, &#39;주방용탈수기&#39;: 824, &#39;유아동침대커버/스커트&#39;: 825, &#39;기타자동차가전기기&#39;: 826, &#39;여성캐쥬얼스포츠양말&#39;: 827, &#39;네일세트&#39;: 828, &#39;남성스포츠베스트&#39;: 829, &#39;채칼/강판/절구&#39;: 830, &#39;캐쥬얼시계&#39;: 831, &#39;니삭스/오버니삭스&#39;: 832, &#39;싸인물/자석/압핀&#39;: 833, &#39;고양이의류/악세서리&#39;: 834, &#39;비타민/에너지음료&#39;: 835, &#39;에어워셔&#39;: 836, &#39;냉장/냉동가전소모품&#39;: 837, &#39;남성부츠&#39;: 838, &#39;물걸레청소기&#39;: 839, &#39;음식물건조기&#39;: 840, &#39;요구르트/청국장제조기&#39;: 841, &#39;골프채세트&#39;: 842, &#39;고양이이동장&#39;: 843, &#39;냉동면&#39;: 844, &#39;소프트웨어&#39;: 845, &#39;캠코더&#39;: 846, &#39;무화과&#39;: 847, &#39;OLED&#39;: 848, &#39;탈수기&#39;: 849, &#39;육가공품선물세트&#39;: 850, &#39;딸기&#39;: 851, &#39;식기세척기&#39;: 852, &#39;차량용충전기&#39;: 853, &#39;유아두유&#39;: 854, &#39;닭근위&#39;: 855, &#39;닭아랫날개(윙)&#39;: 856, &#39;안경소품&#39;: 857, &#39;여성가방액세서리&#39;: 858, &#39;태닝/애프터선케어&#39;: 859, &#39;유아동스포츠스커트&#39;: 860, &#39;카메라렌즈&#39;: 861, &#39;닭안심&#39;: 862, &#39;포도&#39;: 863, &#39;볶음탕용닭&#39;: 864, &#39;콜렉션인형&#39;: 865, &#39;LCD&#39;: 866, &#39;리모컨/액세서리&#39;: 867, &#39;반죽기/제면기&#39;: 868, &#39;식기건조기&#39;: 869, &#39;단무지&#39;: 870, &#39;닭다리&#39;: 871, &#39;감&#39;: 872, &#39;오리고기&#39;: 873, &#39;침구청소기&#39;: 874, &#39;싱크대&#39;: 875, &#39;미용거울&#39;: 876, &#39;남녀공용향수세트&#39;: 877, &#39;인라인/스케이트보드/킥보드기타액세서리&#39;: 878, &#39;남성등산스웨트셔츠/후드/집업&#39;: 879, &#39;커튼류세트&#39;: 880, &#39;오토캠핑용품세트&#39;: 881, &#39;수도용품&#39;: 882} . data[&#39;clac3_nm&#39;] = data[&#39;clac3_nm&#39;].map(lambda x: [clac3_nm_dic[nm] for nm in x]) . clac3_matrix = np.zeros((len(data), len(clac3_nm_dic))) for i in range(len(data)): for j in data[&#39;clac3_nm&#39;][i]: clac3_matrix[i, j] += 1 . clac3_matrix . array([[1., 1., 0., ..., 0., 0., 0.], [0., 0., 1., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) . cols = [&#39;clac3_nm_&#39; + str(i) for i in range(len(clac3_nm_dic))] . clac3_df = pd.DataFrame(clac3_matrix) clac3_df.columns = cols . clac3_df . clac3_nm_0 clac3_nm_1 clac3_nm_2 clac3_nm_3 clac3_nm_4 clac3_nm_5 clac3_nm_6 clac3_nm_7 clac3_nm_8 clac3_nm_9 ... clac3_nm_873 clac3_nm_874 clac3_nm_875 clac3_nm_876 clac3_nm_877 clac3_nm_878 clac3_nm_879 clac3_nm_880 clac3_nm_881 clac3_nm_882 . 0 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 0.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 149995 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149996 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149997 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149998 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 149999 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 150000 rows × 883 columns . data_concat = pd.concat([data.iloc[:, :-6], clac1_df, clac2_df, clac3_df, data.iloc[:, -1]], axis=1) . data_concat . clnt_id num_shopping avg_price total_price avg_ct total_ct avg_sess_view total_sess_view avg_sess_hr total_sess_hr ... clac3_nm_874 clac3_nm_875 clac3_nm_876 clac3_nm_877 clac3_nm_878 clac3_nm_879 clac3_nm_880 clac3_nm_881 clac3_nm_882 label . 0 0 | 2 | 43250 | 86500 | 1 | 2 | 59 | 118 | 922 | 1844 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F20 | . 1 1 | 9 | 77777.8 | 700000 | 1 | 9 | 132.333 | 1191 | 1311.11 | 11800 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | . 2 6 | 4 | 24225 | 96900 | 1 | 4 | 21.75 | 87 | 297.25 | 1189 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F20 | . 3 9 | 2 | 10550 | 21100 | 1 | 2 | 249 | 498 | 5049 | 10098 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | . 4 12 | 16 | 14325.6 | 229210 | 1 | 16 | 144.938 | 2319 | 4187.25 | 66996 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 149995 263094 | 1 | 10000 | 10000 | 1 | 1 | 66 | 66 | 513 | 513 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | . 149996 263095 | 2 | 122000 | 244000 | 1 | 2 | 220.5 | 441 | 1828 | 3656 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | . 149997 263096 | 3 | 28500 | 85500 | 1 | 3 | 256 | 768 | 4237 | 12711 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | . 149998 263102 | 1 | 1080 | 1080 | 1 | 1 | 188 | 188 | 1812 | 1812 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | . 149999 263103 | 7 | 58628.6 | 410400 | 1 | 7 | 280 | 1960 | 3249.43 | 22746 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | . 150000 rows × 1060 columns . data_concat.to_csv(&#39;./train_preprocessed.csv&#39;, index=False) data_concat = pd.read_csv(&#39;./train_preprocessed.csv&#39;) . lookup_table_clac1 = [] for i in range(37): # clac1_nm_number 이 1 이상인 것들(즉 검색 횟수가 1번 이상인 것들) 라벨 별(6차원) value count =&gt; 라벨별 분류 별 개수 파악 prob = data_concat[data_concat[&#39;clac1_nm_&#39; + str(i)] &gt;= 1].groupby(&#39;label&#39;)[&#39;label&#39;].value_counts().values if len(prob) != 6: print(i, prob) lookup_table_clac1.append(prob) . lookup_table_clac1 = np.stack(lookup_table_clac1) . 가중치 스케일링 진행 | . weight = np.array([59892/17727, 59892/59892, 59892/51936, 59892/2588, 59892/7953, 59892/9904]) weight . array([ 3.37857506, 1. , 1.15318854, 23.14219474, 7.53074312, 6.04725363]) . lookup_table_clac1 = lookup_table_clac1 * weight . 가중치를 곱해준 후 열별 합으로 나누어 확률을 계산 | . lookup_table_clac1 = lookup_table_clac1 / lookup_table_clac1.sum(1).reshape(-1, 1) . lookup_table_clac1 . array([[0.1196019 , 0.15351727, 0.20977359, 0.12166872, 0.17288697, 0.22255155], [0.10090549, 0.22714818, 0.27848079, 0.07186492, 0.14375322, 0.1778474 ], [0.10631361, 0.2387599 , 0.22268107, 0.0900885 , 0.15848887, 0.18366804], [0.29039753, 0.19554586, 0.18492875, 0.11494716, 0.10232984, 0.11185087], [0.12042409, 0.17029651, 0.21638294, 0.08393391, 0.18585456, 0.223108 ], [0.13488386, 0.23263313, 0.22147735, 0.09935707, 0.14373666, 0.16791193], [0.11902816, 0.13211609, 0.16521248, 0.21401495, 0.17611567, 0.19351265], [0.16240187, 0.19808438, 0.20762432, 0.1376104 , 0.14092544, 0.1533536 ], [0.07940145, 0.22461032, 0.27383925, 0.05948637, 0.1576259 , 0.2050367 ], [0.03879911, 0.36162852, 0.26285742, 0.01136482, 0.1382575 , 0.18709264], [0.08528045, 0.2160113 , 0.33747026, 0.03723632, 0.11662744, 0.20737422], [0.09358846, 0.18638675, 0.24438927, 0.09532372, 0.16983156, 0.21048024], [0.09206891, 0.20384676, 0.25386732, 0.06491919, 0.1679978 , 0.21730001], [0.10342799, 0.33434196, 0.14533046, 0.04690085, 0.23776732, 0.13223142], [0.08571612, 0.13047683, 0.22820432, 0.03355022, 0.18014104, 0.34191147], [0.17141221, 0.1971363 , 0.22892891, 0.09959467, 0.12696112, 0.17596679], [0.06222505, 0.08604227, 0.23216874, 0.12038342, 0.13869779, 0.36048272], [0.22324304, 0.15132509, 0.21897612, 0.11280584, 0.12399265, 0.16965727], [0.09232229, 0.15829274, 0.22398933, 0.08735568, 0.20083988, 0.23720008], [0.15518083, 0.23827301, 0.35983805, 0.03133611, 0.06798093, 0.14739108], [0.07570349, 0.11247887, 0.15227928, 0.22193144, 0.22689 , 0.21071693], [0.08260026, 0.11442791, 0.23484849, 0.12832265, 0.18221544, 0.25758525], [0.06496974, 0.11974998, 0.31650811, 0.05057075, 0.11519424, 0.33300719], [0.05506505, 0.13947588, 0.20192598, 0.08704121, 0.21007141, 0.30642047], [0.1021726 , 0.16848741, 0.26515277, 0.05554371, 0.17351595, 0.23512756], [0.1075171 , 0.16963273, 0.19924108, 0.13591302, 0.18616776, 0.20152831], [0.05438879, 0.28590838, 0.15388338, 0.03694675, 0.26049635, 0.20837634], [0.17495297, 0.19306772, 0.23370193, 0.10849474, 0.12998832, 0.15979432], [0.15856251, 0.12967989, 0.15168173, 0.17149013, 0.19066678, 0.19791896], [0.08792574, 0.08498125, 0.12449074, 0.2230608 , 0.22501851, 0.25452295], [0.14436981, 0.09515721, 0.10600739, 0.26868961, 0.19830566, 0.18747033], [0.07729805, 0.18127121, 0.22229937, 0.11675443, 0.1661101 , 0.23626684], [0.08819849, 0.1800361 , 0.29377603, 0.08332861, 0.14235959, 0.21230117], [0.15508448, 0.12116081, 0.10063624, 0.3026265 , 0.17685887, 0.1436331 ], [0.07460569, 0.13838052, 0.27962674, 0.08760444, 0.14253759, 0.27724502], [0.05972663, 0.06089107, 0.07248401, 0.18182619, 0.3993866 , 0.22568551], [0.10097066, 0.11264563, 0.11576269, 0.10640274, 0.30008079, 0.26413749]]) . 룩업 테이블 중 6차원이 되지 못하는 것들 결측치 보완해 6차원 만들어주기 | . lookup_table_clac2 = [] for i in range(128): prob = data_concat[data_concat[&#39;clac2_nm_&#39; + str(i)] &gt;= 1].groupby(&#39;label&#39;)[&#39;label&#39;].value_counts().values if len(prob) != 6: print(i, prob) lookup_table_clac2.append(prob) . 42 [133 607 646 27 64] 52 [ 42 1780 1083 87 134] 86 [ 18 1216 1003 58 147] 111 [ 7 605 370 34 39] 120 [ 21 90 124 21 35] 122 [ 20 759 471 47 63] 127 [ 1 9 10 4 4] . data_concat[data_concat[&#39;clac2_nm_&#39; + str(86)] &gt;= 1].groupby(&#39;label&#39;)[&#39;label&#39;].value_counts() . label label F20 F20 18 F30 F30 1216 F40 F40 1003 M30 M30 58 M40 M40 147 Name: label, dtype: int64 . lookup_table_clac2[13] = np.array([56, 1238, 504, 0, 58, 71]) lookup_table_clac2[42] = np.array([109, 519, 550, 0, 21, 52]) lookup_table_clac2[52] = np.array([23, 1036, 685, 0, 52, 75]) lookup_table_clac2[86] = np.array([9, 724, 644, 0, 39, 90]) lookup_table_clac2[111] = np.array([5, 420, 261, 0, 28, 29]) lookup_table_clac2[120] = np.array([18, 88, 121, 0, 20, 35]) lookup_table_clac2[122] = np.array([18, 535, 349, 0, 37, 44]) lookup_table_clac2[127] = np.array([1, 9, 10, 0, 4, 4]) . lookup_table_clac2 = np.stack(lookup_table_clac2) . lookup_table_clac2 . array([[ 167, 791, 1075, 19, 100, 193], [ 71, 454, 706, 7, 45, 86], [ 776, 5888, 4762, 96, 519, 749], [2576, 8291, 7464, 175, 595, 777], [ 347, 1245, 1276, 18, 86, 152], [ 324, 1687, 1673, 36, 257, 343], [5173, 9270, 6888, 160, 339, 613], [1158, 4240, 4284, 28, 160, 401], [ 741, 2125, 2624, 134, 373, 439], [ 75, 3239, 1694, 3, 149, 233], [ 59, 1537, 578, 2, 64, 92], [1046, 4946, 5185, 47, 276, 482], [ 96, 887, 1001, 10, 78, 143], [ 56, 1238, 504, 0, 58, 71], [ 181, 1942, 1187, 25, 169, 182], [ 72, 697, 1094, 4, 55, 134], [ 89, 535, 702, 15, 80, 124], [ 103, 800, 845, 8, 78, 125], [ 131, 1686, 846, 4, 131, 122], [ 18, 97, 162, 2, 16, 47], [ 158, 654, 554, 11, 54, 69], [ 755, 2113, 2934, 400, 772, 842], [ 131, 612, 1432, 37, 131, 424], [ 201, 251, 301, 49, 74, 67], [ 916, 2915, 3431, 280, 636, 756], [ 368, 931, 1218, 27, 112, 194], [ 176, 1073, 1191, 32, 210, 298], [ 911, 2243, 2646, 68, 156, 355], [ 110, 482, 658, 8, 53, 123], [ 175, 708, 719, 72, 210, 221], [ 168, 1059, 1204, 15, 78, 149], [ 366, 1225, 1554, 78, 255, 298], [ 243, 1047, 1350, 120, 344, 378], [1158, 6383, 9230, 33, 236, 723], [ 849, 4607, 5286, 22, 140, 366], [ 74, 865, 282, 7, 65, 37], [1409, 6914, 7984, 580, 1790, 1975], [ 441, 2995, 3678, 284, 873, 1065], [ 321, 1590, 2651, 5, 46, 180], [ 97, 454, 808, 22, 96, 169], [ 111, 3567, 1951, 5, 172, 266], [ 55, 2762, 1891, 2, 131, 256], [ 109, 519, 550, 0, 21, 52], [ 178, 2684, 1138, 9, 255, 218], [ 364, 1916, 2233, 38, 280, 433], [ 44, 274, 628, 5, 35, 126], [ 292, 1630, 1980, 112, 459, 821], [ 142, 3073, 1538, 13, 230, 242], [ 411, 1656, 2061, 48, 143, 316], [ 35, 400, 377, 11, 97, 113], [ 114, 779, 1238, 24, 151, 362], [ 694, 1750, 2069, 50, 97, 181], [ 23, 1036, 685, 0, 52, 75], [ 272, 558, 652, 19, 51, 73], [ 113, 860, 953, 25, 104, 189], [ 88, 533, 1047, 41, 170, 330], [ 63, 351, 479, 5, 48, 81], [ 65, 268, 345, 11, 31, 52], [ 41, 211, 255, 12, 36, 58], [ 242, 2458, 559, 9, 197, 84], [ 94, 743, 162, 11, 93, 44], [ 159, 4015, 2016, 7, 249, 309], [ 735, 3917, 5331, 19, 146, 415], [ 41, 891, 421, 4, 88, 109], [ 243, 906, 951, 22, 81, 124], [ 144, 956, 1367, 8, 50, 133], [ 47, 352, 766, 19, 118, 336], [ 40, 325, 1111, 9, 14, 102], [ 38, 177, 247, 12, 38, 45], [ 59, 166, 144, 10, 27, 25], [ 54, 1965, 1470, 2, 103, 201], [ 85, 963, 1099, 8, 63, 129], [ 16, 114, 171, 2, 10, 14], [ 60, 698, 801, 4, 59, 106], [ 163, 325, 412, 31, 35, 58], [ 615, 2143, 1953, 38, 170, 242], [ 141, 411, 560, 9, 50, 89], [ 46, 499, 776, 10, 71, 143], [ 65, 746, 664, 13, 70, 111], [ 15, 35, 62, 6, 24, 21], [ 75, 262, 376, 10, 46, 70], [ 347, 754, 724, 92, 199, 238], [ 201, 1383, 1930, 21, 109, 228], [ 160, 1410, 1481, 38, 167, 294], [ 43, 244, 282, 6, 28, 65], [ 66, 363, 419, 5, 44, 66], [ 9, 724, 644, 0, 39, 90], [ 32, 2532, 1868, 1, 123, 247], [ 29, 200, 283, 4, 21, 39], [ 310, 515, 605, 38, 43, 71], [ 109, 829, 951, 15, 96, 168], [ 19, 793, 461, 3, 104, 129], [ 195, 1401, 1539, 28, 154, 234], [ 128, 843, 1487, 17, 79, 201], [ 289, 1027, 1218, 40, 105, 164], [ 153, 496, 618, 56, 166, 244], [ 28, 1128, 780, 5, 77, 102], [ 55, 333, 370, 8, 55, 73], [ 244, 1383, 1493, 19, 122, 221], [ 505, 1467, 1173, 8, 79, 147], [ 69, 734, 269, 6, 89, 53], [ 48, 580, 152, 10, 74, 29], [ 172, 454, 327, 49, 88, 89], [ 103, 558, 765, 69, 232, 300], [ 142, 430, 516, 26, 70, 108], [ 549, 1127, 1011, 71, 136, 160], [ 112, 1159, 1165, 11, 107, 163], [ 501, 1016, 1116, 50, 78, 132], [ 178, 303, 447, 43, 84, 95], [ 39, 249, 331, 19, 71, 77], [ 59, 281, 228, 11, 50, 68], [ 5, 420, 261, 0, 28, 29], [ 125, 429, 592, 14, 50, 85], [ 25, 370, 82, 1, 40, 31], [ 9, 206, 115, 1, 27, 25], [ 133, 455, 568, 6, 59, 68], [ 79, 454, 739, 11, 50, 131], [ 9, 31, 32, 4, 27, 19], [ 22, 1116, 821, 1, 65, 122], [ 27, 213, 463, 8, 41, 96], [ 18, 88, 121, 0, 20, 35], [ 11, 45, 49, 6, 23, 23], [ 18, 535, 349, 0, 37, 44], [ 11, 69, 94, 25, 83, 99], [ 30, 107, 71, 5, 37, 34], [ 9, 40, 60, 2, 15, 23], [ 17, 44, 70, 2, 14, 28], [ 1, 9, 10, 0, 4, 4]], dtype=int64) . lookup_table_clac2 = lookup_table_clac2 * weight . lookup_table_clac2 = lookup_table_clac2 / lookup_table_clac2.sum(1).reshape(-1, 1) . lookup_table_clac2 . array([[0.11387393, 0.15964331, 0.25019754, 0.08874265, 0.15198897, 0.2355536 ], [0.09485229, 0.17951954, 0.32192959, 0.0640558 , 0.13400044, 0.20564233], [0.10631361, 0.2387599 , 0.22268107, 0.0900885 , 0.15848887, 0.18366804], [0.22413044, 0.21351497, 0.22166308, 0.10429512, 0.11539214, 0.12100425], [0.19964605, 0.21201521, 0.25058131, 0.07093731, 0.11028945, 0.15653067], [0.11457987, 0.17658134, 0.20194169, 0.08720407, 0.20258192, 0.2171111 ], [0.39140263, 0.20760004, 0.17788575, 0.08292247, 0.05717224, 0.08301687], [0.22523187, 0.24409201, 0.28440517, 0.03730356, 0.06936582, 0.13960156], [0.15435505, 0.1310171 , 0.1865663 , 0.19119582, 0.17318717, 0.16367855], [0.03149144, 0.40253967, 0.24277919, 0.00862827, 0.13945107, 0.17511036], [0.05715761, 0.4407196 , 0.19112463, 0.01327159, 0.13819945, 0.15952712], [0.1720522 , 0.2407959 , 0.29110124, 0.05295383, 0.101191 , 0.14190582], [0.08009934, 0.21905226, 0.28507459, 0.05715163, 0.14506296, 0.21355922], [0.06581918, 0.4306768 , 0.20219094, 0. , 0.15194859, 0.14936449], [0.08895892, 0.28250529, 0.1991262 , 0.08416314, 0.1851407 , 0.16010575], [0.06912809, 0.19807116, 0.35851399, 0.02630589, 0.1177034 , 0.23027747], [0.08990184, 0.15995535, 0.24203737, 0.10378648, 0.18012451, 0.22419445], [0.09531764, 0.21912527, 0.26690671, 0.0507104 , 0.16089217, 0.20704782], [0.08993877, 0.34260968, 0.19824979, 0.01881077, 0.20047083, 0.14992017], [0.07643565, 0.12191626, 0.23480385, 0.0581734 , 0.15144248, 0.35722836], [0.18374676, 0.22511622, 0.21990703, 0.08762464, 0.13997827, 0.14362707], [0.09042372, 0.07490337, 0.11993952, 0.32814546, 0.20609004, 0.18049789], [0.06222505, 0.08604227, 0.23216874, 0.12038342, 0.13869779, 0.36048272], [0.20129571, 0.07440097, 0.10288965, 0.33612863, 0.16518646, 0.12009858], [0.11991786, 0.11295186, 0.15331189, 0.25108305, 0.18558794, 0.1771474 ], [0.19987876, 0.14967006, 0.22580464, 0.10045084, 0.1355942 , 0.18860151], [0.08298891, 0.14975232, 0.19168402, 0.10335425, 0.22071455, 0.25150596], [0.23198729, 0.16906026, 0.22998653, 0.1186112 , 0.08854717, 0.16180755], [0.1263869 , 0.16391656, 0.2580489 , 0.06296081, 0.13573427, 0.25295257], [0.08808163, 0.10547438, 0.12352161, 0.24822801, 0.2355976 , 0.19909676], [0.1170163 , 0.218323 , 0.28624001, 0.07156478, 0.12109772, 0.18575819], [0.12642292, 0.12524121, 0.18321562, 0.18454841, 0.19633113, 0.18424071], [0.07410828, 0.09450909, 0.1405274 , 0.25067597, 0.23384236, 0.2063369 ], [0.14046852, 0.22917209, 0.38215442, 0.02741924, 0.0638097 , 0.15697603], [0.16534633, 0.26556541, 0.35138302, 0.02934814, 0.06077419, 0.12758293], [0.10797639, 0.37357656, 0.14044715, 0.06996262, 0.21140473, 0.09663255], [0.07970245, 0.11575946, 0.15415158, 0.22472928, 0.22569295, 0.19996427], [0.0526235 , 0.1057802 , 0.14980269, 0.23212956, 0.23219863, 0.22746541], [0.14892673, 0.21833893, 0.4198016 , 0.01588944, 0.04756962, 0.14947368], [0.08260026, 0.11442791, 0.23484849, 0.12832265, 0.18221544, 0.25758525], [0.04071252, 0.38723499, 0.24424691, 0.01256163, 0.14061698, 0.17462697], [0.02410322, 0.35826348, 0.28285946, 0.00600362, 0.12796406, 0.20080615], [0.18467518, 0.26026503, 0.31806176, 0. , 0.07930592, 0.15769212], [0.0747562 , 0.33363849, 0.16313089, 0.02589051, 0.23871057, 0.16387334], [0.10856926, 0.16914822, 0.22733221, 0.07763545, 0.18615204, 0.23116283], [0.06496974, 0.11974998, 0.31650811, 0.05057075, 0.11519424, 0.33300719], [0.06199536, 0.10243075, 0.14348559, 0.16287909, 0.21721674, 0.31199247], [0.05437754, 0.34830542, 0.20102697, 0.03409931, 0.19631946, 0.16587129], [0.14586122, 0.17395013, 0.24965642, 0.11668371, 0.11311972, 0.20072881], [0.04510974, 0.15259091, 0.16584804, 0.09711044, 0.27866232, 0.26067856], [0.05949789, 0.1203374 , 0.22053835, 0.08579835, 0.17566204, 0.33816597], [0.24778347, 0.18493424, 0.25213904, 0.12227955, 0.07719494, 0.11566875], [0.02826967, 0.37689392, 0.28737585, 0. , 0.1424625 , 0.16499806], [0.26300913, 0.15969913, 0.21518711, 0.12584225, 0.10991991, 0.12634248], [0.07879122, 0.17748607, 0.22680835, 0.11940166, 0.16163559, 0.23587711], [0.0474765 , 0.08511178, 0.19280108, 0.15151333, 0.20443216, 0.31866516], [0.1021726 , 0.16848741, 0.26515277, 0.05554371, 0.17351595, 0.23512756], [0.13010442, 0.15877419, 0.23570268, 0.15081424, 0.13830715, 0.18629733], [0.08976614, 0.13673433, 0.19056169, 0.17996204, 0.17568531, 0.2272905 ], [0.13359609, 0.40163051, 0.10533118, 0.03403234, 0.24240908, 0.0830008 ], [0.12866043, 0.30100409, 0.0756831 , 0.103129 , 0.28372942, 0.10779396], [0.0498196 , 0.37235319, 0.21560577, 0.01502353, 0.17390285, 0.17329506], [0.14962327, 0.23601076, 0.37041386, 0.02649332, 0.06624741, 0.15121139], [0.04728605, 0.30415382, 0.1657288 , 0.03159949, 0.22622265, 0.22500919], [0.17495297, 0.19306772, 0.23370193, 0.10849474, 0.12998832, 0.15979432], [0.11095275, 0.21802178, 0.35950987, 0.04222178, 0.08587165, 0.18342217], [0.03339958, 0.07403758, 0.18579699, 0.09248424, 0.18690866, 0.42737296], [0.05058002, 0.12163786, 0.47951234, 0.07795293, 0.03945947, 0.23085738], [0.09001799, 0.12410389, 0.19971441, 0.19471433, 0.20064741, 0.19080196], [0.17840406, 0.14856867, 0.14862161, 0.20712079, 0.18197878, 0.13530609], [0.03102731, 0.33417918, 0.28829326, 0.00787139, 0.1319143 , 0.20671456], [0.07257117, 0.24335369, 0.32026513, 0.04678495, 0.11989195, 0.1971331 ], [0.09458732, 0.19947304, 0.34504503, 0.08098673, 0.13177019, 0.14813769], [0.06751951, 0.23248765, 0.30766444, 0.03083252, 0.14799066, 0.21350523], [0.20529292, 0.12115355, 0.17711295, 0.26743549, 0.09825591, 0.13074918], [0.20580528, 0.21226089, 0.22307473, 0.08710357, 0.12680447, 0.14495105], [0.17934696, 0.15473307, 0.24312503, 0.07841306, 0.14175852, 0.20262336], [0.04887015, 0.1569108 , 0.28139368, 0.07277075, 0.16813126, 0.27192336], [0.0679779 , 0.23091898, 0.2370223 , 0.09312552, 0.16317615, 0.20777915], [0.08393841, 0.05797009, 0.11842078, 0.22998087, 0.29935395, 0.2103359 ], [0.12993614, 0.13434961, 0.22234291, 0.11866965, 0.17763592, 0.21706577], [0.14976144, 0.09631819, 0.10665368, 0.27197522, 0.19143788, 0.18385359], [0.09738401, 0.19832625, 0.31916528, 0.06969183, 0.11771244, 0.1977202 ], [0.07137796, 0.18617857, 0.22551008, 0.11611778, 0.16605994, 0.23475567], [0.09969284, 0.16743713, 0.22315745, 0.09528351, 0.14469643, 0.26973264], [0.11642021, 0.18952107, 0.25226977, 0.06041231, 0.17299812, 0.20837854], [0.01302228, 0.31006259, 0.31805117, 0. , 0.12578048, 0.23308348], [0.01493836, 0.34985104, 0.29764368, 0.0031976 , 0.12798599, 0.20638333], [0.08819849, 0.1800361 , 0.29377603, 0.08332861, 0.14235959, 0.21230117], [0.26906271, 0.13230172, 0.17923134, 0.22591569, 0.08318874, 0.11029981], [0.08407927, 0.18927071, 0.25038581, 0.07925464, 0.16505852, 0.23195105], [0.02124516, 0.26244961, 0.17594381, 0.02297728, 0.25920532, 0.25817883], [0.09335259, 0.19851637, 0.25147634, 0.09181651, 0.16432996, 0.20050824], [0.0832595 , 0.16229975, 0.3301426 , 0.07574322, 0.11453948, 0.23401545], [0.15964406, 0.16791589, 0.22965133, 0.15135121, 0.1292851 , 0.1621524 ], [0.08994345, 0.08630307, 0.12400333, 0.2254951 , 0.21751563, 0.25673942], [0.02754419, 0.3284335 , 0.26189865, 0.03369092, 0.16883672, 0.17959602], [0.09355262, 0.16765014, 0.21481357, 0.09320822, 0.20852599, 0.22224946], [0.12445275, 0.20878692, 0.25992092, 0.06638031, 0.13870074, 0.20175837], [0.27541769, 0.23680834, 0.21835604, 0.02988556, 0.0960355 , 0.14349686], [0.09685464, 0.30495365, 0.12888144, 0.05768907, 0.27846179, 0.1331594 ], [0.08619164, 0.30826082, 0.09316102, 0.1229971 , 0.29618284, 0.09320657], [0.15508448, 0.12116081, 0.10063624, 0.3026265 , 0.17685887, 0.1436331 ], [0.05009762, 0.08033051, 0.12700127, 0.22987934, 0.25151977, 0.26117148], [0.14596695, 0.13082811, 0.18104337, 0.18306719, 0.16038675, 0.19870763], [0.23833294, 0.14481117, 0.14980614, 0.21112567, 0.1315997 , 0.12432438], [0.07680261, 0.23523819, 0.27267834, 0.051668 , 0.16354829, 0.20006457], [0.25888198, 0.15539042, 0.1968317 , 0.17697221, 0.0898386 , 0.12208509], [0.16603492, 0.08365434, 0.14231599, 0.27473808, 0.17464774, 0.15860893], [0.05982513, 0.11305372, 0.17330608, 0.1996382 , 0.24276254, 0.21141432], [0.11163667, 0.15737206, 0.1472504 , 0.14256684, 0.21087697, 0.23029706], [0.01502783, 0.37363016, 0.26775246, 0. , 0.18758085, 0.1560087 ], [0.15365239, 0.15608207, 0.24838065, 0.11787679, 0.13699464, 0.18701345], [0.07961858, 0.34877278, 0.08913639, 0.02181451, 0.28394791, 0.17670984], [0.04072331, 0.27588887, 0.17760906, 0.03099356, 0.27231311, 0.20247208], [0.17595768, 0.17816993, 0.25649072, 0.05437244, 0.17398542, 0.16102381], [0.08907588, 0.15151488, 0.28440957, 0.08495651, 0.12566296, 0.2643802 ], [0.05972663, 0.06089107, 0.07248401, 0.18182619, 0.3993866 , 0.22568551], [0.02194203, 0.32944631, 0.27948849, 0.00683164, 0.14450126, 0.21779027], [0.04769548, 0.11136776, 0.27916515, 0.09679979, 0.16143644, 0.30353537], [0.09347154, 0.13525583, 0.21446628, 0. , 0.23149475, 0.3253116 ], [0.06300986, 0.07629477, 0.09580289, 0.23541711, 0.2936621 , 0.23581328], [0.03941322, 0.34672855, 0.26083241, 0. , 0.18058238, 0.17244344], [0.01842693, 0.03421179, 0.05374708, 0.28686082, 0.30991501, 0.29683838], [0.11386041, 0.12019923, 0.09197644, 0.12998477, 0.31300946, 0.2309697 ], [0.06943374, 0.09133862, 0.15799597, 0.10568881, 0.25794288, 0.31759998], [0.11414176, 0.08744093, 0.16042071, 0.09198068, 0.20952118, 0.33649474], [0.04319189, 0.11505649, 0.14742425, 0. , 0.38509371, 0.30923367]]) . lookup_table_clac3 = np.zeros((883, 6)) # 888*6의 zero nd array를 생성 initialize for i in range(883): try: prob = data_concat[data_concat[&#39;clac3_nm_&#39; + str(i)] &gt;= 1].groupby(&#39;label&#39;)[&#39;label&#39;].value_counts() for j in range(len(prob.keys())): if prob.keys()[j][0] == &#39;F20&#39;: lookup_table_clac3[i, 0] = prob.values[j] elif prob.keys()[j][0] == &#39;F30&#39;: lookup_table_clac3[i, 1] = prob.values[j] elif prob.keys()[j][0] == &#39;F40&#39;: lookup_table_clac3[i, 2] = prob.values[j] elif prob.keys()[j][0] == &#39;M20&#39;: lookup_table_clac3[i, 3] = prob.values[j] elif prob.keys()[j][0] == &#39;M30&#39;: lookup_table_clac3[i, 4] = prob.values[j] elif prob.keys()[j][0] == &#39;M40&#39;: lookup_table_clac3[i, 5] = prob.values[j] except: lookup_table_clac3[i] = 1/weight . lookup_table_clac3 lookup_table_clac3 = lookup_table_clac3 * weight . lookup_table_clac3 = lookup_table_clac3 / lookup_table_clac3.sum(1).reshape(-1, 1) . lookup_table_clac3 . array([[0.13835664, 0.14548446, 0.28831758, 0.07481844, 0.13796521, 0.21505768], [0.07972002, 0.21727755, 0.33786185, 0. , 0.13327016, 0.23187043], [0.09684827, 0.1934031 , 0.2083384 , 0.1637977 , 0.13858443, 0.1990281 ], ..., [0. , 0.30244584, 0.69755416, 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 1. , 0. ], [0. , 0. , 1. , 0. , 0. , 0. ]]) . mat_clac1 = np.array(data_concat.iloc[:, 11:48]).dot(lookup_table_clac1) mat_clac1 = mat_clac1 / mat_clac1.sum(1).reshape(-1, 1) . mat_clac1 = pd.DataFrame(mat_clac1) mat_clac1 = mat_clac1.rename(columns={0: &#39;given_clac1_F20_prob&#39;, 1:&#39;given_clac1_F30_prob&#39;, 2:&#39;given_clac1_F40_prob&#39;, 3:&#39;given_clac1_M20_prob&#39;, 4:&#39;given_clac1_M30_prob&#39;, 5:&#39;given_clac1_M40_prob&#39;}) . 본격 룩업 테이블에서 벡터 뽑아내는 코드 | . mat_buy_clac1 = np.where(np.array(data_concat.iloc[:, 11:48]) &gt;= 1, 1, 0).dot(lookup_table_clac1) # np.where(조건, True return value, False return value) # 원 핫 매트릭스와 확률이 담긴 룩업 테이블을 내적 mat_buy_clac1 = mat_buy_clac1 / mat_buy_clac1.sum(1).reshape(-1, 1) # 열 별로 합치고 reshape 한 것과 나눈다 . mat_buy_clac1 = pd.DataFrame(mat_buy_clac1) mat_buy_clac1 = mat_buy_clac1.rename(columns={0: &#39;buy_clac1_F20_prob&#39;, 1:&#39;buy_clac1_F30_prob&#39;, 2:&#39;buy_clac1_F40_prob&#39;, 3:&#39;buy_clac1_M20_prob&#39;, 4:&#39;buy_clac1_M30_prob&#39;, 5:&#39;buy_clac1_M40_prob&#39;}) . mat_clac2 = np.array(data_concat.iloc[:, 48:-884]).dot(lookup_table_clac2) mat_clac2 = mat_clac2 / mat_clac2.sum(1).reshape(-1, 1) . mat_clac2 = pd.DataFrame(mat_clac2) mat_clac2 = mat_clac2.rename(columns={0: &#39;given_clac2_F20_prob&#39;, 1:&#39;given_clac2_F30_prob&#39;, 2:&#39;given_clac2_F40_prob&#39;, 3:&#39;given_clac2_M20_prob&#39;, 4:&#39;given_clac2_M30_prob&#39;, 5:&#39;given_clac2_M40_prob&#39;}) . mat_buy_clac2 = np.where(np.array(data_concat.iloc[:, 48:-884]) &gt;= 1, 1, 0).dot(lookup_table_clac2) mat_buy_clac2 = mat_buy_clac2 / mat_buy_clac2.sum(1).reshape(-1, 1) . mat_buy_clac2 = pd.DataFrame(mat_buy_clac2) mat_buy_clac2 = mat_buy_clac2.rename(columns={0: &#39;buy_clac2_F20_prob&#39;, 1:&#39;buy_clac2_F30_prob&#39;, 2:&#39;buy_clac2_F40_prob&#39;, 3:&#39;buy_clac2_M20_prob&#39;, 4:&#39;buy_clac2_M30_prob&#39;, 5:&#39;buy_clac2_M40_prob&#39;}) . mat_clac3 = np.array(data_concat.iloc[:, -884:-1]).dot(lookup_table_clac3) mat_clac3 = mat_clac3 / mat_clac3.sum(1).reshape(-1, 1) . mat_clac3 = pd.DataFrame(mat_clac3) mat_clac3 = mat_clac3.rename(columns={0: &#39;given_clac3_F20_prob&#39;, 1:&#39;given_clac3_F30_prob&#39;, 2:&#39;given_clac3_F40_prob&#39;, 3:&#39;given_clac3_M20_prob&#39;, 4:&#39;given_clac3_M30_prob&#39;, 5:&#39;given_clac3_M40_prob&#39;}) . mat_buy_clac3 = np.where(np.array(data_concat.iloc[:, -884:-1]) &gt;= 1, 1, 0).dot(lookup_table_clac3) mat_buy_clac3 = mat_buy_clac3 / mat_buy_clac3.sum(1).reshape(-1, 1) . mat_buy_clac3 = pd.DataFrame(mat_buy_clac3) mat_buy_clac3 = mat_buy_clac3.rename(columns={0: &#39;buy_clac3_F20_prob&#39;, 1:&#39;buy_clac3_F30_prob&#39;, 2:&#39;buy_clac3_F40_prob&#39;, 3:&#39;buy_clac3_M20_prob&#39;, 4:&#39;buy_clac3_M30_prob&#39;, 5:&#39;buy_clac3_M40_prob&#39;}) . clac1_embed = np.load(&#39;C:/Users/user/Desktop/DHCN-main/datasets/clac1_nm/embedding.npy&#39;) . train_clac1_mean = np.array(data_concat.iloc[:, 11:48]).dot(clac1_embed) train_clac1_mean = train_clac1_mean / train_clac1_mean.sum(1).reshape(-1, 1) . train_clac1_weight = np.where(np.array(data_concat.iloc[:, 11:48]) &gt;= 1, 1, 0).dot(clac1_embed) train_clac1_weight = train_clac1_weight / train_clac1_weight.sum(1).reshape(-1, 1) . train_clac1_mean = pd.DataFrame(train_clac1_mean) train_clac1_weight = pd.DataFrame(train_clac1_weight) train_clac1_mean.columns = [&#39;clac1mean&#39;+str(i) for i in range(5)] train_clac1_weight.columns = [&#39;clac1weight&#39;+str(i) for i in range(5)] . clac2_embed = np.load(&#39;C:/Users/user/Desktop/DHCN-main/datasets/clac2_nm/embedding.npy&#39;) . train_clac2_mean = np.array(data_concat.iloc[:, 48:-884]).dot(clac2_embed) train_clac2_mean = train_clac2_mean / train_clac2_mean.sum(1).reshape(-1, 1) . train_clac2_weight = np.where(np.array(data_concat.iloc[:, 48:-884]) &gt;= 1, 1, 0).dot(clac2_embed) train_clac2_weight = train_clac2_weight / train_clac2_weight.sum(1).reshape(-1, 1) . train_clac2_mean = pd.DataFrame(train_clac2_mean) train_clac2_weight = pd.DataFrame(train_clac2_weight) train_clac2_mean.columns = [&#39;clac2mean&#39;+str(i) for i in range(10)] train_clac2_weight.columns = [&#39;clac2weight&#39;+str(i) for i in range(10)] . clac3_embed = np.load(&#39;C:/Users/user/Desktop/DHCN-main/datasets/clac3_nm/embedding.npy&#39;) . train_clac3_mean = np.array(data_concat.iloc[:, -884:-1]).dot(clac3_embed[:883]) train_clac3_mean = train_clac3_mean / train_clac3_mean.sum(1).reshape(-1, 1) . train_clac3_weight = np.where(np.array(data_concat.iloc[:, -884:-1]) &gt;= 1, 1, 0).dot(clac3_embed[:883]) train_clac3_weight = train_clac3_weight / train_clac3_weight.sum(1).reshape(-1, 1) . train_clac3_mean = pd.DataFrame(train_clac3_mean) train_clac3_weight = pd.DataFrame(train_clac3_weight) train_clac3_mean.columns = [&#39;clac3mean&#39;+str(i) for i in range(30)] train_clac3_weight.columns = [&#39;clac3weight&#39;+str(i) for i in range(30)] . data_concat[&#39;gender&#39;] = data_concat[&#39;label&#39;].map(lambda x: x[:1]) data_concat[&#39;age&#39;] = data_concat[&#39;label&#39;].map(lambda x: x[1:]) . data_concat . clnt_id num_shopping avg_price total_price avg_ct total_ct avg_sess_view total_sess_view avg_sess_hr total_sess_hr ... clac3_nm_876 clac3_nm_877 clac3_nm_878 clac3_nm_879 clac3_nm_880 clac3_nm_881 clac3_nm_882 label gender age . 0 0 | 2 | 43250.000000 | 86500 | 1.0 | 2 | 59.000000 | 118.0 | 922.000000 | 1844 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F20 | F | 20 | . 1 1 | 9 | 77777.777778 | 700000 | 1.0 | 9 | 132.333333 | 1191.0 | 1311.111111 | 11800 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | F | 30 | . 2 6 | 4 | 24225.000000 | 96900 | 1.0 | 4 | 21.750000 | 87.0 | 297.250000 | 1189 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F20 | F | 20 | . 3 9 | 2 | 10550.000000 | 21100 | 1.0 | 2 | 249.000000 | 498.0 | 5049.000000 | 10098 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | F | 30 | . 4 12 | 16 | 14325.625000 | 229210 | 1.0 | 16 | 144.937500 | 2319.0 | 4187.250000 | 66996 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | F | 30 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 149995 263094 | 1 | 10000.000000 | 10000 | 1.0 | 1 | 66.000000 | 66.0 | 513.000000 | 513 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | F | 30 | . 149996 263095 | 2 | 122000.000000 | 244000 | 1.0 | 2 | 220.500000 | 441.0 | 1828.000000 | 3656 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | F | 30 | . 149997 263096 | 3 | 28500.000000 | 85500 | 1.0 | 3 | 256.000000 | 768.0 | 4237.000000 | 12711 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | F | 30 | . 149998 263102 | 1 | 1080.000000 | 1080 | 1.0 | 1 | 188.000000 | 188.0 | 1812.000000 | 1812 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | F | 30 | . 149999 263103 | 7 | 58628.571429 | 410400 | 1.0 | 7 | 280.000000 | 1960.0 | 3249.428571 | 22746 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | F30 | F | 30 | . 150000 rows × 1062 columns . label_dic = {} for i, label in enumerate([&#39;F20&#39;, &#39;F30&#39;, &#39;F40&#39;, &#39;M20&#39;, &#39;M30&#39;, &#39;M40&#39;]): label_dic[label] = i . label_dic . {&#39;F20&#39;: 0, &#39;F30&#39;: 1, &#39;F40&#39;: 2, &#39;M20&#39;: 3, &#39;M30&#39;: 4, &#39;M40&#39;: 5} . gender_dic = {} for i, gender in enumerate([&#39;F&#39;, &#39;M&#39;]): gender_dic[gender] = i . gender_dic . {&#39;F&#39;: 0, &#39;M&#39;: 1} . age_dic = {} for i, age in enumerate([&#39;20&#39;, &#39;30&#39;, &#39;40&#39;]): age_dic[age] = i . age_dic . {&#39;20&#39;: 0, &#39;30&#39;: 1, &#39;40&#39;: 2} . data_concat[&#39;label&#39;] = data_concat[&#39;label&#39;].map(lambda x: label_dic[x]) data_concat[&#39;gender&#39;] = data_concat[&#39;gender&#39;].map(lambda x: gender_dic[x]) data_concat[&#39;age&#39;] = data_concat[&#39;age&#39;].map(lambda x: age_dic[x]) . (np.array(mat_clac1).argmax(1) == data_concat[&#39;label&#39;].values).sum() # 50456 . 52069 . (np.array(mat_buy_clac1).argmax(1) == data_concat[&#39;label&#39;].values).sum() # 51031 . 52940 . (np.array(mat_clac2).argmax(1) == data_concat[&#39;label&#39;].values).sum() # 55530 . 57561 . (np.array(mat_buy_clac2).argmax(1) == data_concat[&#39;label&#39;].values).sum() # 55932 . 58263 . (np.array(mat_clac3).argmax(1) == data_concat[&#39;label&#39;].values).sum() # 56845 . 58976 . (np.array(mat_buy_clac3).argmax(1) == data_concat[&#39;label&#39;].values).sum() # 57160 . 59451 . word2vec . from gensim.models import word2vec . def oversample(x, n): lst = [] for i in x: tmp = [] for j in range(n): random.shuffle(i) tmp += list(i) lst.append(tmp) return lst . Product code Word2vec . df[&#39;PD_C&#39;] = df[&#39;PD_C&#39;].astype(&#39;string&#39;) df_test[&#39;PD_C&#39;] = df_test[&#39;PD_C&#39;].astype(&#39;string&#39;) . train_pd_w2v = list(df.groupby(&#39;CLNT_ID&#39;)[&#39;PD_C&#39;].unique()[y[&#39;CLNT_ID&#39;].values]) test_pd_w2v = list(df_test.groupby(&#39;CLNT_ID&#39;)[&#39;PD_C&#39;].unique()[df_test[&#39;CLNT_ID&#39;].unique()]) . tt = df.groupby(&#39;CLNT_ID&#39;)[&#39;PD_C&#39;].value_counts()[y[&#39;CLNT_ID&#39;].values] kk = y[&#39;CLNT_ID&#39;].values ww = df.groupby([&#39;CLNT_ID&#39;, &#39;PD_C&#39;])[&#39;KWD_NM&#39;].count() . train_pd_w2v.extend(test_pd_w2v) . pd_w2v_input = oversample(train_pd_w2v, 10) . pd_w2v = word2vec.Word2Vec(sentences=pd_w2v_input, size=256, window=3, min_count=1, sg=1).wv . pd_mean_vector = [] for words in tqdm(train_pd_w2v[:-len(test_pd_w2v)]): tmp = np.zeros(256) cnt = 0 for word in words: try: tmp += pd_w2v[word] cnt += 1 except: pass #tmp /= cnt pd_mean_vector.append(tmp) pd_mean_vector = np.array(pd_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:05&lt;00:00, 28873.49it/s] . pd_unweight_vector = [] for i, words in tqdm(enumerate(train_pd_w2v[:-len(test_pd_w2v)])): tmp = np.zeros(256) cnt = 0 for word in words: try: tmp += pd_w2v[word] * tt[kk[i]][word] cnt += tt[kk[i]][word] except: pass #tmp /= cnt pd_unweight_vector.append(tmp) pd_unweight_vector = np.array(pd_unweight_vector) . 150000it [04:30, 553.55it/s] . pd_weight_vector = [] for i, words in tqdm(enumerate(train_pd_w2v[:-len(test_pd_w2v)])): tmp = np.zeros(256) cnt = 0 for word in words: try: tmp += pd_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt pd_weight_vector.append(tmp) pd_weight_vector = np.array(pd_weight_vector) . 150000it [12:04, 207.08it/s] . train_pd_mean = pd_mean_vector train_pd_unweight = pd_unweight_vector train_pd_weight = pd_weight_vector train_pd_mean = pd.DataFrame(train_pd_mean) train_pd_unweight = pd.DataFrame(train_pd_unweight) train_pd_weight = pd.DataFrame(train_pd_weight) train_pd_mean.columns = [&#39;pd_mean_&#39;+str(i) for i in range(256)] train_pd_unweight.columns = [&#39;pd_unweight_&#39;+str(i) for i in range(256)] train_pd_weight.columns = [&#39;pd_weight_&#39;+str(i) for i in range(256)] . df[&#39;PD_BRA_NM&#39;] = df[&#39;PD_BRA_NM&#39;].astype(&#39;string&#39;) df_test[&#39;PD_BRA_NM&#39;] = df_test[&#39;PD_BRA_NM&#39;].astype(&#39;string&#39;) . brand name word2vec . train_bra_w2v = list(df.groupby(&#39;CLNT_ID&#39;)[&#39;PD_BRA_NM&#39;].unique()[y[&#39;CLNT_ID&#39;].values]) test_bra_w2v = list(df_test.groupby(&#39;CLNT_ID&#39;)[&#39;PD_BRA_NM&#39;].unique()[df_test[&#39;CLNT_ID&#39;].unique()]) . tt = df.groupby(&#39;CLNT_ID&#39;)[&#39;PD_BRA_NM&#39;].value_counts()[y[&#39;CLNT_ID&#39;].values] ## tt : kk에 해당하는 product name count 값 kk = y[&#39;CLNT_ID&#39;].values ## kk : client id list ww = df.groupby([&#39;CLNT_ID&#39;, &#39;PD_BRA_NM&#39;])[&#39;KWD_NM&#39;].count() ## client id, brand name로 그룹핑한 keyword들마다의 개수 . train_bra_w2v.extend(test_bra_w2v) . bra_w2v_input = oversample(train_bra_w2v, 10) . bra_w2v = word2vec.Word2Vec(sentences=bra_w2v_input, size=128, window=3, min_count=1, sg=1).wv . bra_mean_vector = [] for words in tqdm(train_bra_w2v[:-len(test_bra_w2v)]): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += bra_w2v[word] cnt += 1 except: pass #tmp /= cnt bra_mean_vector.append(tmp) bra_mean_vector = np.array(bra_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:03&lt;00:00, 40477.85it/s] . bra_unweight_vector = [] for i, words in tqdm(enumerate(train_bra_w2v[:-len(test_bra_w2v)])): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += bra_w2v[word] * tt[kk[i]][word] cnt += tt[kk[i]][word] except: pass #tmp /= cnt bra_unweight_vector.append(tmp) bra_unweight_vector = np.array(bra_unweight_vector) . 150000it [03:17, 758.58it/s] . bra_weight_vector = [] for i, words in tqdm(enumerate(train_bra_w2v[:-len(test_bra_w2v)])): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += bra_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt bra_weight_vector.append(tmp) bra_weight_vector = np.array(bra_weight_vector) . 150000it [08:57, 278.88it/s] . train_bra_mean = bra_mean_vector train_bra_unweight = bra_unweight_vector train_bra_weight = bra_weight_vector train_bra_mean = pd.DataFrame(train_bra_mean) train_bra_unweight = pd.DataFrame(train_bra_unweight) train_bra_weight = pd.DataFrame(train_bra_weight) train_bra_mean.columns = [&#39;bra_mean_&#39;+str(i) for i in range(128)] train_bra_unweight.columns = [&#39;bra_unweight_&#39;+str(i) for i in range(128)] train_bra_weight.columns = [&#39;bra_weight_&#39;+str(i) for i in range(128)] . keyword_name word2vec . df[&#39;KWD_NM&#39;] = df[&#39;KWD_NM&#39;].astype(&#39;string&#39;) df_test[&#39;KWD_NM&#39;] = df_test[&#39;KWD_NM&#39;].astype(&#39;string&#39;) . train_kwd_w2v = list(df.groupby(&#39;CLNT_ID&#39;)[&#39;KWD_NM&#39;].unique()[y[&#39;CLNT_ID&#39;].values]) test_kwd_w2v = list(df_test.groupby(&#39;CLNT_ID&#39;)[&#39;KWD_NM&#39;].unique()[df_test[&#39;CLNT_ID&#39;].unique()]) . tt = df.groupby(&#39;CLNT_ID&#39;)[&#39;KWD_NM&#39;].value_counts()[y[&#39;CLNT_ID&#39;].values] kk = y[&#39;CLNT_ID&#39;].values ww = df.groupby([&#39;CLNT_ID&#39;, &#39;KWD_NM&#39;])[&#39;KWD_NM&#39;].count() . train_kwd_w2v.extend(test_kwd_w2v) . kwd_w2v_input = oversample(train_kwd_w2v, 10) . kwd_w2v = word2vec.Word2Vec(sentences=kwd_w2v_input, size=128, window=3, min_count=1, sg=1).wv . kwd_mean_vector = [] for words in tqdm(train_kwd_w2v[:-len(test_kwd_w2v)]): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += kwd_w2v[word] cnt += 1 except: pass #tmp /= cnt kwd_mean_vector.append(tmp) kwd_mean_vector = np.array(kwd_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:06&lt;00:00, 23661.92it/s] . kwd_unweight_vector = [] for i, words in tqdm(enumerate(train_kwd_w2v[:-len(test_kwd_w2v)])): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += kwd_w2v[word] * tt[kk[i]][word] cnt += tt[kk[i]][word] except: pass #tmp /= cnt kwd_unweight_vector.append(tmp) kwd_unweight_vector = np.array(kwd_unweight_vector) . 150000it [05:42, 437.61it/s] . kwd_weight_vector = [] for i, words in tqdm(enumerate(train_kwd_w2v[:-len(test_kwd_w2v)])): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += kwd_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt kwd_weight_vector.append(tmp) kwd_weight_vector = np.array(kwd_weight_vector) . 150000it [15:38, 159.76it/s] . train_kwd_mean = kwd_mean_vector train_kwd_unweight = kwd_unweight_vector train_kwd_weight = kwd_weight_vector train_kwd_mean = pd.DataFrame(train_kwd_mean) train_kwd_unweight = pd.DataFrame(train_kwd_unweight) train_kwd_weight = pd.DataFrame(train_kwd_weight) train_kwd_mean.columns = [&#39;kwd_mean_&#39;+str(i) for i in range(128)] train_kwd_weight.columns = [&#39;kwd_unweight_&#39;+str(i) for i in range(128)] train_kwd_weight.columns = [&#39;kwd_weight_&#39;+str(i) for i in range(128)] . CLAC3_NM word2vec . df[&#39;CLAC3_NM&#39;] = df[&#39;CLAC3_NM&#39;].astype(&#39;string&#39;) df_test[&#39;CLAC3_NM&#39;] = df_test[&#39;CLAC3_NM&#39;].astype(&#39;string&#39;) . train_clac3_w2v = list(df.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC3_NM&#39;].unique()[y[&#39;CLNT_ID&#39;].values]) test_clac3_w2v = list(df_test.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC3_NM&#39;].unique()[df_test[&#39;CLNT_ID&#39;].unique()]) . tt = df.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC3_NM&#39;].value_counts()[y[&#39;CLNT_ID&#39;].values] kk = y[&#39;CLNT_ID&#39;].values ww = df.groupby([&#39;CLNT_ID&#39;, &#39;CLAC3_NM&#39;])[&#39;KWD_NM&#39;].count() . train_clac3_w2v.extend(test_clac3_w2v) . clac3_w2v_input = oversample(train_clac3_w2v, 5) . clac3_w2v = word2vec.Word2Vec(sentences=clac3_w2v_input, size=30, window=3, min_count=1, sg=1).wv . clac3_mean_vector = [] for words in tqdm(train_clac3_w2v[:-len(test_clac3_w2v)]): tmp = np.zeros(30) cnt = 0 for word in words: try: tmp += clac3_w2v[word] cnt += 1 except: pass #tmp /= cnt clac3_mean_vector.append(tmp) clac3_mean_vector = np.array(clac3_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:03&lt;00:00, 40631.44it/s] . clac3_unweight_vector = [] for i, words in tqdm(enumerate(train_clac3_w2v[:-len(test_clac3_w2v)])): tmp = np.zeros(30) cnt = 0 for word in words: try: tmp += clac3_w2v[word] * clac3_matrix[i, clac3_nm_dic[word]] cnt += clac3_matrix[i, clac3_nm_dic[word]] except: pass #tmp /= cnt clac3_unweight_vector.append(tmp) clac3_unweight_vector = np.array(clac3_unweight_vector) . 150000it [00:05, 26368.42it/s] . clac3_weight_vector = [] for i, words in tqdm(enumerate(train_clac3_w2v[:-len(test_clac3_w2v)])): tmp = np.zeros(30) cnt = 0 for word in words: try: tmp += clac3_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt clac3_weight_vector.append(tmp) clac3_weight_vector = np.array(clac3_weight_vector) . 150000it [08:58, 278.79it/s] . train_clac3_mean = clac3_mean_vector train_clac3_unweight = clac3_unweight_vector train_clac3_weight = clac3_weight_vector train_clac3_mean = pd.DataFrame(train_clac3_mean) train_clac3_unweight = pd.DataFrame(train_clac3_unweight) train_clac3_weight = pd.DataFrame(train_clac3_weight) train_clac3_mean.columns = [&#39;clac3_mean_&#39;+str(i) for i in range(30)] train_clac3_unweight.columns = [&#39;clac3_unweight_&#39;+str(i) for i in range(30)] train_clac3_weight.columns = [&#39;clac3_weight_&#39;+str(i) for i in range(30)] . CLAC2_NM word2vec . df[&#39;CLAC2_NM&#39;] = df[&#39;CLAC2_NM&#39;].astype(&#39;string&#39;) df_test[&#39;CLAC2_NM&#39;] = df_test[&#39;CLAC2_NM&#39;].astype(&#39;string&#39;) . train_clac2_w2v = list(df.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC2_NM&#39;].unique()[y[&#39;CLNT_ID&#39;].values]) test_clac2_w2v = list(df_test.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC2_NM&#39;].unique()[df_test[&#39;CLNT_ID&#39;].unique()]) . tt = df.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC2_NM&#39;].value_counts()[y[&#39;CLNT_ID&#39;].values] kk = y[&#39;CLNT_ID&#39;].values ww = df.groupby([&#39;CLNT_ID&#39;, &#39;CLAC2_NM&#39;])[&#39;KWD_NM&#39;].count() . train_clac2_w2v.extend(test_clac2_w2v) . clac2_w2v_input = oversample(train_clac2_w2v, 5) . clac2_w2v = word2vec.Word2Vec(sentences=clac2_w2v_input, size=10, window=3, min_count=1, sg=1).wv . clac2_mean_vector = [] for words in tqdm(train_clac2_w2v[:-len(test_clac2_w2v)]): tmp = np.zeros(10) cnt = 0 for word in words: try: tmp += clac2_w2v[word] cnt += 1 except: pass #tmp /= cnt clac2_mean_vector.append(tmp) clac2_mean_vector = np.array(clac2_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:03&lt;00:00, 45694.59it/s] . clac2_unweight_vector = [] for i, words in tqdm(enumerate(train_clac2_w2v[:-len(test_clac2_w2v)])): tmp = np.zeros(10) cnt = 0 for word in words: try: tmp += clac2_w2v[word] * clac2_matrix[i, clac2_nm_dic[word]] cnt += clac2_matrix[i, clac2_nm_dic[word]] except: pass #tmp /= cnt clac2_unweight_vector.append(tmp) clac2_unweight_vector = np.array(clac2_unweight_vector) . 150000it [00:04, 31135.69it/s] . clac2_weight_vector = [] for i, words in tqdm(enumerate(train_clac2_w2v[:-len(test_clac2_w2v)])): tmp = np.zeros(10) cnt = 0 for word in words: try: tmp += clac2_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt clac2_weight_vector.append(tmp) clac2_weight_vector = np.array(clac2_weight_vector) . 150000it [07:40, 325.76it/s] . train_clac2_mean = clac2_mean_vector train_clac2_unweight = clac2_unweight_vector train_clac2_weight = clac2_weight_vector train_clac2_mean = pd.DataFrame(train_clac2_mean) train_clac2_unweight = pd.DataFrame(train_clac2_unweight) train_clac2_weight = pd.DataFrame(train_clac2_weight) train_clac2_mean.columns = [&#39;clac2_mean_&#39;+str(i) for i in range(10)] train_clac2_unweight.columns = [&#39;clac2_unweight_&#39;+str(i) for i in range(10)] train_clac2_weight.columns = [&#39;clac2_weight_&#39;+str(i) for i in range(10)] . CLAC1_NM word2vec . df[&#39;CLAC1_NM&#39;] = df[&#39;CLAC1_NM&#39;].astype(&#39;string&#39;) df_test[&#39;CLAC1_NM&#39;] = df_test[&#39;CLAC1_NM&#39;].astype(&#39;string&#39;) . train_clac1_w2v = list(df.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC1_NM&#39;].unique()[y[&#39;CLNT_ID&#39;].values]) test_clac1_w2v = list(df_test.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC1_NM&#39;].unique()[df_test[&#39;CLNT_ID&#39;].unique()]) . tt = df.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC1_NM&#39;].value_counts()[y[&#39;CLNT_ID&#39;].values] kk = y[&#39;CLNT_ID&#39;].values ww = df.groupby([&#39;CLNT_ID&#39;, &#39;CLAC1_NM&#39;])[&#39;KWD_NM&#39;].count() . train_clac1_w2v.extend(test_clac1_w2v) . clac1_w2v_input = oversample(train_clac1_w2v, 5) . clac1_w2v = word2vec.Word2Vec(sentences=clac1_w2v_input, size=5, window=3, min_count=1, sg=1).wv . clac1_mean_vector = [] for words in tqdm(train_clac1_w2v[:-len(test_clac1_w2v)]): tmp = np.zeros(5) cnt = 0 for word in words: try: tmp += clac1_w2v[word] cnt += 1 except: pass #tmp /= cnt clac1_mean_vector.append(tmp) clac1_mean_vector = np.array(clac1_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 150000/150000 [00:02&lt;00:00, 56621.56it/s] . clac1_unweight_vector = [] for i, words in tqdm(enumerate(train_clac1_w2v[:-len(test_clac1_w2v)])): tmp = np.zeros(5) cnt = 0 for word in words: try: tmp += clac1_w2v[word] * clac1_matrix[i, clac1_nm_dic[word]] cnt += clac1_matrix[i, clac1_nm_dic[word]] except: pass #tmp /= cnt clac1_unweight_vector.append(tmp) clac1_unweight_vector = np.array(clac1_unweight_vector) . 150000it [00:03, 38266.09it/s] . clac1_weight_vector = [] for i, words in tqdm(enumerate(train_clac1_w2v[:-len(test_clac1_w2v)])): tmp = np.zeros(5) cnt = 0 for word in words: try: tmp += clac1_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt clac1_weight_vector.append(tmp) clac1_weight_vector = np.array(clac1_weight_vector) . 150000it [05:59, 416.81it/s] . train_clac1_mean = clac1_mean_vector train_clac1_unweight = clac1_unweight_vector train_clac1_weight = clac1_weight_vector train_clac1_mean = pd.DataFrame(train_clac1_mean) train_clac1_unweight = pd.DataFrame(train_clac1_unweight) train_clac1_weight = pd.DataFrame(train_clac1_weight) train_clac1_mean.columns = [&#39;clac1_mean_&#39;+str(i) for i in range(5)] train_clac1_unweight.columns = [&#39;clac1_unweight_&#39;+str(i) for i in range(5)] train_clac1_weight.columns = [&#39;clac1_weight_&#39;+str(i) for i in range(5)] . data_concat = pd.concat([data_concat.iloc[:, 1:11], mat_clac1, mat_buy_clac1, mat_clac2, mat_buy_clac2, mat_clac3, mat_buy_clac3, train_pd_mean, train_bra_mean, train_kwd_mean, train_clac1_mean, train_clac2_mean, train_clac3_mean, train_pd_unweight, train_bra_unweight, train_kwd_unweight, train_clac1_unweight, train_clac2_unweight, train_clac3_unweight, train_pd_weight, train_bra_weight, train_kwd_weight, train_clac1_weight, train_clac2_weight, train_clac3_weight, data_concat.iloc[:, -3:]], axis=1) . data_concat . num_shopping avg_price total_price avg_ct total_ct avg_sess_view total_sess_view avg_sess_hr total_sess_hr avg_shopping_interval ... clac3_weight_23 clac3_weight_24 clac3_weight_25 clac3_weight_26 clac3_weight_27 clac3_weight_28 clac3_weight_29 label gender age . 0 2 | 43250.000000 | 86500 | 1.0 | 2 | 59.000000 | 118.0 | 922.000000 | 1844 | 0.000000 | ... | -0.025476 | -0.358678 | -0.186819 | 0.192818 | 0.434122 | -0.107545 | 0.063859 | 0 | 0 | 0 | . 1 9 | 77777.777778 | 700000 | 1.0 | 9 | 132.333333 | 1191.0 | 1311.111111 | 11800 | 3.625000 | ... | -0.513720 | 0.286228 | 0.180560 | 0.383307 | 0.273483 | 0.445723 | -0.301021 | 1 | 0 | 1 | . 2 4 | 24225.000000 | 96900 | 1.0 | 4 | 21.750000 | 87.0 | 297.250000 | 1189 | 5.666667 | ... | -0.164428 | 0.047147 | 0.298240 | -0.305354 | 0.558061 | 0.195794 | 0.031537 | 0 | 0 | 0 | . 3 2 | 10550.000000 | 21100 | 1.0 | 2 | 249.000000 | 498.0 | 5049.000000 | 10098 | 0.000000 | ... | -0.527290 | -0.260950 | 0.230153 | -0.477711 | -0.060666 | -0.118800 | -0.559288 | 1 | 0 | 1 | . 4 16 | 14325.625000 | 229210 | 1.0 | 16 | 144.937500 | 2319.0 | 4187.250000 | 66996 | 4.200000 | ... | -0.257740 | -0.554550 | -0.216515 | -0.312947 | -0.201764 | 0.072622 | -0.378338 | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 149995 1 | 10000.000000 | 10000 | 1.0 | 1 | 66.000000 | 66.0 | 513.000000 | 513 | 183.000000 | ... | -0.105944 | -1.130829 | 0.387124 | 0.286016 | 0.126334 | 0.426996 | 0.029120 | 1 | 0 | 1 | . 149996 2 | 122000.000000 | 244000 | 1.0 | 2 | 220.500000 | 441.0 | 1828.000000 | 3656 | 83.000000 | ... | -0.719438 | -0.272537 | -0.384826 | -0.215291 | -0.045238 | -0.005138 | -0.127866 | 1 | 0 | 1 | . 149997 3 | 28500.000000 | 85500 | 1.0 | 3 | 256.000000 | 768.0 | 4237.000000 | 12711 | 0.000000 | ... | -0.675275 | -0.445863 | -0.125119 | 0.457581 | 0.359894 | 0.512207 | -0.033372 | 1 | 0 | 1 | . 149998 1 | 1080.000000 | 1080 | 1.0 | 1 | 188.000000 | 188.0 | 1812.000000 | 1812 | 183.000000 | ... | 0.113508 | 0.063089 | 0.235095 | 0.160695 | 0.016483 | 0.776254 | -0.536355 | 1 | 0 | 1 | . 149999 7 | 58628.571429 | 410400 | 1.0 | 7 | 280.000000 | 1960.0 | 3249.428571 | 22746 | 4.666667 | ... | -0.240615 | 0.007781 | -0.318051 | 0.228597 | 0.180339 | 0.140052 | -0.201356 | 1 | 0 | 1 | . 150000 rows × 1720 columns . data_concat = data_concat.loc[:, list(data_concat.columns[:-3].values) + [&#39;gender&#39;, &#39;age&#39;, &#39;label&#39;]] . data_concat . num_shopping avg_price total_price avg_ct total_ct avg_sess_view total_sess_view avg_sess_hr total_sess_hr avg_shopping_interval ... clac3_weight_23 clac3_weight_24 clac3_weight_25 clac3_weight_26 clac3_weight_27 clac3_weight_28 clac3_weight_29 gender age label . 0 2 | 43250.000000 | 86500 | 1.0 | 2 | 59.000000 | 118.0 | 922.000000 | 1844 | 0.000000 | ... | -0.025476 | -0.358678 | -0.186819 | 0.192818 | 0.434122 | -0.107545 | 0.063859 | 0 | 0 | 0 | . 1 9 | 77777.777778 | 700000 | 1.0 | 9 | 132.333333 | 1191.0 | 1311.111111 | 11800 | 3.625000 | ... | -0.513720 | 0.286228 | 0.180560 | 0.383307 | 0.273483 | 0.445723 | -0.301021 | 0 | 1 | 1 | . 2 4 | 24225.000000 | 96900 | 1.0 | 4 | 21.750000 | 87.0 | 297.250000 | 1189 | 5.666667 | ... | -0.164428 | 0.047147 | 0.298240 | -0.305354 | 0.558061 | 0.195794 | 0.031537 | 0 | 0 | 0 | . 3 2 | 10550.000000 | 21100 | 1.0 | 2 | 249.000000 | 498.0 | 5049.000000 | 10098 | 0.000000 | ... | -0.527290 | -0.260950 | 0.230153 | -0.477711 | -0.060666 | -0.118800 | -0.559288 | 0 | 1 | 1 | . 4 16 | 14325.625000 | 229210 | 1.0 | 16 | 144.937500 | 2319.0 | 4187.250000 | 66996 | 4.200000 | ... | -0.257740 | -0.554550 | -0.216515 | -0.312947 | -0.201764 | 0.072622 | -0.378338 | 0 | 1 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 149995 1 | 10000.000000 | 10000 | 1.0 | 1 | 66.000000 | 66.0 | 513.000000 | 513 | 183.000000 | ... | -0.105944 | -1.130829 | 0.387124 | 0.286016 | 0.126334 | 0.426996 | 0.029120 | 0 | 1 | 1 | . 149996 2 | 122000.000000 | 244000 | 1.0 | 2 | 220.500000 | 441.0 | 1828.000000 | 3656 | 83.000000 | ... | -0.719438 | -0.272537 | -0.384826 | -0.215291 | -0.045238 | -0.005138 | -0.127866 | 0 | 1 | 1 | . 149997 3 | 28500.000000 | 85500 | 1.0 | 3 | 256.000000 | 768.0 | 4237.000000 | 12711 | 0.000000 | ... | -0.675275 | -0.445863 | -0.125119 | 0.457581 | 0.359894 | 0.512207 | -0.033372 | 0 | 1 | 1 | . 149998 1 | 1080.000000 | 1080 | 1.0 | 1 | 188.000000 | 188.0 | 1812.000000 | 1812 | 183.000000 | ... | 0.113508 | 0.063089 | 0.235095 | 0.160695 | 0.016483 | 0.776254 | -0.536355 | 0 | 1 | 1 | . 149999 7 | 58628.571429 | 410400 | 1.0 | 7 | 280.000000 | 1960.0 | 3249.428571 | 22746 | 4.666667 | ... | -0.240615 | 0.007781 | -0.318051 | 0.228597 | 0.180339 | 0.140052 | -0.201356 | 0 | 1 | 1 | . 150000 rows × 1720 columns . data_concat.to_csv(&#39;train.csv&#39;, index=False) . import pandas as pd import numpy as np data_concat = pd.read_csv(&#39;train.csv&#39;) train_stat = pd.read_csv(&#39;stat_df.csv&#39;) . data_concat = pd.concat([train_stat.iloc[:, 1:], data_concat.iloc[:, 10:]], axis=1) . train . import tensorflow as tf from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import log_loss . early_stop = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=15, restore_best_weights=True) . X_train, X_valid, y_train, y_valid = train_test_split(data_concat.iloc[:, :-1], data_concat.iloc[:, -1], test_size=0.2, stratify=data_concat.iloc[:, -1]) . scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.iloc[:, :-2]) X_valid_scaled = scaler.transform(X_valid.iloc[:, :-2]) . model1 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model1.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model1.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) # 1.1643 . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 14us/sample - loss: 1.2876 - accuracy: 0.4757 - val_loss: 1.1988 - val_accuracy: 0.5098 Epoch 2/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.2033 - accuracy: 0.5061 - val_loss: 1.1858 - val_accuracy: 0.5216 Epoch 3/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1860 - accuracy: 0.5135 - val_loss: 1.1775 - val_accuracy: 0.5181 Epoch 4/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1728 - accuracy: 0.5189 - val_loss: 1.1708 - val_accuracy: 0.5203 Epoch 5/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1614 - accuracy: 0.5215 - val_loss: 1.1714 - val_accuracy: 0.5197 Epoch 6/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1547 - accuracy: 0.5244 - val_loss: 1.1686 - val_accuracy: 0.5238 Epoch 7/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1477 - accuracy: 0.5273 - val_loss: 1.1698 - val_accuracy: 0.5240 Epoch 8/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1396 - accuracy: 0.5285 - val_loss: 1.1671 - val_accuracy: 0.5233 Epoch 9/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1311 - accuracy: 0.5331 - val_loss: 1.1669 - val_accuracy: 0.5263 Epoch 10/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1241 - accuracy: 0.5363 - val_loss: 1.1680 - val_accuracy: 0.5226 Epoch 11/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1168 - accuracy: 0.5394 - val_loss: 1.1749 - val_accuracy: 0.5262 Epoch 12/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1100 - accuracy: 0.5412 - val_loss: 1.1693 - val_accuracy: 0.5222 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0998 - accuracy: 0.5455 - val_loss: 1.1746 - val_accuracy: 0.5241 Epoch 14/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0928 - accuracy: 0.5491 - val_loss: 1.1764 - val_accuracy: 0.5183 Epoch 15/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0833 - accuracy: 0.5515 - val_loss: 1.1762 - val_accuracy: 0.5215 Epoch 16/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0752 - accuracy: 0.5545 - val_loss: 1.1845 - val_accuracy: 0.5182 Epoch 17/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0647 - accuracy: 0.5591 - val_loss: 1.1855 - val_accuracy: 0.5202 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0572 - accuracy: 0.5630 - val_loss: 1.1898 - val_accuracy: 0.5177 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0477 - accuracy: 0.5668 - val_loss: 1.1938 - val_accuracy: 0.5160 Epoch 20/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0414 - accuracy: 0.5679 - val_loss: 1.1938 - val_accuracy: 0.5190 Epoch 21/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0305 - accuracy: 0.5741 - val_loss: 1.1996 - val_accuracy: 0.5205 Epoch 22/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0219 - accuracy: 0.5776 - val_loss: 1.2094 - val_accuracy: 0.5138 Epoch 23/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0132 - accuracy: 0.5825 - val_loss: 1.2100 - val_accuracy: 0.5175 Epoch 24/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0028 - accuracy: 0.5836 - val_loss: 1.2150 - val_accuracy: 0.5166 . model2 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model2.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model2.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 20us/sample - loss: 1.2883 - accuracy: 0.4768 - val_loss: 1.1937 - val_accuracy: 0.5115 Epoch 2/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.2037 - accuracy: 0.5047 - val_loss: 1.1856 - val_accuracy: 0.5169 Epoch 3/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1870 - accuracy: 0.5116 - val_loss: 1.1714 - val_accuracy: 0.5220 Epoch 4/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1753 - accuracy: 0.5175 - val_loss: 1.1733 - val_accuracy: 0.5183 Epoch 5/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1671 - accuracy: 0.5186 - val_loss: 1.1650 - val_accuracy: 0.5235 Epoch 6/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1574 - accuracy: 0.5222 - val_loss: 1.1639 - val_accuracy: 0.5215 Epoch 7/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1482 - accuracy: 0.5269 - val_loss: 1.1697 - val_accuracy: 0.5242 Epoch 8/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1407 - accuracy: 0.5287 - val_loss: 1.1659 - val_accuracy: 0.5224 Epoch 9/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1326 - accuracy: 0.5319 - val_loss: 1.1650 - val_accuracy: 0.5239 Epoch 10/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1299 - accuracy: 0.5336 - val_loss: 1.1712 - val_accuracy: 0.5198 Epoch 11/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1207 - accuracy: 0.5364 - val_loss: 1.1685 - val_accuracy: 0.5249 Epoch 12/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1113 - accuracy: 0.5391 - val_loss: 1.1700 - val_accuracy: 0.5238 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1016 - accuracy: 0.5449 - val_loss: 1.1721 - val_accuracy: 0.5237 Epoch 14/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0938 - accuracy: 0.5469 - val_loss: 1.1760 - val_accuracy: 0.5224 Epoch 15/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0920 - accuracy: 0.5491 - val_loss: 1.1775 - val_accuracy: 0.5231 Epoch 16/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0815 - accuracy: 0.5513 - val_loss: 1.1788 - val_accuracy: 0.5232 Epoch 17/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0699 - accuracy: 0.5575 - val_loss: 1.1816 - val_accuracy: 0.5170 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0605 - accuracy: 0.5599 - val_loss: 1.1851 - val_accuracy: 0.5183 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0544 - accuracy: 0.5630 - val_loss: 1.1870 - val_accuracy: 0.5181 Epoch 20/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0445 - accuracy: 0.5677 - val_loss: 1.1968 - val_accuracy: 0.5166 Epoch 21/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0313 - accuracy: 0.5734 - val_loss: 1.1980 - val_accuracy: 0.5177 . model3 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model3.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model3.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 13us/sample - loss: 1.2943 - accuracy: 0.4743 - val_loss: 1.1932 - val_accuracy: 0.5142 Epoch 2/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.2040 - accuracy: 0.5047 - val_loss: 1.1846 - val_accuracy: 0.5208 Epoch 3/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1854 - accuracy: 0.5118 - val_loss: 1.1800 - val_accuracy: 0.5165 Epoch 4/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1731 - accuracy: 0.5170 - val_loss: 1.1680 - val_accuracy: 0.5226 Epoch 5/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1612 - accuracy: 0.5207 - val_loss: 1.1703 - val_accuracy: 0.5201 Epoch 6/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1582 - accuracy: 0.5248 - val_loss: 1.1647 - val_accuracy: 0.5209 Epoch 7/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1482 - accuracy: 0.5284 - val_loss: 1.1669 - val_accuracy: 0.5240 Epoch 8/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1471 - accuracy: 0.5282 - val_loss: 1.1637 - val_accuracy: 0.5226 Epoch 9/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1350 - accuracy: 0.5323 - val_loss: 1.1623 - val_accuracy: 0.5271 Epoch 10/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1264 - accuracy: 0.5356 - val_loss: 1.1682 - val_accuracy: 0.5209 Epoch 11/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1188 - accuracy: 0.5386 - val_loss: 1.1714 - val_accuracy: 0.5229 Epoch 12/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1104 - accuracy: 0.5401 - val_loss: 1.1712 - val_accuracy: 0.5230 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1029 - accuracy: 0.5439 - val_loss: 1.1687 - val_accuracy: 0.5228 Epoch 14/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0960 - accuracy: 0.5459 - val_loss: 1.1703 - val_accuracy: 0.5251 Epoch 15/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0880 - accuracy: 0.5498 - val_loss: 1.1787 - val_accuracy: 0.5218 Epoch 16/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0809 - accuracy: 0.5530 - val_loss: 1.1783 - val_accuracy: 0.5242 Epoch 17/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0695 - accuracy: 0.5559 - val_loss: 1.1844 - val_accuracy: 0.5206 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0642 - accuracy: 0.5592 - val_loss: 1.1886 - val_accuracy: 0.5204 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0524 - accuracy: 0.5621 - val_loss: 1.1909 - val_accuracy: 0.5228 Epoch 20/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0445 - accuracy: 0.5690 - val_loss: 1.1981 - val_accuracy: 0.5181 Epoch 21/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0343 - accuracy: 0.5729 - val_loss: 1.1950 - val_accuracy: 0.5169 Epoch 22/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0262 - accuracy: 0.5761 - val_loss: 1.2008 - val_accuracy: 0.5169 Epoch 23/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0172 - accuracy: 0.5792 - val_loss: 1.2079 - val_accuracy: 0.5141 Epoch 24/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0089 - accuracy: 0.5843 - val_loss: 1.2081 - val_accuracy: 0.5143 . model4 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model4.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model4.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 13us/sample - loss: 1.2951 - accuracy: 0.4740 - val_loss: 1.1908 - val_accuracy: 0.5107 Epoch 2/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.2020 - accuracy: 0.5061 - val_loss: 1.1779 - val_accuracy: 0.5170 Epoch 3/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1874 - accuracy: 0.5119 - val_loss: 1.1815 - val_accuracy: 0.5165 Epoch 4/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1766 - accuracy: 0.5186 - val_loss: 1.1692 - val_accuracy: 0.5212 Epoch 5/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1630 - accuracy: 0.5203 - val_loss: 1.1670 - val_accuracy: 0.5247 Epoch 6/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1538 - accuracy: 0.5250 - val_loss: 1.1682 - val_accuracy: 0.5216 Epoch 7/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1473 - accuracy: 0.5275 - val_loss: 1.1663 - val_accuracy: 0.5242 Epoch 8/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1396 - accuracy: 0.5300 - val_loss: 1.1675 - val_accuracy: 0.5204 Epoch 9/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1312 - accuracy: 0.5339 - val_loss: 1.1672 - val_accuracy: 0.5237 Epoch 10/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1266 - accuracy: 0.5357 - val_loss: 1.1663 - val_accuracy: 0.5231 Epoch 11/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1186 - accuracy: 0.5383 - val_loss: 1.1713 - val_accuracy: 0.5235 Epoch 12/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1114 - accuracy: 0.5422 - val_loss: 1.1750 - val_accuracy: 0.5208 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1013 - accuracy: 0.5446 - val_loss: 1.1702 - val_accuracy: 0.5206 Epoch 14/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0930 - accuracy: 0.5503 - val_loss: 1.1706 - val_accuracy: 0.5206 Epoch 15/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0846 - accuracy: 0.5520 - val_loss: 1.1801 - val_accuracy: 0.5181 Epoch 16/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0779 - accuracy: 0.5547 - val_loss: 1.1785 - val_accuracy: 0.5214 Epoch 17/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0696 - accuracy: 0.5565 - val_loss: 1.1873 - val_accuracy: 0.5112 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0579 - accuracy: 0.5625 - val_loss: 1.1875 - val_accuracy: 0.5191 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0497 - accuracy: 0.5654 - val_loss: 1.1907 - val_accuracy: 0.5200 Epoch 20/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0421 - accuracy: 0.5694 - val_loss: 1.1976 - val_accuracy: 0.5152 Epoch 21/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0335 - accuracy: 0.5727 - val_loss: 1.1989 - val_accuracy: 0.5132 Epoch 22/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0248 - accuracy: 0.5777 - val_loss: 1.1998 - val_accuracy: 0.5153 Epoch 23/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0148 - accuracy: 0.5830 - val_loss: 1.2058 - val_accuracy: 0.5125 Epoch 24/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0064 - accuracy: 0.5848 - val_loss: 1.2153 - val_accuracy: 0.5112 Epoch 25/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0071 - accuracy: 0.5852 - val_loss: 1.2159 - val_accuracy: 0.5074 . model5 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model5.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model5.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 13us/sample - loss: 1.2930 - accuracy: 0.4740 - val_loss: 1.1976 - val_accuracy: 0.5134 Epoch 2/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.2040 - accuracy: 0.5046 - val_loss: 1.1800 - val_accuracy: 0.5182 Epoch 3/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1853 - accuracy: 0.5137 - val_loss: 1.1729 - val_accuracy: 0.5201 Epoch 4/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1721 - accuracy: 0.5186 - val_loss: 1.1771 - val_accuracy: 0.5190 Epoch 5/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1631 - accuracy: 0.5195 - val_loss: 1.1690 - val_accuracy: 0.5244 Epoch 6/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1563 - accuracy: 0.5236 - val_loss: 1.1703 - val_accuracy: 0.5247 Epoch 7/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1507 - accuracy: 0.5268 - val_loss: 1.1635 - val_accuracy: 0.5251 Epoch 8/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1403 - accuracy: 0.5305 - val_loss: 1.1666 - val_accuracy: 0.5208 Epoch 9/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1331 - accuracy: 0.5331 - val_loss: 1.1676 - val_accuracy: 0.5223 Epoch 10/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1230 - accuracy: 0.5363 - val_loss: 1.1728 - val_accuracy: 0.5177 Epoch 11/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1192 - accuracy: 0.5374 - val_loss: 1.1720 - val_accuracy: 0.5211 Epoch 12/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1171 - accuracy: 0.5386 - val_loss: 1.1727 - val_accuracy: 0.5219 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1050 - accuracy: 0.5420 - val_loss: 1.1697 - val_accuracy: 0.5249 Epoch 14/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0959 - accuracy: 0.5462 - val_loss: 1.1731 - val_accuracy: 0.5203 Epoch 15/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0887 - accuracy: 0.5499 - val_loss: 1.1766 - val_accuracy: 0.5233 Epoch 16/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0786 - accuracy: 0.5535 - val_loss: 1.1817 - val_accuracy: 0.5207 Epoch 17/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0806 - accuracy: 0.5548 - val_loss: 1.1892 - val_accuracy: 0.5153 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0698 - accuracy: 0.5584 - val_loss: 1.1856 - val_accuracy: 0.5199 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0542 - accuracy: 0.5616 - val_loss: 1.1866 - val_accuracy: 0.5206 Epoch 20/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0485 - accuracy: 0.5668 - val_loss: 1.1923 - val_accuracy: 0.5160 Epoch 21/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0379 - accuracy: 0.5697 - val_loss: 1.1964 - val_accuracy: 0.5183 Epoch 22/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0314 - accuracy: 0.5734 - val_loss: 1.2004 - val_accuracy: 0.5174 . model6 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model6.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model6.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 14us/sample - loss: 1.2860 - accuracy: 0.4771 - val_loss: 1.1983 - val_accuracy: 0.5124 Epoch 2/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.2021 - accuracy: 0.5062 - val_loss: 1.1923 - val_accuracy: 0.5175 Epoch 3/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1859 - accuracy: 0.5118 - val_loss: 1.1751 - val_accuracy: 0.5204 Epoch 4/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1713 - accuracy: 0.5184 - val_loss: 1.1688 - val_accuracy: 0.5214 Epoch 5/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1660 - accuracy: 0.5202 - val_loss: 1.1671 - val_accuracy: 0.5232 Epoch 6/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1557 - accuracy: 0.5253 - val_loss: 1.1602 - val_accuracy: 0.5221 Epoch 7/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1473 - accuracy: 0.5271 - val_loss: 1.1706 - val_accuracy: 0.5229 Epoch 8/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1395 - accuracy: 0.5309 - val_loss: 1.1674 - val_accuracy: 0.5188 Epoch 9/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1376 - accuracy: 0.5309 - val_loss: 1.1697 - val_accuracy: 0.5215 Epoch 10/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1259 - accuracy: 0.5347 - val_loss: 1.1650 - val_accuracy: 0.5224 Epoch 11/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1203 - accuracy: 0.5387 - val_loss: 1.1711 - val_accuracy: 0.5225 Epoch 12/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1100 - accuracy: 0.5409 - val_loss: 1.1714 - val_accuracy: 0.5224 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1002 - accuracy: 0.5456 - val_loss: 1.1767 - val_accuracy: 0.5248 Epoch 14/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0932 - accuracy: 0.5472 - val_loss: 1.1780 - val_accuracy: 0.5170 Epoch 15/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0930 - accuracy: 0.5481 - val_loss: 1.1783 - val_accuracy: 0.5206 Epoch 16/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0795 - accuracy: 0.5529 - val_loss: 1.1865 - val_accuracy: 0.5171 Epoch 17/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0686 - accuracy: 0.5576 - val_loss: 1.1858 - val_accuracy: 0.5170 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0589 - accuracy: 0.5628 - val_loss: 1.1871 - val_accuracy: 0.5176 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0502 - accuracy: 0.5652 - val_loss: 1.1943 - val_accuracy: 0.5133 Epoch 20/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0434 - accuracy: 0.5677 - val_loss: 1.1954 - val_accuracy: 0.5207 Epoch 21/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0350 - accuracy: 0.5709 - val_loss: 1.2038 - val_accuracy: 0.5110 . model7 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model7.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model7.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 14us/sample - loss: 1.2899 - accuracy: 0.4717 - val_loss: 1.1929 - val_accuracy: 0.5090 Epoch 2/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.2027 - accuracy: 0.5056 - val_loss: 1.1800 - val_accuracy: 0.5172 Epoch 3/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1840 - accuracy: 0.5141 - val_loss: 1.1750 - val_accuracy: 0.5214 Epoch 4/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1723 - accuracy: 0.5174 - val_loss: 1.1762 - val_accuracy: 0.5196 Epoch 5/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1613 - accuracy: 0.5229 - val_loss: 1.1698 - val_accuracy: 0.5234 Epoch 6/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1548 - accuracy: 0.5259 - val_loss: 1.1739 - val_accuracy: 0.5193 Epoch 7/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1455 - accuracy: 0.5266 - val_loss: 1.1678 - val_accuracy: 0.5217 Epoch 8/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1393 - accuracy: 0.5294 - val_loss: 1.1656 - val_accuracy: 0.5208 Epoch 9/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1313 - accuracy: 0.5330 - val_loss: 1.1665 - val_accuracy: 0.5224 Epoch 10/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1235 - accuracy: 0.5358 - val_loss: 1.1748 - val_accuracy: 0.5201 Epoch 11/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1161 - accuracy: 0.5399 - val_loss: 1.1681 - val_accuracy: 0.5221 Epoch 12/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1091 - accuracy: 0.5411 - val_loss: 1.1781 - val_accuracy: 0.5160 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1024 - accuracy: 0.5455 - val_loss: 1.1723 - val_accuracy: 0.5224 Epoch 14/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0929 - accuracy: 0.5491 - val_loss: 1.1795 - val_accuracy: 0.5196 Epoch 15/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0837 - accuracy: 0.5517 - val_loss: 1.1800 - val_accuracy: 0.5174 Epoch 16/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0739 - accuracy: 0.5544 - val_loss: 1.1837 - val_accuracy: 0.5157 Epoch 17/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0686 - accuracy: 0.5574 - val_loss: 1.1911 - val_accuracy: 0.5174 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0572 - accuracy: 0.5621 - val_loss: 1.1934 - val_accuracy: 0.5146 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0492 - accuracy: 0.5667 - val_loss: 1.1964 - val_accuracy: 0.5127 Epoch 20/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0404 - accuracy: 0.5687 - val_loss: 1.1958 - val_accuracy: 0.5181 Epoch 21/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0308 - accuracy: 0.5747 - val_loss: 1.2008 - val_accuracy: 0.5140 Epoch 22/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0209 - accuracy: 0.5799 - val_loss: 1.2083 - val_accuracy: 0.5166 Epoch 23/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0167 - accuracy: 0.5801 - val_loss: 1.2089 - val_accuracy: 0.5119 . model8 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model8.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model8.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 14us/sample - loss: 1.2908 - accuracy: 0.4766 - val_loss: 1.1975 - val_accuracy: 0.5047 Epoch 2/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.2042 - accuracy: 0.5065 - val_loss: 1.1811 - val_accuracy: 0.5183 Epoch 3/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1839 - accuracy: 0.5130 - val_loss: 1.1677 - val_accuracy: 0.5185 Epoch 4/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1734 - accuracy: 0.5169 - val_loss: 1.1724 - val_accuracy: 0.5209 Epoch 5/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1649 - accuracy: 0.5195 - val_loss: 1.1728 - val_accuracy: 0.5232 Epoch 6/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1558 - accuracy: 0.5231 - val_loss: 1.1637 - val_accuracy: 0.5260 Epoch 7/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1482 - accuracy: 0.5286 - val_loss: 1.1695 - val_accuracy: 0.5235 Epoch 8/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1399 - accuracy: 0.5296 - val_loss: 1.1628 - val_accuracy: 0.5224 Epoch 9/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1327 - accuracy: 0.5322 - val_loss: 1.1750 - val_accuracy: 0.5127 Epoch 10/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1247 - accuracy: 0.5349 - val_loss: 1.1735 - val_accuracy: 0.5205 Epoch 11/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1175 - accuracy: 0.5389 - val_loss: 1.1686 - val_accuracy: 0.5248 Epoch 12/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1107 - accuracy: 0.5418 - val_loss: 1.1730 - val_accuracy: 0.5185 Epoch 13/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1035 - accuracy: 0.5439 - val_loss: 1.1769 - val_accuracy: 0.5096 Epoch 14/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0947 - accuracy: 0.5478 - val_loss: 1.1767 - val_accuracy: 0.5184 Epoch 15/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0897 - accuracy: 0.5495 - val_loss: 1.1793 - val_accuracy: 0.5207 Epoch 16/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0793 - accuracy: 0.5536 - val_loss: 1.1799 - val_accuracy: 0.5187 Epoch 17/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0668 - accuracy: 0.5591 - val_loss: 1.1837 - val_accuracy: 0.5190 Epoch 18/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0584 - accuracy: 0.5622 - val_loss: 1.1871 - val_accuracy: 0.5213 Epoch 19/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0512 - accuracy: 0.5643 - val_loss: 1.1939 - val_accuracy: 0.5163 Epoch 20/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0417 - accuracy: 0.5699 - val_loss: 1.1928 - val_accuracy: 0.5194 Epoch 21/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0342 - accuracy: 0.5737 - val_loss: 1.2008 - val_accuracy: 0.5174 Epoch 22/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0234 - accuracy: 0.5777 - val_loss: 1.2110 - val_accuracy: 0.5142 Epoch 23/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0175 - accuracy: 0.5799 - val_loss: 1.2115 - val_accuracy: 0.5156 . model9 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model9.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model9.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 13us/sample - loss: 1.2875 - accuracy: 0.4755 - val_loss: 1.1945 - val_accuracy: 0.5144 Epoch 2/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.2027 - accuracy: 0.5052 - val_loss: 1.1797 - val_accuracy: 0.5165 Epoch 3/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1842 - accuracy: 0.5137 - val_loss: 1.1764 - val_accuracy: 0.5203 Epoch 4/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1724 - accuracy: 0.5181 - val_loss: 1.1704 - val_accuracy: 0.5241 Epoch 5/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1661 - accuracy: 0.5199 - val_loss: 1.1699 - val_accuracy: 0.5212 Epoch 6/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1567 - accuracy: 0.5242 - val_loss: 1.1631 - val_accuracy: 0.5213 Epoch 7/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1517 - accuracy: 0.5276 - val_loss: 1.1663 - val_accuracy: 0.5245 Epoch 8/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1416 - accuracy: 0.5274 - val_loss: 1.1630 - val_accuracy: 0.5251 Epoch 9/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1335 - accuracy: 0.5320 - val_loss: 1.1681 - val_accuracy: 0.5204 Epoch 10/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1273 - accuracy: 0.5350 - val_loss: 1.1664 - val_accuracy: 0.5211 Epoch 11/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1158 - accuracy: 0.5380 - val_loss: 1.1703 - val_accuracy: 0.5230 Epoch 12/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1097 - accuracy: 0.5409 - val_loss: 1.1713 - val_accuracy: 0.5218 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1031 - accuracy: 0.5430 - val_loss: 1.1724 - val_accuracy: 0.5214 Epoch 14/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0940 - accuracy: 0.5475 - val_loss: 1.1744 - val_accuracy: 0.5197 Epoch 15/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0854 - accuracy: 0.5513 - val_loss: 1.1804 - val_accuracy: 0.5191 Epoch 16/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0781 - accuracy: 0.5543 - val_loss: 1.1814 - val_accuracy: 0.5195 Epoch 17/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0726 - accuracy: 0.5570 - val_loss: 1.1851 - val_accuracy: 0.5197 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0636 - accuracy: 0.5588 - val_loss: 1.1912 - val_accuracy: 0.5189 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0512 - accuracy: 0.5653 - val_loss: 1.1921 - val_accuracy: 0.5203 Epoch 20/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0437 - accuracy: 0.5669 - val_loss: 1.1958 - val_accuracy: 0.5175 Epoch 21/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0358 - accuracy: 0.5716 - val_loss: 1.1995 - val_accuracy: 0.5165 Epoch 22/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0263 - accuracy: 0.5764 - val_loss: 1.1990 - val_accuracy: 0.5162 Epoch 23/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0182 - accuracy: 0.5780 - val_loss: 1.2084 - val_accuracy: 0.5071 . model10 = tf.keras.models.Sequential([ tf.keras.layers.Dense(512, input_shape=[1175,]), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(512), tf.keras.layers.ReLU(), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(6, activation=&quot;softmax&quot;) ]) model10.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&#39;adam&#39;, metrics=[&quot;accuracy&quot;]) history = model10.fit(X_train_scaled[:, :-557], y_train, batch_size=512, epochs=1000, callbacks=[early_stop], validation_data=(X_valid_scaled[:, :-557], y_valid)) . Train on 120000 samples, validate on 30000 samples Epoch 1/1000 120000/120000 [==============================] - 2s 13us/sample - loss: 1.2838 - accuracy: 0.4745 - val_loss: 1.1907 - val_accuracy: 0.5142 Epoch 2/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.2023 - accuracy: 0.5058 - val_loss: 1.1769 - val_accuracy: 0.5170 Epoch 3/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1862 - accuracy: 0.5131 - val_loss: 1.1721 - val_accuracy: 0.5176 Epoch 4/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1715 - accuracy: 0.5184 - val_loss: 1.1686 - val_accuracy: 0.5201 Epoch 5/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1621 - accuracy: 0.5195 - val_loss: 1.1705 - val_accuracy: 0.5208 Epoch 6/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1539 - accuracy: 0.5249 - val_loss: 1.1696 - val_accuracy: 0.5246 Epoch 7/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.1470 - accuracy: 0.5261 - val_loss: 1.1696 - val_accuracy: 0.5193 Epoch 8/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1382 - accuracy: 0.5307 - val_loss: 1.1715 - val_accuracy: 0.5221 Epoch 9/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1349 - accuracy: 0.5309 - val_loss: 1.1713 - val_accuracy: 0.5199 Epoch 10/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1246 - accuracy: 0.5346 - val_loss: 1.1696 - val_accuracy: 0.5232 Epoch 11/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1153 - accuracy: 0.5385 - val_loss: 1.1677 - val_accuracy: 0.5244 Epoch 12/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1089 - accuracy: 0.5404 - val_loss: 1.1711 - val_accuracy: 0.5219 Epoch 13/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.1016 - accuracy: 0.5443 - val_loss: 1.1720 - val_accuracy: 0.5249 Epoch 14/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0937 - accuracy: 0.5474 - val_loss: 1.1824 - val_accuracy: 0.5193 Epoch 15/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0828 - accuracy: 0.5512 - val_loss: 1.1799 - val_accuracy: 0.5218 Epoch 16/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0754 - accuracy: 0.5535 - val_loss: 1.1849 - val_accuracy: 0.5214 Epoch 17/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0659 - accuracy: 0.5568 - val_loss: 1.1852 - val_accuracy: 0.5185 Epoch 18/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0600 - accuracy: 0.5611 - val_loss: 1.1873 - val_accuracy: 0.5173 Epoch 19/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0513 - accuracy: 0.5650 - val_loss: 1.1919 - val_accuracy: 0.5188 Epoch 20/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0414 - accuracy: 0.5688 - val_loss: 1.1965 - val_accuracy: 0.5144 Epoch 21/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0318 - accuracy: 0.5723 - val_loss: 1.2072 - val_accuracy: 0.5132 Epoch 22/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0246 - accuracy: 0.5766 - val_loss: 1.2045 - val_accuracy: 0.5159 Epoch 23/1000 120000/120000 [==============================] - 1s 9us/sample - loss: 1.0138 - accuracy: 0.5798 - val_loss: 1.2109 - val_accuracy: 0.5128 Epoch 24/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 1.0076 - accuracy: 0.5838 - val_loss: 1.2152 - val_accuracy: 0.5164 Epoch 25/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 0.9975 - accuracy: 0.5874 - val_loss: 1.2164 - val_accuracy: 0.5154 Epoch 26/1000 120000/120000 [==============================] - 1s 10us/sample - loss: 0.9899 - accuracy: 0.5902 - val_loss: 1.2231 - val_accuracy: 0.5160 . pred1 = model1.predict(X_valid_scaled[:, :-557]) pred2 = model2.predict(X_valid_scaled[:, :-557]) pred3 = model3.predict(X_valid_scaled[:, :-557]) pred4 = model4.predict(X_valid_scaled[:, :-557]) pred5 = model5.predict(X_valid_scaled[:, :-557]) pred6 = model6.predict(X_valid_scaled[:, :-557]) pred7 = model7.predict(X_valid_scaled[:, :-557]) pred8 = model8.predict(X_valid_scaled[:, :-557]) pred9 = model9.predict(X_valid_scaled[:, :-557]) pred10 = model10.predict(X_valid_scaled[:, :-557]) . pred_mlp = (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10) / 10 log_loss(y_valid, pred_mlp) # 1.1524238256888075 . 1.1524238256888075 . import lightgbm as lgb . bagging_fraction = 0.7 # 0.7 feature_fraction = 0.9 # 0.9 n_model = 10 # 10 . model_dict = {} for i in range(n_model): print(&#39;Training %d model&#39;%(i+1)) d_train = lgb.Dataset(X_train_scaled, label=y_train) params = {} params[&#39;learning_rate&#39;] = 0.1 params[&#39;boosting_type&#39;] = &#39;gbdt&#39; params[&#39;objective&#39;] = &#39;multiclass&#39; params[&#39;num_classes&#39;] = 6 params[&#39;metric&#39;] = &#39;multi_logloss&#39; params[&#39;bagging_fraction&#39;] = bagging_fraction params[&#39;bagging_freq&#39;] = 1 params[&#39;bagging_seed&#39;] = i+1 params[&#39;feature_fraction&#39;] = feature_fraction params[&#39;feature_fraction_seed&#39;] = i+1 params[&#39;max_depth&#39;] = 10 params[&#39;num_leaves&#39;] = 32 params[&#39;lambda_l2&#39;] = 24 #params[&#39;max_bin&#39;] = 64 clf = lgb.train(params, d_train, 100) model_dict[&#39;model_&#39;+str(i)] = clf # 1.1645216634935185 . Training 1 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.835240 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 2 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.719457 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 3 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.793521 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 4 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.748699 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 5 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.689484 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 6 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.815211 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 7 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.769633 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 8 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.716562 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 9 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.788493 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 Training 10 model [LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.829176 seconds. You can set `force_col_wise=true` to remove the overhead. [LightGBM] [Info] Total Bins 438357 [LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 1732 [LightGBM] [Info] Start training from score -2.135518 [LightGBM] [Info] Start training from score -0.918084 [LightGBM] [Info] Start training from score -1.060618 [LightGBM] [Info] Start training from score -4.059943 [LightGBM] [Info] Start training from score -2.937149 [LightGBM] [Info] Start training from score -2.717722 . pred_lgb = np.zeros((len(y_valid), 6)) for i in range(n_model): pred_lgb += model_dict[&#39;model_&#39;+str(i)].predict(X_valid_scaled) pred_lgb = pred_lgb / n_model log_loss(y_valid, pred_lgb) # 1.1590863715913093 . 1.1590863715913093 . pred = (pred_mlp + pred_lgb) / 2 . pred . array([[3.57072546e-03, 5.57289383e-02, 8.97353582e-01, 2.22444551e-04, 1.85221898e-03, 4.12720756e-02], [6.92599978e-02, 2.74608884e-01, 6.21422029e-01, 1.17425804e-03, 7.18916246e-03, 2.63456698e-02], [2.28711285e-01, 2.70365329e-01, 1.99727329e-01, 1.52601798e-01, 8.85579810e-02, 6.00362813e-02], ..., [6.39873077e-02, 3.29615641e-01, 4.69737381e-01, 2.69650596e-03, 3.71115569e-02, 9.68516209e-02], [1.86162125e-01, 3.25978530e-01, 3.96424325e-01, 1.27466881e-02, 3.73194014e-02, 4.13689147e-02], [5.89294910e-01, 3.01848296e-01, 8.59463698e-02, 7.33122963e-03, 7.73586397e-03, 7.84330762e-03]]) . log_loss(y_valid, pred) # 1.147216886499985 . 1.147216886499985 . test . clnt_id = df_test[&#39;CLNT_ID&#39;].unique() . df_test[&#39;PD_BUY_AM&#39;] = df_test[&#39;PD_BUY_AM&#39;].astype(&#39;string&#39;) df_test[&#39;PD_BUY_CT&#39;] = df_test[&#39;PD_BUY_CT&#39;].astype(&#39;string&#39;) df_test[&#39;TOT_SESS_HR_V&#39;] = df_test[&#39;TOT_SESS_HR_V&#39;].astype(&#39;string&#39;) df_test[&#39;PD_BUY_AM&#39;] = df_test[&#39;PD_BUY_AM&#39;].map(lambda x: x.replace(&#39;,&#39;, &#39;&#39;)) df_test[&#39;PD_BUY_CT&#39;] = df_test[&#39;PD_BUY_CT&#39;].map(lambda x: x.replace(&#39;,&#39;, &#39;&#39;)) df_test[&#39;TOT_SESS_HR_V&#39;] = df_test[&#39;TOT_SESS_HR_V&#39;].map(lambda x: x.replace(&#39;,&#39;, &#39;&#39;)) . df_test[&#39;PD_BUY_AM&#39;] = df_test[&#39;PD_BUY_AM&#39;].astype(&#39;int&#39;) df_test[&#39;PD_BUY_CT&#39;] = df_test[&#39;PD_BUY_CT&#39;].astype(&#39;int&#39;) df_test[&#39;TOT_SESS_HR_V&#39;] = df_test[&#39;TOT_SESS_HR_V&#39;].astype(&#39;int&#39;) . df_test[&#39;SESS_DT&#39;] = df_test[&#39;SESS_DT&#39;].map(lambda x: date.fromisoformat(str(x)[:4] + &#39;-&#39; + str(x)[4:6] + &#39;-&#39; + str(x)[6:])) . num_shoppings = [] avg_prices = [] total_prices = [] avg_cts = [] total_cts = [] avg_sess_views = [] total_sess_views = [] avg_sess_hrs = [] total_sess_hrs = [] avg_shopping_intervals = [] main_devices = [] pd_cs = [] clac1_nms = [] clac2_nms = [] clac3_nms = [] for i in tqdm(range(len(clnt_id))): temp_df_test = df_test[df_test[&#39;CLNT_ID&#39;] == clnt_id[i]] temp_df_test = temp_df_test.sort_values(by=[&#39;SESS_DT&#39;, &#39;HITS_SEQ&#39;, &#39;PD_C&#39;]) temp_df_test = temp_df_test[~temp_df_test.duplicated(subset=[&#39;SESS_ID&#39;, &#39;HITS_SEQ&#39;, &#39;PD_C&#39;], keep=&#39;last&#39;)] num_shopping = len(temp_df_test) avg_price, total_price, avg_ct, total_ct = calc_avg_total_price_ct(temp_df_test) avg_sess_view = temp_df_test[&#39;TOT_PAG_VIEW_CT&#39;].values.mean() total_sess_view = temp_df_test[&#39;TOT_PAG_VIEW_CT&#39;].values.sum() avg_sess_hr = temp_df_test[&#39;TOT_SESS_HR_V&#39;].values.mean() total_sess_hr = temp_df_test[&#39;TOT_SESS_HR_V&#39;].values.sum() avg_shopping_interval = calc_avg_shopping_interval(temp_df_test) main_device = scipy.stats.mode(temp_df_test[&#39;DVC_CTG_NM&#39;].values).mode[0] pd_c = temp_df_test[&#39;PD_C&#39;].values clac1_nm = temp_df_test[&#39;CLAC1_NM&#39;].values clac2_nm = temp_df_test[&#39;CLAC2_NM&#39;].values clac3_nm = temp_df_test[&#39;CLAC3_NM&#39;].values num_shoppings.append(num_shopping) avg_prices.append(avg_price) total_prices.append(total_price) avg_cts.append(avg_ct) total_cts.append(total_ct) avg_sess_views.append(avg_sess_view) total_sess_views.append(total_sess_view) avg_sess_hrs.append(avg_sess_hr) total_sess_hrs.append(total_sess_hr) avg_shopping_intervals.append(avg_shopping_interval) main_devices.append(main_device) pd_cs.append(pd_c) clac1_nms.append(clac1_nm) clac2_nms.append(clac2_nm) clac3_nms.append(clac3_nm) . 100%|█████████████████████████████████████████████████████████████████████████| 113104/113104 [13:29&lt;00:00, 139.73it/s] . data_test = pd.DataFrame([clnt_id, num_shoppings, avg_prices, total_prices, avg_cts, total_cts, avg_sess_views, total_sess_views, avg_sess_hrs, total_sess_hrs, avg_shopping_intervals, main_devices, pd_cs, clac1_nms, clac2_nms, clac3_nms]).T . data_test.columns = [&#39;clnt_id&#39;, &#39;num_shopping&#39;, &#39;avg_price&#39;, &#39;total_price&#39;, &#39;avg_ct&#39;, &#39;total_ct&#39;, &#39;avg_sess_view&#39;, &#39;total_sess_view&#39;, &#39;avg_sess_hr&#39;, &#39;total_sess_hr&#39;, &#39;avg_shopping_interval&#39;, &#39;main_device&#39;, &#39;pd_c&#39;, &#39;clac1_nm&#39;, &#39;clac2_nm&#39;, &#39;clac3_nm&#39;] . data_test . clnt_id num_shopping avg_price total_price avg_ct total_ct avg_sess_view total_sess_view avg_sess_hr total_sess_hr avg_shopping_interval main_device pd_c clac1_nm clac2_nm clac3_nm . 0 2 | 3 | 32000 | 128000 | 1.33333 | 4 | 211.333 | 634 | 3315 | 9945 | 40.5 | mobile | [234664, 325761, 752670] | [스포츠패션, 스포츠패션, 출산/육아용품] | [유아동스포츠화, 유아동스포츠화, 유아스킨/바디케어] | [유아동런닝/트레이닝화, 유아동스포츠샌들/슬리퍼, 유아용화장품] | . 1 3 | 2 | 106400 | 212800 | 1 | 2 | 93 | 186 | 1051 | 2102 | 0 | mobile | [567605, 556431] | [패션잡화, 패션잡화] | [여성지갑, 여성지갑] | [여성일반지갑, 여성일반지갑] | . 2 10 | 3 | 123000 | 369000 | 1 | 3 | 134.667 | 404 | 1745.67 | 5237 | 9.5 | mobile | [269142, 269142, 797432] | [남성의류, 남성의류, 패션잡화] | [남성의류하의, 남성의류하의, 남성가방] | [남성캐주얼바지, 남성캐주얼바지, 남성서류가방] | . 3 15 | 17 | 17587.9 | 334170 | 1.11765 | 19 | 105.412 | 1792 | 1356.71 | 23064 | 7.25 | mobile | [738180, 514099, 566303, 843952, 843603, 76426... | [모바일, 여성의류, 식기/조리기구, 화장품/뷰티케어, 출산/육아용품, 문구/사무용... | [모바일액세서리, 여성의류상의, 밀폐/보관용기, 메이크업, 유아위생용품, 일반문구/... | [기타모바일액세서리, 여성티셔츠/탑, 보온병/텀블러, BB/파운데이션/컴팩트류, 유... | . 4 29 | 2 | 63000 | 126000 | 1 | 2 | 69 | 138 | 2120 | 4240 | 0 | mobile | [674855, 674855] | [화장품/뷰티케어, 화장품/뷰티케어] | [메이크업, 메이크업] | [BB/파운데이션/컴팩트류, BB/파운데이션/컴팩트류] | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 113099 263089 | 3 | 68633.3 | 205900 | 1 | 3 | 63.3333 | 190 | 1700.33 | 5101 | 6 | mobile | [50185, 760443, 9617] | [화장품/뷰티케어, 화장품/뷰티케어, 원예/애완] | [스킨케어, 메이크업, 고양이용품] | [스킨케어세트, BB/파운데이션/컴팩트류, 고양이사료] | . 113100 263097 | 4 | 60610 | 242440 | 1 | 4 | 469.75 | 1879 | 3422 | 13688 | 17 | mobile | [737465, 537882, 755981, 301542] | [화장품/뷰티케어, 구기/필드스포츠, 건강식품, 스포츠패션] | [스킨케어, 골프, 홍삼/인삼가공식품, 여성일반스포츠의류] | [스킨케어세트, 골프패션잡화, 홍삼/인삼혼합세트, 여성스포츠티셔츠/탑] | . 113101 263098 | 1 | 100200 | 100200 | 1 | 1 | 136 | 136 | 1175 | 1175 | 183 | mobile | [593872] | [남성의류] | [남성의류상의] | [남성남방셔츠] | . 113102 263099 | 4 | 12250 | 49000 | 1 | 4 | 274 | 1096 | 3890 | 15560 | 0 | mobile | [193132, 470807, 665269, 692330] | [남성의류, 남성의류, 남성의류, 남성의류] | [남성의류하의, 남성의류상의, 남성의류하의, 남성의류상의] | [남성캐주얼바지, 남성티셔츠, 남성캐주얼바지, 남성티셔츠] | . 113103 263100 | 2 | 11200 | 22400 | 1 | 2 | 104 | 208 | 1474.5 | 2949 | 25 | mobile | [310231, 678538] | [여성의류, 남성의류] | [여성의류하의, 남성의류상의] | [여성바지, 남성티셔츠] | . 113104 rows × 16 columns . for nm in df_test[&#39;CLAC3_NM&#39;].unique(): if nm not in clac3_nm_dic.keys(): clac3_nm_dic[nm] = len(clac3_nm_dic.keys()) . clac3_nm_dic . {&#39;블랜더&#39;: 0, &#39;거실수예소품&#39;: 1, &#39;남성수영복&#39;: 2, &#39;페이셜클렌저&#39;: 3, &#39;선크림류&#39;: 4, &#39;여성비치웨어&#39;: 5, &#39;홍삼액&#39;: 6, &#39;BB/파운데이션/컴팩트류&#39;: 7, &#39;에센스/세럼&#39;: 8, &#39;여성속옷세트&#39;: 9, &#39;브래지어&#39;: 10, &#39;여성팬티&#39;: 11, &#39;여성스포츠샌들/슬리퍼&#39;: 12, &#39;유아동팬티&#39;: 13, &#39;유아동일반양말&#39;: 14, &#39;여성샌들&#39;: 15, &#39;유아동타이즈&#39;: 16, &#39;주방칼/가위&#39;: 17, &#39;영유아점프수트/오버롤&#39;: 18, &#39;아동모&#39;: 19, &#39;프라이팬&#39;: 20, &#39;롤티슈&#39;: 21, &#39;플라스틱서랍장&#39;: 22, &#39;유아용화장품&#39;: 23, &#39;일반형냉장고&#39;: 24, &#39;핸드로션/크림&#39;: 25, &#39;남성스포츠티셔츠&#39;: 26, &#39;크림/밤/오일&#39;: 27, &#39;골프공&#39;: 28, &#39;골프연습장비&#39;: 29, &#39;립글로즈/틴트&#39;: 30, &#39;남성일반지갑&#39;: 31, &#39;남성런닝/트레이닝화&#39;: 32, &#39;애견주거/실내용품&#39;: 33, &#39;생수&#39;: 34, &#39;여성트레이닝복&#39;: 35, &#39;여성런닝셔츠/캐미솔&#39;: 36, &#39;채소즙&#39;: 37, &#39;남성용스킨케어류&#39;: 38, &#39;3단우산&#39;: 39, &#39;스포츠가방&#39;: 40, &#39;남성팬티&#39;: 41, &#39;여성남방셔츠&#39;: 42, &#39;여성원피스&#39;: 43, &#39;유아동이불/이불커버&#39;: 44, &#39;남성티셔츠&#39;: 45, &#39;남성캐주얼바지&#39;: 46, &#39;여성코트&#39;: 47, &#39;남성청바지&#39;: 48, &#39;기타여성속옷&#39;: 49, &#39;기타냉방가전&#39;: 50, &#39;영유아티셔츠/탑&#39;: 51, &#39;여아티셔츠/탑&#39;: 52, &#39;생리대&#39;: 53, &#39;유아용기저귀&#39;: 54, &#39;유산균/프로바이오틱스&#39;: 55, &#39;김치류&#39;: 56, &#39;남성등산바지&#39;: 57, &#39;유아동스포츠샌들/슬리퍼&#39;: 58, &#39;골프패션잡화&#39;: 59, &#39;염모제&#39;: 60, &#39;장우산&#39;: 61, &#39;골프필드용품&#39;: 62, &#39;텐트&#39;: 63, &#39;기타에어컨&#39;: 64, &#39;여성스니커즈&#39;: 65, &#39;남성정장셔츠&#39;: 66, &#39;토스터/제빵기&#39;: 67, &#39;선풍기&#39;: 68, &#39;유아동런닝셔츠&#39;: 69, &#39;배낭&#39;: 70, &#39;여성크로스백&#39;: 71, &#39;남성런닝셔츠&#39;: 72, &#39;남성남방셔츠&#39;: 73, &#39;여성스웨터/풀오버&#39;: 74, &#39;영유아스커트&#39;: 75, &#39;여아가디건&#39;: 76, &#39;아동수영복&#39;: 77, &#39;미스트&#39;: 78, &#39;스킨/토너&#39;: 79, &#39;애견장난감/훈련&#39;: 80, &#39;고양이캣타워/실내용품&#39;: 81, &#39;기타구강관리용품&#39;: 82, &#39;남성등산티셔츠&#39;: 83, &#39;남성점퍼&#39;: 84, &#39;팬티라이너&#39;: 85, &#39;여성로퍼&#39;: 86, &#39;여성티셔츠/탑&#39;: 87, &#39;냉동국탕류&#39;: 88, &#39;서랍장/수납장&#39;: 89, &#39;책상의자&#39;: 90, &#39;스킨케어세트&#39;: 91, &#39;페이셜팩류&#39;: 92, &#39;출산/신생아용품세트&#39;: 93, &#39;아기띠/캐리어&#39;: 94, &#39;여성스포츠티셔츠/탑&#39;: 95, &#39;유아동샌들&#39;: 96, &#39;여성바지&#39;: 97, &#39;영유아원피스&#39;: 98, &#39;남성정장바지&#39;: 99, &#39;남성일반스포츠바지&#39;: 100, &#39;미술/창작완구&#39;: 101, &#39;기타요가/필라테스소품&#39;: 102, &#39;숟가락/젓가락&#39;: 103, &#39;아쿠아슈즈&#39;: 104, &#39;여성재킷&#39;: 105, &#39;영유아바지&#39;: 106, &#39;남성골프바지&#39;: 107, &#39;유아동침구세트&#39;: 108, &#39;유아/아동용치약&#39;: 109, &#39;스포츠모자&#39;: 110, &#39;유아동슬리퍼&#39;: 111, &#39;여성골프패딩&#39;: 112, &#39;여성숄더백&#39;: 113, &#39;야구모자&#39;: 114, &#39;유아동스니커즈&#39;: 115, &#39;헤어에센스&#39;: 116, &#39;탁자&#39;: 117, &#39;닭가슴살&#39;: 118, &#39;유아동내의&#39;: 119, &#39;남아티셔츠/탑&#39;: 120, &#39;오리발/스노클링&#39;: 121, &#39;남아잠옷&#39;: 122, &#39;식탁의자&#39;: 123, &#39;유아용물티슈&#39;: 124, &#39;애견간식&#39;: 125, &#39;여성플랫&#39;: 126, &#39;애견사료&#39;: 127, &#39;반찬통/밀폐용기&#39;: 128, &#39;어린이홍삼&#39;: 129, &#39;스포츠시계&#39;: 130, &#39;고양이모래/배변용품&#39;: 131, &#39;남성스포츠샌들/슬리퍼&#39;: 132, &#39;혼합즙&#39;: 133, &#39;아이브로우&#39;: 134, &#39;아이케어&#39;: 135, &#39;바디워시&#39;: 136, &#39;유아용샴푸/바디워시&#39;: 137, &#39;샴푸&#39;: 138, &#39;남성가디건&#39;: 139, &#39;여성가디건&#39;: 140, &#39;캐리어&#39;: 141, &#39;한방음료&#39;: 142, &#39;아동용가방&#39;: 143, &#39;여성일반스포츠바지&#39;: 144, &#39;노트북&#39;: 145, &#39;메이크업세트&#39;: 146, &#39;여성임부속옷&#39;: 147, &#39;접시&#39;: 148, &#39;반상기세트/홈세트&#39;: 149, &#39;장롱&#39;: 150, &#39;매트리스&#39;: 151, &#39;여성점퍼&#39;: 152, &#39;홈웨어세트&#39;: 153, &#39;남성골프티셔츠&#39;: 154, &#39;여성스웨트셔츠/후드/집업&#39;: 155, &#39;남성비치웨어&#39;: 156, &#39;기타모바일액세서리&#39;: 157, &#39;성인침구속통/솜&#39;: 158, &#39;바구니&#39;: 159, &#39;테이프&#39;: 160, &#39;칼/가위&#39;: 161, &#39;수정용품&#39;: 162, &#39;유아동선글라스&#39;: 163, &#39;여성클러치백&#39;: 164, &#39;영유아청바지&#39;: 165, &#39;캠핑테이블/의자&#39;: 166, &#39;여아바지&#39;: 167, &#39;남아청바지&#39;: 168, &#39;고양이사료&#39;: 169, &#39;커튼&#39;: 170, &#39;기타물놀이용품&#39;: 171, &#39;여성신발부속품&#39;: 172, &#39;노트북가방&#39;: 173, &#39;여성카드/명함지갑&#39;: 174, &#39;책상&#39;: 175, &#39;여성스커트&#39;: 176, &#39;욕실발판&#39;: 177, &#39;마스카라&#39;: 178, &#39;남성시계&#39;: 179, &#39;남성패딩&#39;: 180, &#39;식탁세트&#39;: 181, &#39;피규어&#39;: 182, &#39;욕실소품&#39;: 183, &#39;롤플레잉완구&#39;: 184, &#39;기타캠핑용품&#39;: 185, &#39;분말표백제&#39;: 186, &#39;풀&#39;: 187, &#39;종합영양제&#39;: 188, &#39;피트니스용품&#39;: 189, &#39;스툴/리빙의자&#39;: 190, &#39;책장&#39;: 191, &#39;칫솔&#39;: 192, &#39;수건&#39;: 193, &#39;액상세탁세제&#39;: 194, &#39;영유아블라우스&#39;: 195, &#39;남아바지&#39;: 196, &#39;헤어케어선물세트&#39;: 197, &#39;여성등산티셔츠/탑&#39;: 198, &#39;남성베스트&#39;: 199, &#39;썬캡&#39;: 200, &#39;스카프&#39;: 201, &#39;블러셔/쉐이딩/하이라이터&#39;: 202, &#39;여아레깅스&#39;: 203, &#39;애견목욕/위생용품&#39;: 204, &#39;기타일반문구/사무용품&#39;: 205, &#39;린스/컨디셔너&#39;: 206, &#39;샴푸/린스세트&#39;: 207, &#39;미용비누&#39;: 208, &#39;성인매트리스커버&#39;: 209, &#39;스냅백&#39;: 210, &#39;기타컴퓨터액세서리&#39;: 211, &#39;유아동플랫&#39;: 212, &#39;기타영양제&#39;: 213, &#39;여아남방셔츠&#39;: 214, &#39;남아의류세트&#39;: 215, &#39;탄산수&#39;: 216, &#39;일반청소기&#39;: 217, &#39;손싸개/발싸개&#39;: 218, &#39;유아동런닝/트레이닝화&#39;: 219, &#39;여성토트백&#39;: 220, &#39;여성일반양말&#39;: 221, &#39;여성런닝/트레이닝화&#39;: 222, &#39;치약&#39;: 223, &#39;샤워/목욕도구/목욕헤어밴드&#39;: 224, &#39;봉제인형&#39;: 225, &#39;유아동베개/베개커버&#39;: 226, &#39;놀이방매트&#39;: 227, &#39;식음료모바일상품권&#39;: 228, &#39;여아베스트&#39;: 229, &#39;성인베개/베개커버&#39;: 230, &#39;이유식용품&#39;: 231, &#39;아이섀도우&#39;: 232, &#39;남성샌들&#39;: 233, &#39;기타이미용가전&#39;: 234, &#39;요가/필라테스복&#39;: 235, &#39;여성향수&#39;: 236, &#39;기타정리용품&#39;: 237, &#39;트리트먼트/팩&#39;: 238, &#39;욕실청소용품&#39;: 239, &#39;기타주방정리용품/소모품&#39;: 240, &#39;여아청바지&#39;: 241, &#39;기름종이&#39;: 242, &#39;아이라이너&#39;: 243, &#39;홍삼/인삼혼합세트&#39;: 244, &#39;여성시계&#39;: 245, &#39;손수건&#39;: 246, &#39;팔찌&#39;: 247, &#39;남성숄더/크로스백&#39;: 248, &#39;지퍼백/비닐백&#39;: 249, &#39;성인패드/스프레드&#39;: 250, &#39;여성펌프스&#39;: 251, &#39;구강청정제&#39;: 252, &#39;남성일반양말&#39;: 253, &#39;여성오픈토&#39;: 254, &#39;공기청정기&#39;: 255, &#39;여성등산바지&#39;: 256, &#39;커튼링/커튼봉/부속품&#39;: 257, &#39;메이크업베이스/프라이머&#39;: 258, &#39;여아점퍼&#39;: 259, &#39;남성등산점퍼/재킷&#39;: 260, &#39;유아동슬립온&#39;: 261, &#39;인삼가공식품&#39;: 262, &#39;여성덧신류&#39;: 263, &#39;냉동핫도그&#39;: 264, &#39;여성블라우스&#39;: 265, &#39;식기건조대/수저통&#39;: 266, &#39;국자/뒤지개/주걱&#39;: 267, &#39;여성부츠&#39;: 268, &#39;여성베스트&#39;: 269, &#39;에멀젼/로션&#39;: 270, &#39;기타조리도구&#39;: 271, &#39;바디보습&#39;: 272, &#39;여성패딩&#39;: 273, &#39;성인이불/이불커버&#39;: 274, &#39;솥&#39;: 275, &#39;이불/옷압축팩&#39;: 276, &#39;섬유유연제/향기지속제&#39;: 277, &#39;양산&#39;: 278, &#39;여성수영복&#39;: 279, &#39;남성내의&#39;: 280, &#39;기타등산용품&#39;: 281, &#39;유아용세척용품&#39;: 282, &#39;도마&#39;: 283, &#39;생활모바일상품권&#39;: 284, &#39;젖병/젖꼭지&#39;: 285, &#39;아동우산&#39;: 286, &#39;케이스/보호필름&#39;: 287, &#39;남성스킨케어세트&#39;: 288, &#39;여성백팩&#39;: 289, &#39;커피머신&#39;: 290, &#39;방석/방석커버&#39;: 291, &#39;장식장/진열장&#39;: 292, &#39;다이어트보조식품&#39;: 293, &#39;남성스니커즈&#39;: 294, &#39;남성용클렌저&#39;: 295, &#39;소품가방&#39;: 296, &#39;변기시트/커버&#39;: 297, &#39;헤드웨어&#39;: 298, &#39;여행용소품&#39;: 299, &#39;목걸이&#39;: 300, &#39;미용보조식품&#39;: 301, &#39;제빵용품&#39;: 302, &#39;PC부품&#39;: 303, &#39;등산화&#39;: 304, &#39;젤네일/케어류&#39;: 305, &#39;핸디형청소기&#39;: 306, &#39;주방선반/걸이대&#39;: 307, &#39;옷걸이&#39;: 308, &#39;유아동침대&#39;: 309, &#39;로봇청소기&#39;: 310, &#39;헤어드라이어&#39;: 311, &#39;레저모바일상품권&#39;: 312, &#39;스폰지/퍼프&#39;: 313, &#39;커피용품&#39;: 314, &#39;커피잔&#39;: 315, &#39;저장장치&#39;: 316, &#39;우주복&#39;: 317, &#39;립스틱/립라이너&#39;: 318, &#39;조립/프라모델&#39;: 319, &#39;스케이트보드/킥보드&#39;: 320, &#39;남성로퍼&#39;: 321, &#39;여성쪼리&#39;: 322, &#39;여성트렌치코트&#39;: 323, &#39;남성정장재킷&#39;: 324, &#39;골프화&#39;: 325, &#39;남성트레이닝복&#39;: 326, &#39;전기찜기&#39;: 327, &#39;남성향수&#39;: 328, &#39;일반비타민&#39;: 329, &#39;여성슬링백&#39;: 330, &#39;촉각놀이/오뚝이&#39;: 331, &#39;기타패션잡화&#39;: 332, &#39;데오도란트&#39;: 333, &#39;수영모자&#39;: 334, &#39;여성슬리퍼&#39;: 335, &#39;고무장갑&#39;: 336, &#39;일반두유&#39;: 337, &#39;속눈썹/쌍꺼풀&#39;: 338, &#39;물안경&#39;: 339, &#39;기타여행용가방&#39;: 340, &#39;남성트렌치코트&#39;: 341, &#39;발찌&#39;: 342, &#39;남성스웨터/풀오버&#39;: 343, &#39;호두&#39;: 344, &#39;캠핑침구&#39;: 345, &#39;아동비치웨어&#39;: 346, &#39;애견의류/악세서리&#39;: 347, &#39;성인침구세트&#39;: 348, &#39;남성등산패딩&#39;: 349, &#39;냉동만두&#39;: 350, &#39;냄비&#39;: 351, &#39;남성캐주얼재킷&#39;: 352, &#39;승마운동기&#39;: 353, &#39;바디케어세트&#39;: 354, &#39;거들&#39;: 355, &#39;성인요/요커버&#39;: 356, &#39;여성내의&#39;: 357, &#39;루테인&#39;: 358, &#39;여성청바지&#39;: 359, &#39;여성잠옷&#39;: 360, &#39;인덕션/가스레인지&#39;: 361, &#39;캠핑취사&#39;: 362, &#39;마우스&#39;: 363, &#39;블랙박스&#39;: 364, &#39;포크/나이프&#39;: 365, &#39;남성속옷세트&#39;: 366, &#39;여성슬립온&#39;: 367, &#39;오메가3/기타추출오일&#39;: 368, &#39;헤어세팅기&#39;: 369, &#39;키보드&#39;: 370, &#39;모바일배터리/충전기&#39;: 371, &#39;채반/바구니/쟁반&#39;: 372, &#39;선반장/행거&#39;: 373, &#39;안경테&#39;: 374, &#39;여성골프바지&#39;: 375, &#39;커피메이커/포트&#39;: 376, &#39;유아동스포츠티셔츠/탑&#39;: 377, &#39;유아동스포츠스웨트셔츠/후드/집업&#39;: 378, &#39;고양이간식&#39;: 379, &#39;사무용/학생용가구세트&#39;: 380, &#39;벽걸이형에어컨&#39;: 381, &#39;여성등산점퍼/재킷&#39;: 382, &#39;홍삼정/분말/환&#39;: 383, &#39;토마토&#39;: 384, &#39;스킨케어디바이스&#39;: 385, &#39;입욕제/스파제품&#39;: 386, &#39;과일즙&#39;: 387, &#39;전기튀김기&#39;: 388, &#39;성인담요&#39;: 389, &#39;귀걸이&#39;: 390, &#39;소파&#39;: 391, &#39;행주&#39;: 392, &#39;여성골프남방셔츠&#39;: 393, &#39;영유아남방셔츠&#39;: 394, &#39;스텝퍼/트위스트&#39;: 395, &#39;여성등산패딩&#39;: 396, &#39;식탁&#39;: 397, &#39;전기밥솥&#39;: 398, &#39;대접/볼&#39;: 399, &#39;밥공기&#39;: 400, &#39;찬기/종지&#39;: 401, &#39;여성발가락양말&#39;: 402, &#39;역할놀이&#39;: 403, &#39;유아용카시트/매트&#39;: 404, &#39;젖병소독/건조용품&#39;: 405, &#39;유아/아동용칫솔&#39;: 406, &#39;기타냉동간편식&#39;: 407, &#39;유아목욕용품&#39;: 408, &#39;유아동트레이닝복&#39;: 409, &#39;여아잠옷&#39;: 410, &#39;선반/걸이&#39;: 411, &#39;여성양말선물세트&#39;: 412, &#39;물티슈&#39;: 413, &#39;남성코트&#39;: 414, &#39;분말세탁세제&#39;: 415, &#39;수유패드/보조용품&#39;: 416, &#39;유아동의자&#39;: 417, &#39;학생용가방&#39;: 418, &#39;남성스포츠점퍼/재킷&#39;: 419, &#39;유아공부상/디딤대&#39;: 420, &#39;잉크/토너&#39;: 421, &#39;여성향수세트&#39;: 422, &#39;펜던트&#39;: 423, &#39;여성일반지갑&#39;: 424, &#39;보드게임&#39;: 425, &#39;레고&#39;: 426, &#39;유아동스포츠점퍼/재킷&#39;: 427, &#39;치약/칫솔세트&#39;: 428, &#39;수영가방&#39;: 429, &#39;패션인형&#39;: 430, &#39;도시락/찬합&#39;: 431, &#39;발효원액&#39;: 432, &#39;남성등산베스트&#39;: 433, &#39;여성선글라스&#39;: 434, &#39;여성점프수트/오버롤&#39;: 435, &#39;치아발육기/딸랑이&#39;: 436, &#39;남성카드/명함지갑&#39;: 437, &#39;남성용선크림/메이크업류&#39;: 438, &#39;남성골프점퍼/재킷&#39;: 439, &#39;여성골프티셔츠/탑&#39;: 440, &#39;기타국산과일류&#39;: 441, &#39;여아스커트&#39;: 442, &#39;건조기&#39;: 443, &#39;기타기능성음료&#39;: 444, &#39;스피커&#39;: 445, &#39;얼음/빙수용품&#39;: 446, &#39;스타킹&#39;: 447, &#39;보온병/텀블러&#39;: 448, &#39;전동칫솔/칫솔모&#39;: 449, &#39;여아스웨트셔츠/후드/집업&#39;: 450, &#39;혼합견과&#39;: 451, &#39;볼펜&#39;: 452, &#39;필통&#39;: 453, &#39;샤프/샤프심&#39;: 454, &#39;필기구세트&#39;: 455, &#39;엽산/철분&#39;: 456, &#39;유아패션잡화&#39;: 457, &#39;휴대폰&#39;: 458, &#39;각티슈/미용티슈&#39;: 459, &#39;골프가방&#39;: 460, &#39;여성타이즈&#39;: 461, &#39;요가/스포츠매트&#39;: 462, &#39;여성골프스커트&#39;: 463, &#39;골프장갑&#39;: 464, &#39;여성골프니트/가디건&#39;: 465, &#39;여성골프베스트&#39;: 466, &#39;영유아점퍼&#39;: 467, &#39;영유아가디건&#39;: 468, &#39;남성스웨트셔츠/후드/집업&#39;: 469, &#39;사인펜&#39;: 470, &#39;남성선글라스&#39;: 471, &#39;반지&#39;: 472, &#39;이어폰/헤드폰&#39;: 473, &#39;조리도구세트&#39;: 474, &#39;키친타올&#39;: 475, &#39;남녀공용향수&#39;: 476, &#39;유아동레인부츠/슈즈&#39;: 477, &#39;유아건강보조제&#39;: 478, &#39;여성가운&#39;: 479, &#39;남성잠옷&#39;: 480, &#39;유아동스포츠패딩&#39;: 481, &#39;여아패딩&#39;: 482, &#39;전통/종교장신구&#39;: 483, &#39;복근/벨트마사지기구&#39;: 484, &#39;벙거지&#39;: 485, &#39;슬립&#39;: 486, &#39;만년필&#39;: 487, &#39;공병/모델링팩전용도구&#39;: 488, &#39;화장대&#39;: 489, &#39;핸드카트&#39;: 490, &#39;여성레인부츠/슈즈&#39;: 491, &#39;퍼즐&#39;: 492, &#39;캐쥬얼크로스백&#39;: 493, &#39;음악/악기완구&#39;: 494, &#39;아기체육관/러닝홈&#39;: 495, &#39;면봉/화장솜&#39;: 496, &#39;남성골프남방셔츠&#39;: 497, &#39;수예소품속통/솜&#39;: 498, &#39;쿠션/쿠션커버&#39;: 499, &#39;붙박이장&#39;: 500, &#39;기타견과류&#39;: 501, &#39;아몬드&#39;: 502, &#39;여성등산베스트&#39;: 503, &#39;영유아레깅스&#39;: 504, &#39;여성컴포트화&#39;: 505, &#39;남성정장화&#39;: 506, &#39;블라인드/버티컬&#39;: 507, &#39;스포츠두건/머플러/마스크&#39;: 508, &#39;남성골프패딩&#39;: 509, &#39;스포츠양말&#39;: 510, &#39;남성정장세트&#39;: 511, &#39;음료용컵&#39;: 512, &#39;인라인/스케이트보드/킥보드안전용품&#39;: 513, &#39;애견식기/물병&#39;: 514, &#39;복숭아&#39;: 515, &#39;글루코사민&#39;: 516, &#39;헤어브러쉬/롤&#39;: 517, &#39;거실화/실내화&#39;: 518, &#39;유아동담요&#39;: 519, &#39;고데기&#39;: 520, &#39;칼슘/미네랄&#39;: 521, &#39;주방수예소품&#39;: 522, &#39;무선조종&#39;: 523, &#39;수세미/솔&#39;: 524, &#39;유모차&#39;: 525, &#39;남성덧신류&#39;: 526, &#39;참외&#39;: 527, &#39;사과&#39;: 528, &#39;제습기&#39;: 529, &#39;양문형냉장고&#39;: 530, &#39;메이크업브러쉬&#39;: 531, &#39;가습기&#39;: 532, &#39;유아동요/요커버&#39;: 533, &#39;구명조끼/안전용품&#39;: 534, &#39;네일케어도구&#39;: 535, &#39;애견건강용품&#39;: 536, &#39;오븐/전자레인지&#39;: 537, &#39;압력솥&#39;: 538, &#39;기타유아동화&#39;: 539, &#39;유아동일반스포츠바지&#39;: 540, &#39;여성세정제&#39;: 541, &#39;비닐장갑&#39;: 542, &#39;스포츠선글라스&#39;: 543, &#39;미니자동차&#39;: 544, &#39;전자교육완구&#39;: 545, &#39;수박&#39;: 546, &#39;바디슬리밍/리프팅&#39;: 547, &#39;남아셔츠&#39;: 548, &#39;헤어무스/젤&#39;: 549, &#39;남아실내복&#39;: 550, &#39;풋케어&#39;: 551, &#39;여아재킷&#39;: 552, &#39;제기&#39;: 553, &#39;일반교육완구&#39;: 554, &#39;올인원&#39;: 555, &#39;유축기&#39;: 556, &#39;욕실화&#39;: 557, &#39;필기도구소모품&#39;: 558, &#39;남성머니클립&#39;: 559, &#39;헤어스프레이&#39;: 560, &#39;스팀청소기&#39;: 561, &#39;여성스포츠점퍼/재킷&#39;: 562, &#39;기타보석류&#39;: 563, &#39;자두&#39;: 564, &#39;메론&#39;: 565, &#39;멀티형에어컨&#39;: 566, &#39;기타모자&#39;: 567, &#39;유아동패드/스프레드&#39;: 568, &#39;카메라액세서리&#39;: 569, &#39;전기면도기&#39;: 570, &#39;유아동방한화&#39;: 571, &#39;남성서류가방&#39;: 572, &#39;책상정리용품&#39;: 573, &#39;비니&#39;: 574, &#39;남성슬립온&#39;: 575, &#39;디저트포크/스푼&#39;: 576, &#39;목욕용장난감&#39;: 577, &#39;우비&#39;: 578, &#39;주방수납장&#39;: 579, &#39;유아동수납장&#39;: 580, &#39;냉동떡볶이&#39;: 581, &#39;건강보조식품세트&#39;: 582, &#39;특수용세탁세제&#39;: 583, &#39;여성레깅스&#39;: 584, &#39;집게/클립&#39;: 585, &#39;캐노피&#39;: 586, &#39;핸드워시/손세정제&#39;: 587, &#39;유아용욕조&#39;: 588, &#39;모유보관용품&#39;: 589, &#39;바운서/쏘서/보행기&#39;: 590, &#39;발포비타민&#39;: 591, &#39;여성스포츠베스트&#39;: 592, &#39;드럼세탁기&#39;: 593, &#39;고양이건강용품&#39;: 594, &#39;스탠드형에어컨&#39;: 595, &#39;남성신발부속품&#39;: 596, &#39;유아동속옷세트&#39;: 597, &#39;자연유래영양제&#39;: 598, &#39;조리기구세트&#39;: 599, &#39;남성골프스웨트셔츠/후드/집업&#39;: 600, &#39;여성방한화&#39;: 601, &#39;LED&#39;: 602, &#39;UHD&#39;: 603, &#39;냉동튀김&#39;: 604, &#39;캐쥬얼백팩&#39;: 605, &#39;욕실수납용품&#39;: 606, &#39;연필깎이&#39;: 607, &#39;연필&#39;: 608, &#39;탐폰&#39;: 609, &#39;운동보조식품&#39;: 610, &#39;남성백팩&#39;: 611, &#39;유아동로퍼&#39;: 612, &#39;헤어왁스&#39;: 613, &#39;배냇저고리&#39;: 614, &#39;영유아베스트&#39;: 615, &#39;고양이장난감&#39;: 616, &#39;고양이목욕/위생용품&#39;: 617, &#39;여성스포츠스웨트셔츠/후드/집업&#39;: 618, &#39;국그릇&#39;: 619, &#39;남성힙색&#39;: 620, &#39;남성스포츠화부속품&#39;: 621, &#39;여아블라우스&#39;: 622, &#39;기타주방가전&#39;: 623, &#39;남성스포츠스웨트셔츠/후드/집업&#39;: 624, &#39;서류정리용품&#39;: 625, &#39;숙취해소음료&#39;: 626, &#39;배&#39;: 627, &#39;영화/문화모바일상품권&#39;: 628, &#39;남성스포츠속옷&#39;: 629, &#39;보온도시락&#39;: 630, &#39;남아스웨트셔츠/후드/집업&#39;: 631, &#39;홍삼절편&#39;: 632, &#39;침실가구세트&#39;: 633, &#39;일반네일/케어류&#39;: 634, &#39;남성슬리퍼&#39;: 635, &#39;시계세트&#39;: 636, &#39;한우선물세트&#39;: 637, &#39;남성클러치백&#39;: 638, &#39;붕붕카/스프링카/흔들말&#39;: 639, &#39;이발기&#39;: 640, &#39;삼계탕용닭&#39;: 641, &#39;남아레깅스&#39;: 642, &#39;블록&#39;: 643, &#39;기타청소기&#39;: 644, &#39;남성양말선물세트&#39;: 645, &#39;남성캐쥬얼스포츠양말&#39;: 646, &#39;목욕타올&#39;: 647, &#39;여성골프스웨트셔츠/후드/집업&#39;: 648, &#39;호일/랩/기름종이&#39;: 649, &#39;파일/바인더&#39;: 650, &#39;기타피트니스기구&#39;: 651, &#39;영양제세트&#39;: 652, &#39;튜브/보트&#39;: 653, &#39;에어로빅복&#39;: 654, &#39;여성사파리&#39;: 655, &#39;러닝/워킹머신&#39;: 656, &#39;롤스크린&#39;: 657, &#39;애견이동장&#39;: 658, &#39;액상표백제&#39;: 659, &#39;야외용돗자리&#39;: 660, &#39;침대&#39;: 661, &#39;여성등산전신/원피스&#39;: 662, &#39;메탈미용소도구&#39;: 663, &#39;모빌&#39;: 664, &#39;주전자&#39;: 665, &#39;주류잔&#39;: 666, &#39;오븐팬/피자팬&#39;: 667, &#39;홍삼근&#39;: 668, &#39;남성사파리&#39;: 669, &#39;여성골프점퍼/재킷&#39;: 670, &#39;형광펜&#39;: 671, &#39;독서대&#39;: 672, &#39;보석세트&#39;: 673, &#39;열쇠고리&#39;: 674, &#39;기타여성의류아우터&#39;: 675, &#39;영유아코트&#39;: 676, &#39;군모&#39;: 677, &#39;헬스바이크&#39;: 678, &#39;남성등산/아웃도어세트&#39;: 679, &#39;남성등산전신&#39;: 680, &#39;유아동부츠&#39;: 681, &#39;남성컴포트화&#39;: 682, &#39;영유아스웨터/풀오버&#39;: 683, &#39;스탠드형김치냉장고&#39;: 684, &#39;부분세탁제&#39;: 685, &#39;닭윗날개(봉)&#39;: 686, &#39;샤워커튼&#39;: 687, &#39;골프채&#39;: 688, &#39;미러리스&#39;: 689, &#39;기타유아안전용품&#39;: 690, &#39;영유아재킷&#39;: 691, &#39;등산지팡이/스틱&#39;: 692, &#39;땅콩&#39;: 693, &#39;냉동밥&#39;: 694, &#39;스포츠아대/헤어밴드&#39;: 695, &#39;순금/순은/장식품&#39;: 696, &#39;가발/부분가발&#39;: 697, &#39;여성등산/아웃도어세트&#39;: 698, &#39;전기그릴&#39;: 699, &#39;그릴/구이불판&#39;: 700, &#39;컵/행주살균기&#39;: 701, &#39;뚝배기&#39;: 702, &#39;기타유아동양말류&#39;: 703, &#39;유아변기/배변훈련기&#39;: 704, &#39;여성골프전신/원피스&#39;: 705, &#39;무릎담요&#39;: 706, &#39;태블릿PC&#39;: 707, &#39;캐슈넛&#39;: 708, &#39;남성스포츠패딩&#39;: 709, &#39;싱크대/배수구용품&#39;: 710, &#39;여성스포츠속옷&#39;: 711, &#39;교자상/다용도상&#39;: 712, &#39;캐쥬얼힙색&#39;: 713, &#39;매직/보드마카&#39;: 714, &#39;영유아스웨트셔츠/후드/집업&#39;: 715, &#39;남성골프베스트&#39;: 716, &#39;여성스포츠스커트&#39;: 717, &#39;자/제도용품&#39;: 718, &#39;문구세트&#39;: 719, &#39;남성수면양말&#39;: 720, &#39;남아스웨터/풀오버&#39;: 721, &#39;일반세탁기&#39;: 722, &#39;캐쥬얼숄더백&#39;: 723, &#39;이불/옷커버류&#39;: 724, &#39;전기냄비/뚝배기&#39;: 725, &#39;피스타치오&#39;: 726, &#39;돼지고기선물세트&#39;: 727, &#39;전자계산기&#39;: 728, &#39;힙색/사이드백&#39;: 729, &#39;머플러&#39;: 730, &#39;넥워머&#39;: 731, &#39;젓갈&#39;: 732, &#39;세탁비누&#39;: 733, &#39;목욕가운&#39;: 734, &#39;남성발가락양말&#39;: 735, &#39;공유기&#39;: 736, &#39;협탁&#39;: 737, &#39;성인침대커버/스커트&#39;: 738, &#39;명함정리용품&#39;: 739, &#39;절임반찬&#39;: 740, &#39;과실주병&#39;: 741, &#39;립밤/립스크럽&#39;: 742, &#39;제모용품&#39;: 743, &#39;제모기&#39;: 744, &#39;계량도구&#39;: 745, &#39;보드류&#39;: 746, &#39;프린터/복합기/스캐너&#39;: 747, &#39;양념통&#39;: 748, &#39;방울토마토&#39;: 749, &#39;밤&#39;: 750, &#39;스포츠목걸이/팔찌&#39;: 751, &#39;고양이식기/급수&#39;: 752, &#39;그늘막/타프&#39;: 753, &#39;기타모바일기기&#39;: 754, &#39;유아동옷장&#39;: 755, &#39;남아가디건&#39;: 756, &#39;테이블데코&#39;: 757, &#39;여성스포츠전신/원피스&#39;: 758, &#39;냅킨&#39;: 759, &#39;바란스&#39;: 760, &#39;뚜껑형김치냉장고&#39;: 761, &#39;남성골프니트/가디건&#39;: 762, &#39;여아코트&#39;: 763, &#39;유아동매트리스커버&#39;: 764, &#39;여성스포츠패딩&#39;: 765, &#39;다기류&#39;: 766, &#39;컴팩트&#39;: 767, &#39;2단우산&#39;: 768, &#39;기타냉장고&#39;: 769, &#39;물병&#39;: 770, &#39;데스크탑/올인원PC&#39;: 771, &#39;기타카메라&#39;: 772, &#39;마카다미아&#39;: 773, &#39;냉동부침&#39;: 774, &#39;브로치&#39;: 775, &#39;남아베스트&#39;: 776, &#39;여행용세트&#39;: 777, &#39;귤류&#39;: 778, &#39;하이앤드&#39;: 779, &#39;항아리/쌀독류&#39;: 780, &#39;헤어롤&#39;: 781, &#39;여아스웨터/풀오버&#39;: 782, &#39;냉동고&#39;: 783, &#39;하이브리드&#39;: 784, &#39;기타여성양말류&#39;: 785, &#39;패션액세서리세트&#39;: 786, &#39;기타영유아아우터&#39;: 787, &#39;잣&#39;: 788, &#39;스포츠음료&#39;: 789, &#39;스테이플러&#39;: 790, &#39;영유아패딩&#39;: 791, &#39;기타배낭소품&#39;: 792, &#39;시공가구&#39;: 793, &#39;DIY가구&#39;: 794, &#39;살구&#39;: 795, &#39;여성수면양말&#39;: 796, &#39;유아동침구매트&#39;: 797, &#39;기타남성양말류&#39;: 798, &#39;정수기&#39;: 799, &#39;여아실내복&#39;: 800, &#39;모니터&#39;: 801, &#39;유아동침구속통/솜&#39;: 802, &#39;전동보드/전동킥보드&#39;: 803, &#39;공간박스&#39;: 804, &#39;하이패스&#39;: 805, &#39;신발장&#39;: 806, &#39;DSLR&#39;: 807, &#39;남성실내복&#39;: 808, &#39;기타남성화&#39;: 809, &#39;네비게이션&#39;: 810, &#39;전기프라이팬&#39;: 811, &#39;여성실내복&#39;: 812, &#39;오프너/와인스크류&#39;: 813, &#39;유아동시계&#39;: 814, &#39;환풍기&#39;: 815, &#39;볶음반찬&#39;: 816, &#39;파티션&#39;: 817, &#39;냉온풍기&#39;: 818, &#39;펀치류&#39;: 819, &#39;냉동피자&#39;: 820, &#39;기차/레일완구&#39;: 821, &#39;인라인/롤러스케이트&#39;: 822, &#39;매실&#39;: 823, &#39;주방용탈수기&#39;: 824, &#39;유아동침대커버/스커트&#39;: 825, &#39;기타자동차가전기기&#39;: 826, &#39;여성캐쥬얼스포츠양말&#39;: 827, &#39;네일세트&#39;: 828, &#39;남성스포츠베스트&#39;: 829, &#39;채칼/강판/절구&#39;: 830, &#39;캐쥬얼시계&#39;: 831, &#39;니삭스/오버니삭스&#39;: 832, &#39;싸인물/자석/압핀&#39;: 833, &#39;고양이의류/악세서리&#39;: 834, &#39;비타민/에너지음료&#39;: 835, &#39;에어워셔&#39;: 836, &#39;냉장/냉동가전소모품&#39;: 837, &#39;남성부츠&#39;: 838, &#39;물걸레청소기&#39;: 839, &#39;음식물건조기&#39;: 840, &#39;요구르트/청국장제조기&#39;: 841, &#39;골프채세트&#39;: 842, &#39;고양이이동장&#39;: 843, &#39;냉동면&#39;: 844, &#39;소프트웨어&#39;: 845, &#39;캠코더&#39;: 846, &#39;무화과&#39;: 847, &#39;OLED&#39;: 848, &#39;탈수기&#39;: 849, &#39;육가공품선물세트&#39;: 850, &#39;딸기&#39;: 851, &#39;식기세척기&#39;: 852, &#39;차량용충전기&#39;: 853, &#39;유아두유&#39;: 854, &#39;닭근위&#39;: 855, &#39;닭아랫날개(윙)&#39;: 856, &#39;안경소품&#39;: 857, &#39;여성가방액세서리&#39;: 858, &#39;태닝/애프터선케어&#39;: 859, &#39;유아동스포츠스커트&#39;: 860, &#39;카메라렌즈&#39;: 861, &#39;닭안심&#39;: 862, &#39;포도&#39;: 863, &#39;볶음탕용닭&#39;: 864, &#39;콜렉션인형&#39;: 865, &#39;LCD&#39;: 866, &#39;리모컨/액세서리&#39;: 867, &#39;반죽기/제면기&#39;: 868, &#39;식기건조기&#39;: 869, &#39;단무지&#39;: 870, &#39;닭다리&#39;: 871, &#39;감&#39;: 872, &#39;오리고기&#39;: 873, &#39;침구청소기&#39;: 874, &#39;싱크대&#39;: 875, &#39;미용거울&#39;: 876, &#39;남녀공용향수세트&#39;: 877, &#39;인라인/스케이트보드/킥보드기타액세서리&#39;: 878, &#39;남성등산스웨트셔츠/후드/집업&#39;: 879, &#39;커튼류세트&#39;: 880, &#39;오토캠핑용품세트&#39;: 881, &#39;수도용품&#39;: 882, &#39;석류&#39;: 883, &#39;양푼/믹싱볼&#39;: 884, &#39;자연/과학완구&#39;: 885, &#39;기저귀크림/파우더&#39;: 886, &#39;세탁기소모품&#39;: 887, &#39;냉동디저트&#39;: 888, &#39;기타광학기기&#39;: 889} . data_test[&#39;clac1_nm&#39;] = data_test[&#39;clac1_nm&#39;].map(lambda x: [clac1_nm_dic[nm] for nm in x]) data_test[&#39;clac2_nm&#39;] = data_test[&#39;clac2_nm&#39;].map(lambda x: [clac2_nm_dic[nm] for nm in x]) data_test[&#39;clac3_nm&#39;] = data_test[&#39;clac3_nm&#39;].map(lambda x: [clac3_nm_dic[nm] for nm in x]) . clac1_matrix = np.zeros((len(data_test), len(clac1_nm_dic))) for i in range(len(data_test)): for j in data_test[&#39;clac1_nm&#39;][i]: clac1_matrix[i, j] += 1 . cols = [&#39;clac1_nm_&#39; + str(i) for i in range(len(clac1_nm_dic))] . clac1_df = pd.DataFrame(clac1_matrix) clac1_df.columns = cols . clac2_matrix = np.zeros((len(data_test), len(clac2_nm_dic))) for i in range(len(data_test)): for j in data_test[&#39;clac2_nm&#39;][i]: clac2_matrix[i, j] += 1 . cols = [&#39;clac2_nm_&#39; + str(i) for i in range(len(clac2_nm_dic))] . clac2_df = pd.DataFrame(clac2_matrix) clac2_df.columns = cols . clac3_matrix = np.zeros((len(data_test), len(clac3_nm_dic))) for i in range(len(data_test)): for j in data_test[&#39;clac3_nm&#39;][i]: clac3_matrix[i, j] += 1 . cols = [&#39;clac3_nm_&#39; + str(i) for i in range(len(clac3_nm_dic))] . clac3_df = pd.DataFrame(clac3_matrix) clac3_df.columns = cols . data_test_concat = pd.concat([data_test.iloc[:, :-5], clac1_df, clac2_df, clac3_df], axis=1) . data_test_concat . clnt_id num_shopping avg_price total_price avg_ct total_ct avg_sess_view total_sess_view avg_sess_hr total_sess_hr ... clac3_nm_880 clac3_nm_881 clac3_nm_882 clac3_nm_883 clac3_nm_884 clac3_nm_885 clac3_nm_886 clac3_nm_887 clac3_nm_888 clac3_nm_889 . 0 2 | 3 | 32000 | 128000 | 1.33333 | 4 | 211.333 | 634 | 3315 | 9945 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 3 | 2 | 106400 | 212800 | 1 | 2 | 93 | 186 | 1051 | 2102 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 10 | 3 | 123000 | 369000 | 1 | 3 | 134.667 | 404 | 1745.67 | 5237 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 15 | 17 | 17587.9 | 334170 | 1.11765 | 19 | 105.412 | 1792 | 1356.71 | 23064 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 29 | 2 | 63000 | 126000 | 1 | 2 | 69 | 138 | 2120 | 4240 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 113099 263089 | 3 | 68633.3 | 205900 | 1 | 3 | 63.3333 | 190 | 1700.33 | 5101 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 113100 263097 | 4 | 60610 | 242440 | 1 | 4 | 469.75 | 1879 | 3422 | 13688 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 113101 263098 | 1 | 100200 | 100200 | 1 | 1 | 136 | 136 | 1175 | 1175 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 113102 263099 | 4 | 12250 | 49000 | 1 | 4 | 274 | 1096 | 3890 | 15560 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 113103 263100 | 2 | 11200 | 22400 | 1 | 2 | 104 | 208 | 1474.5 | 2949 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 113104 rows × 1066 columns . mat_clac1 = np.array(data_test_concat.iloc[:, 11:48]).dot(lookup_table_clac1) mat_clac1 = mat_clac1 / mat_clac1.sum(1).reshape(-1, 1) . mat_clac1 = pd.DataFrame(mat_clac1) mat_clac1 = mat_clac1.rename(columns={0: &#39;given_clac1_F20_prob&#39;, 1:&#39;given_clac1_F30_prob&#39;, 2:&#39;given_clac1_F40_prob&#39;, 3:&#39;given_clac1_M20_prob&#39;, 4:&#39;given_clac1_M30_prob&#39;, 5:&#39;given_clac1_M40_prob&#39;}) . mat_buy_clac1 = np.where(np.array(data_test_concat.iloc[:, 11:48]) &gt;= 1, 1, 0).dot(lookup_table_clac1) mat_buy_clac1 = mat_buy_clac1 / mat_buy_clac1.sum(1).reshape(-1, 1) . mat_buy_clac1 = pd.DataFrame(mat_buy_clac1) mat_buy_clac1 = mat_buy_clac1.rename(columns={0: &#39;buy_clac1_F20_prob&#39;, 1:&#39;buy_clac1_F30_prob&#39;, 2:&#39;buy_clac1_F40_prob&#39;, 3:&#39;buy_clac1_M20_prob&#39;, 4:&#39;buy_clac1_M30_prob&#39;, 5:&#39;buy_clac1_M40_prob&#39;}) . mat_clac2 = np.array(data_test_concat.iloc[:, 48:-890]).dot(lookup_table_clac2) mat_clac2 = mat_clac2 / mat_clac2.sum(1).reshape(-1, 1) . mat_clac2 = pd.DataFrame(mat_clac2) mat_clac2 = mat_clac2.rename(columns={0: &#39;given_clac2_F20_prob&#39;, 1:&#39;given_clac2_F30_prob&#39;, 2:&#39;given_clac2_F40_prob&#39;, 3:&#39;given_clac2_M20_prob&#39;, 4:&#39;given_clac2_M30_prob&#39;, 5:&#39;given_clac2_M40_prob&#39;}) . mat_buy_clac2 = np.where(np.array(data_test_concat.iloc[:, 48:-890]) &gt;= 1, 1, 0).dot(lookup_table_clac2) mat_buy_clac2 = mat_buy_clac2 / mat_buy_clac2.sum(1).reshape(-1, 1) . mat_buy_clac2 = pd.DataFrame(mat_buy_clac2) mat_buy_clac2 = mat_buy_clac2.rename(columns={0: &#39;buy_clac2_F20_prob&#39;, 1:&#39;buy_clac2_F30_prob&#39;, 2:&#39;buy_clac2_F40_prob&#39;, 3:&#39;buy_clac2_M20_prob&#39;, 4:&#39;buy_clac2_M30_prob&#39;, 5:&#39;buy_clac2_M40_prob&#39;}) . lookup_table_clac3 = np.concatenate([lookup_table_clac3, np.ones([7, 6]) * 1/6], axis=0) . mat_clac3 = np.array(data_test_concat.iloc[:, -890:]).dot(lookup_table_clac3) mat_clac3 = mat_clac3 / mat_clac3.sum(1).reshape(-1, 1) . mat_clac3 = pd.DataFrame(mat_clac3) mat_clac3 = mat_clac3.rename(columns={0: &#39;given_clac3_F20_prob&#39;, 1:&#39;given_clac3_F30_prob&#39;, 2:&#39;given_clac3_F40_prob&#39;, 3:&#39;given_clac3_M20_prob&#39;, 4:&#39;given_clac3_M30_prob&#39;, 5:&#39;given_clac3_M40_prob&#39;}) . mat_buy_clac3 = np.where(np.array(data_test_concat.iloc[:, -890:]) &gt;= 1, 1, 0).dot(lookup_table_clac3) mat_buy_clac3 = mat_buy_clac3 / mat_buy_clac3.sum(1).reshape(-1, 1) . mat_buy_clac3 = pd.DataFrame(mat_buy_clac3) mat_buy_clac3 = mat_buy_clac3.rename(columns={0: &#39;buy_clac3_F20_prob&#39;, 1:&#39;buy_clac3_F30_prob&#39;, 2:&#39;buy_clac3_F40_prob&#39;, 3:&#39;buy_clac3_M20_prob&#39;, 4:&#39;buy_clac3_M30_prob&#39;, 5:&#39;buy_clac3_M40_prob&#39;}) . tt = df_test.groupby(&#39;CLNT_ID&#39;)[&#39;PD_C&#39;].value_counts()[df_test[&#39;CLNT_ID&#39;].unique()] kk = df_test[&#39;CLNT_ID&#39;].unique() ww = df_test.groupby([&#39;CLNT_ID&#39;, &#39;PD_C&#39;])[&#39;KWD_NM&#39;].count() . pd_mean_vector = [] for words in tqdm(train_pd_w2v[-len(test_pd_w2v):]): tmp = np.zeros(256) cnt = 0 for word in words: try: tmp += pd_w2v[word] cnt += 1 except: pass #tmp /= cnt pd_mean_vector.append(tmp) pd_mean_vector = np.array(pd_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:04&lt;00:00, 27884.46it/s] . pd_unweight_vector = [] for i, words in tqdm(enumerate(train_pd_w2v[-len(test_pd_w2v):])): tmp = np.zeros(256) cnt = 0 for word in words: try: tmp += pd_w2v[word] * tt[kk[i]][word] cnt += tt[kk[i]][word] except: pass #tmp /= cnt pd_unweight_vector.append(tmp) pd_unweight_vector = np.array(pd_unweight_vector) . 113104it [17:48, 105.88it/s] . pd_weight_vector = [] for i, words in tqdm(enumerate(train_pd_w2v[-len(test_pd_w2v):])): tmp = np.zeros(256) cnt = 0 for word in words: try: tmp += pd_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt pd_weight_vector.append(tmp) pd_weight_vector = np.array(pd_weight_vector) . 113104it [09:06, 207.03it/s] . test_pd_mean = pd_mean_vector test_pd_unweight = pd_unweight_vector test_pd_weight = pd_weight_vector test_pd_mean = pd.DataFrame(test_pd_mean) test_pd_unweight = pd.DataFrame(test_pd_unweight) test_pd_weight = pd.DataFrame(test_pd_weight) test_pd_mean.columns = [&#39;pd_mean_&#39;+str(i) for i in range(256)] test_pd_unweight.columns = [&#39;pd_unweight_&#39;+str(i) for i in range(256)] test_pd_weight.columns = [&#39;pd_weight_&#39;+str(i) for i in range(256)] . tt = df_test.groupby(&#39;CLNT_ID&#39;)[&#39;PD_BRA_NM&#39;].value_counts()[df_test[&#39;CLNT_ID&#39;].unique()] kk = df_test[&#39;CLNT_ID&#39;].unique() ww = df_test.groupby([&#39;CLNT_ID&#39;, &#39;PD_BRA_NM&#39;])[&#39;KWD_NM&#39;].count() . bra_mean_vector = [] for words in tqdm(train_bra_w2v[-len(test_bra_w2v):]): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += bra_w2v[word] cnt += 1 except: pass #tmp /= cnt bra_mean_vector.append(tmp) bra_mean_vector = np.array(bra_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:02&lt;00:00, 39453.33it/s] . bra_unweight_vector = [] for i, words in tqdm(enumerate(train_bra_w2v[-len(test_bra_w2v):])): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += bra_w2v[word] * tt[kk[i]][word] cnt += tt[kk[i]][word] except: pass #tmp /= cnt bra_unweight_vector.append(tmp) bra_unweight_vector = np.array(bra_unweight_vector) . 113104it [10:29, 179.56it/s] . bra_weight_vector = [] for i, words in tqdm(enumerate(train_bra_w2v[-len(test_bra_w2v):])): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += bra_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt bra_weight_vector.append(tmp) bra_weight_vector = np.array(bra_weight_vector) . 113104it [06:47, 277.46it/s] . test_bra_mean = bra_mean_vector test_bra_unweight = bra_unweight_vector test_bra_weight = bra_weight_vector test_bra_mean = pd.DataFrame(test_bra_mean) test_bra_unweight = pd.DataFrame(test_bra_unweight) test_bra_weight = pd.DataFrame(test_bra_weight) test_bra_mean.columns = [&#39;bra_mean_&#39;+str(i) for i in range(128)] test_bra_unweight.columns = [&#39;bra_unweight_&#39;+str(i) for i in range(128)] test_bra_weight.columns = [&#39;bra_weight_&#39;+str(i) for i in range(128)] . tt = df_test.groupby(&#39;CLNT_ID&#39;)[&#39;KWD_NM&#39;].value_counts()[df_test[&#39;CLNT_ID&#39;].unique()] kk = df_test[&#39;CLNT_ID&#39;].unique() ww = df_test.groupby([&#39;CLNT_ID&#39;, &#39;KWD_NM&#39;])[&#39;KWD_NM&#39;].count() . kwd_mean_vector = [] for words in tqdm(train_kwd_w2v[-len(test_kwd_w2v):]): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += kwd_w2v[word] cnt += 1 except: pass #tmp /= cnt kwd_mean_vector.append(tmp) kwd_mean_vector = np.array(kwd_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:04&lt;00:00, 23558.67it/s] . kwd_unweight_vector = [] for i, words in tqdm(enumerate(train_kwd_w2v[-len(test_kwd_w2v):])): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += kwd_w2v[word] * tt[kk[i]][word] cnt += tt[kk[i]][word] except: pass #tmp /= cnt kwd_unweight_vector.append(tmp) kwd_unweight_vector = np.array(kwd_unweight_vector) . 113104it [27:14, 69.20it/s] . kwd_weight_vector = [] for i, words in tqdm(enumerate(train_kwd_w2v[-len(test_kwd_w2v):])): tmp = np.zeros(128) cnt = 0 for word in words: try: tmp += kwd_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt kwd_weight_vector.append(tmp) kwd_weight_vector = np.array(kwd_weight_vector) . 113104it [11:43, 160.87it/s] . test_kwd_mean = kwd_mean_vector test_kwd_unweight = kwd_unweight_vector test_kwd_weight = kwd_weight_vector test_kwd_mean = pd.DataFrame(test_kwd_mean) test_kwd_unweight = pd.DataFrame(test_kwd_unweight) test_kwd_weight = pd.DataFrame(test_kwd_weight) test_kwd_mean.columns = [&#39;kwd_mean_&#39;+str(i) for i in range(128)] test_kwd_unweight.columns = [&#39;kwd_unweight_&#39;+str(i) for i in range(128)] test_kwd_weight.columns = [&#39;kwd_weight_&#39;+str(i) for i in range(128)] . tt = df_test.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC3_NM&#39;].value_counts()[df_test[&#39;CLNT_ID&#39;].unique()] kk = df_test[&#39;CLNT_ID&#39;].unique() ww = df_test.groupby([&#39;CLNT_ID&#39;, &#39;CLAC3_NM&#39;])[&#39;KWD_NM&#39;].count() . clac3_mean_vector = [] for words in tqdm(train_clac3_w2v[-len(test_clac3_w2v):]): tmp = np.zeros(30) cnt = 0 for word in words: try: tmp += clac3_w2v[word] cnt += 1 except: pass #tmp /= cnt clac3_mean_vector.append(tmp) clac3_mean_vector = np.array(clac3_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:02&lt;00:00, 41843.84it/s] . clac3_unweight_vector = [] for i, words in tqdm(enumerate(train_clac3_w2v[-len(test_clac3_w2v):])): tmp = np.zeros(30) cnt = 0 for word in words: try: tmp += clac3_w2v[word] * clac3_matrix[i, clac3_nm_dic[word]] cnt += clac3_matrix[i, clac3_nm_dic[word]] except: pass #tmp /= cnt clac3_unweight_vector.append(tmp) clac3_unweight_vector = np.array(clac3_unweight_vector) . 113104it [00:04, 26316.69it/s] . clac3_weight_vector = [] for i, words in tqdm(enumerate(train_clac3_w2v[-len(test_clac3_w2v):])): tmp = np.zeros(30) cnt = 0 for word in words: try: tmp += clac3_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt clac3_weight_vector.append(tmp) clac3_weight_vector = np.array(clac3_weight_vector) . 113104it [06:47, 277.63it/s] . test_clac3_mean = clac3_mean_vector test_clac3_unweight = clac3_unweight_vector test_clac3_weight = clac3_weight_vector test_clac3_mean = pd.DataFrame(test_clac3_mean) test_clac3_unweight = pd.DataFrame(test_clac3_unweight) test_clac3_weight = pd.DataFrame(test_clac3_weight) test_clac3_mean.columns = [&#39;clac3_mean_&#39;+str(i) for i in range(30)] test_clac3_unweight.columns = [&#39;clac3_unweight_&#39;+str(i) for i in range(30)] test_clac3_weight.columns = [&#39;clac3_weight_&#39;+str(i) for i in range(30)] . tt = df_test.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC2_NM&#39;].value_counts()[df_test[&#39;CLNT_ID&#39;].unique()] kk = df_test[&#39;CLNT_ID&#39;].unique() ww = df_test.groupby([&#39;CLNT_ID&#39;, &#39;CLAC2_NM&#39;])[&#39;KWD_NM&#39;].count() . clac2_mean_vector = [] for words in tqdm(train_clac2_w2v[-len(test_clac2_w2v):]): tmp = np.zeros(10) cnt = 0 for word in words: try: tmp += clac2_w2v[word] cnt += 1 except: pass #tmp /= cnt clac2_mean_vector.append(tmp) clac2_mean_vector = np.array(clac2_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:02&lt;00:00, 46386.39it/s] . clac2_unweight_vector = [] for i, words in tqdm(enumerate(train_clac2_w2v[-len(test_clac2_w2v):])): tmp = np.zeros(10) cnt = 0 for word in words: try: tmp += clac2_w2v[word] * clac2_matrix[i, clac2_nm_dic[word]] cnt += clac2_matrix[i, clac2_nm_dic[word]] except: pass #tmp /= cnt clac2_unweight_vector.append(tmp) clac2_unweight_vector = np.array(clac2_unweight_vector) . 113104it [00:03, 31331.93it/s] . clac2_weight_vector = [] for i, words in tqdm(enumerate(train_clac2_w2v[-len(test_clac2_w2v):])): tmp = np.zeros(10) cnt = 0 for word in words: try: tmp += clac2_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt clac2_weight_vector.append(tmp) clac2_weight_vector = np.array(clac2_weight_vector) . 113104it [05:47, 325.31it/s] . test_clac2_mean = clac2_mean_vector test_clac2_unweight = clac2_unweight_vector test_clac2_weight = clac2_weight_vector test_clac2_mean = pd.DataFrame(test_clac2_mean) test_clac2_unweight = pd.DataFrame(test_clac2_unweight) test_clac2_weight = pd.DataFrame(test_clac2_weight) test_clac2_mean.columns = [&#39;clac2_mean_&#39;+str(i) for i in range(10)] test_clac2_unweight.columns = [&#39;clac2_unweight_&#39;+str(i) for i in range(10)] test_clac2_weight.columns = [&#39;clac2_weight_&#39;+str(i) for i in range(10)] . tt = df_test.groupby(&#39;CLNT_ID&#39;)[&#39;CLAC1_NM&#39;].value_counts()[df_test[&#39;CLNT_ID&#39;].unique()] kk = df_test[&#39;CLNT_ID&#39;].unique() ww = df_test.groupby([&#39;CLNT_ID&#39;, &#39;CLAC1_NM&#39;])[&#39;KWD_NM&#39;].count() . clac1_mean_vector = [] for words in tqdm(train_clac1_w2v[-len(test_clac1_w2v):]): tmp = np.zeros(5) cnt = 0 for word in words: try: tmp += clac1_w2v[word] cnt += 1 except: pass #tmp /= cnt clac1_mean_vector.append(tmp) clac1_mean_vector = np.array(clac1_mean_vector) . 100%|███████████████████████████████████████████████████████████████████████| 113104/113104 [00:01&lt;00:00, 57800.50it/s] . clac1_unweight_vector = [] for i, words in tqdm(enumerate(train_clac1_w2v[-len(test_clac1_w2v):])): tmp = np.zeros(5) cnt = 0 for word in words: try: tmp += clac1_w2v[word] * clac1_matrix[i, clac1_nm_dic[word]] cnt += clac1_matrix[i, clac1_nm_dic[word]] except: pass #tmp /= cnt clac1_unweight_vector.append(tmp) clac1_unweight_vector = np.array(clac1_unweight_vector) . 113104it [00:02, 38104.20it/s] . clac1_weight_vector = [] for i, words in tqdm(enumerate(train_clac1_w2v[-len(test_clac1_w2v):])): tmp = np.zeros(5) cnt = 0 for word in words: try: tmp += clac1_w2v[word] * (ww[kk[i]] / ww[kk[i]].sum())[word] cnt += (ww[kk[i]] / ww[kk[i]].sum())[word] except: pass #tmp /= cnt clac1_weight_vector.append(tmp) clac1_weight_vector = np.array(clac1_weight_vector) . 113104it [04:28, 421.84it/s] . test_clac1_mean = clac1_mean_vector test_clac1_unweight = clac1_unweight_vector test_clac1_weight = clac1_weight_vector test_clac1_mean = pd.DataFrame(test_clac1_mean) test_clac1_unweight = pd.DataFrame(test_clac1_unweight) test_clac1_weight = pd.DataFrame(test_clac1_weight) test_clac1_mean.columns = [&#39;clac1_mean_&#39;+str(i) for i in range(5)] test_clac1_unweight.columns = [&#39;clac1_unweight_&#39;+str(i) for i in range(5)] test_clac1_weight.columns = [&#39;clac1_weight_&#39;+str(i) for i in range(5)] . data_test_concat = pd.concat([data_test_concat.iloc[:, 1:11], mat_clac1, mat_buy_clac1, mat_clac2, mat_buy_clac2, mat_clac3, mat_buy_clac3, test_pd_mean, test_bra_mean, test_kwd_mean, test_clac1_mean, test_clac2_mean, test_clac3_mean, test_pd_unweight, test_bra_unweight, test_kwd_unweight, test_clac1_unweight, test_clac2_unweight, test_clac3_unweight, test_pd_weight, test_bra_weight, test_kwd_weight, test_clac1_weight, test_clac2_weight, test_clac3_weight], axis=1) . data_test_concat.to_csv(&#39;test.csv&#39;, index=False) . data_test_concat = pd.read_csv(&#39;test.csv&#39;) test_stat = pd.read_csv(&#39;stat_test_df.csv&#39;) . data_test_concat = pd.concat([test_stat.iloc[:, 1:], data_test_concat.iloc[:, 10:]], axis=1) . X_test_scaled = scaler.transform(data_test_concat) . pred1 = model1.predict(X_test_scaled[:, :-557]) pred2 = model2.predict(X_test_scaled[:, :-557]) pred3 = model3.predict(X_test_scaled[:, :-557]) pred4 = model4.predict(X_test_scaled[:, :-557]) pred5 = model5.predict(X_test_scaled[:, :-557]) pred6 = model6.predict(X_test_scaled[:, :-557]) pred7 = model7.predict(X_test_scaled[:, :-557]) pred8 = model8.predict(X_test_scaled[:, :-557]) pred9 = model9.predict(X_test_scaled[:, :-557]) pred10 = model10.predict(X_test_scaled[:, :-557]) . pred_mlp = (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10) / 10 . pred_lgb = np.zeros((len(X_test_scaled), 6)) for i in range(n_model): pred_lgb += model_dict[&#39;model_&#39;+str(i)].predict(X_test_scaled) pred_lgb = pred_lgb / n_model . pred_lgb . array([[0.01483922, 0.75846819, 0.12915512, 0.00084974, 0.06176788, 0.03491985], [0.49494485, 0.2411636 , 0.16966314, 0.05907518, 0.01124558, 0.02390764], [0.04323738, 0.2232262 , 0.49311356, 0.01315682, 0.09265073, 0.13461531], ..., [0.05330672, 0.37161363, 0.31219931, 0.02595297, 0.09278512, 0.14414225], [0.17001405, 0.47227626, 0.16736071, 0.05017716, 0.09524455, 0.04492727], [0.0470015 , 0.20116348, 0.6817618 , 0.00733369, 0.01636128, 0.04637825]]) . pred = (pred_mlp + pred_lgb) / 2 . pred . array([[0.03059318, 0.72713347, 0.1353143 , 0.00091057, 0.07431102, 0.03173749], [0.57986506, 0.20258786, 0.12928684, 0.06264269, 0.01032955, 0.01528801], [0.04360476, 0.2405963 , 0.4654439 , 0.01737814, 0.09299009, 0.13998683], ..., [0.04565929, 0.35647236, 0.34295776, 0.01862472, 0.08863805, 0.14764782], [0.13791403, 0.45997391, 0.23494042, 0.0298647 , 0.08869868, 0.04860827], [0.05066791, 0.18960662, 0.67846334, 0.00921454, 0.01420041, 0.05784716]]) . y_test = pd.read_csv(&#39;./sample_submission.csv&#39;) . y_test.iloc[:, 1:] = pred . y_test . CLNT_ID F20 F30 F40 M20 M30 M40 . 0 2 | 0.030593 | 0.727133 | 0.135314 | 0.000911 | 0.074311 | 0.031737 | . 1 3 | 0.579865 | 0.202588 | 0.129287 | 0.062643 | 0.010330 | 0.015288 | . 2 10 | 0.043605 | 0.240596 | 0.465444 | 0.017378 | 0.092990 | 0.139987 | . 3 15 | 0.017765 | 0.550447 | 0.280376 | 0.001969 | 0.073255 | 0.076189 | . 4 29 | 0.552116 | 0.329472 | 0.091156 | 0.007910 | 0.011378 | 0.007968 | . ... ... | ... | ... | ... | ... | ... | ... | . 113099 263089 | 0.173268 | 0.391064 | 0.402903 | 0.002203 | 0.011206 | 0.019356 | . 113100 263097 | 0.143175 | 0.268077 | 0.522866 | 0.004085 | 0.011733 | 0.050065 | . 113101 263098 | 0.045659 | 0.356472 | 0.342958 | 0.018625 | 0.088638 | 0.147648 | . 113102 263099 | 0.137914 | 0.459974 | 0.234940 | 0.029865 | 0.088699 | 0.048608 | . 113103 263100 | 0.050668 | 0.189607 | 0.678463 | 0.009215 | 0.014200 | 0.057847 | . 113104 rows × 7 columns . y_test.to_csv(&#39;./sample_submission.csv&#39;, index=False) .",
            "url": "https://hajunyoo.github.io/Blog/datascience/project/2022/06/25/2021_LPoint.html",
            "relUrl": "/datascience/project/2022/06/25/2021_LPoint.html",
            "date": " • Jun 25, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "“Connecting the dots, So you have to trust that the dots will somehow connect in your future.” - Steve Jobs . . 📚간단 소개 문구 . 안녕하세요 👋 . 👀 빅데이터 연합 분석 동아리 BITAMIN 7th 운영진, 8th 멤버 👀 HUFS 멋쟁이사자처럼 10th 멤버 . 👀 저는 데이터 사이언스와 데이터 엔지니어링에 관심이 많습니다 🌱 Now I’m studying deep about Django 🌱 Hope to study : Elastic search &amp; Spark 📫 How to reach me -&gt; Yuki’s 링크 트리 👀 This is my portfolio resume : Yuki’s Portfolio 👀 My Blog : Yuki’s Blog . . Yuki&#39;s Github . . . .",
          "url": "https://hajunyoo.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
  

  

  

  
  

  

  
  

  
  

  

  
  

  

  

  

  

  

  

}