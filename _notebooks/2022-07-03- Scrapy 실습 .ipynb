{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d95f083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T07:19:05.707732Z",
     "start_time": "2022-07-03T07:19:05.695734Z"
    }
   },
   "source": [
    "# Scrapy 실습\n",
    "> 간단한 Scrapy 실습을 정리하였습니다\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [crawling]\n",
    "- image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03c84b",
   "metadata": {},
   "source": [
    "# Scrapy 실습\n",
    "\n",
    "http꞉// 를 붙여주면, start_urls 에서 http꞉// 를 무조건 붙여주기 때문에, 결과적으로 http꞉// 가 두 번 붙게되어, 강제로 http꞉// 를 start_urls 에서 삭제해야 한다\n",
    "\n",
    "``` shell\n",
    "// 프로젝트를 생성하고 크롤러 모듈을 생성하자\n",
    "\n",
    "scrapy startproject ecommerce1\n",
    "\n",
    "scrapy genspider gmarket_best corners.gmarket.co.kr/Bestsellers\n",
    "```\n",
    "위의 명령어를 통해 url에 맞는 크롤러 python 모듈을 생성하자\n",
    "\n",
    "start_urls는 def parse의 response로 들어가게 된다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2545ced",
   "metadata": {},
   "source": [
    "- 구조를 한번 살펴보고 내려가자\n",
    "- ** ** 표시를 한 것이 오늘의 핵심 파일이 될 예정이다.\n",
    "\n",
    "\n",
    "```shell\n",
    "\n",
    "scrapy.cfg # deploy configuration file\n",
    "    ecommerce1/ # project's Python module, you'll import your code from her\n",
    "        __init__.py\n",
    "        **items.py** # project items definition file\n",
    "        pipelines.py # project pipelines file\n",
    "        settings.py # project settings file\n",
    "        spiders/ # a directory where you'll later put your spiders\n",
    "            __init__.py\n",
    "            **gmarket.py**\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047b256",
   "metadata": {},
   "source": [
    " spiders/gmarket_best.py <br>\n",
    "``def parse`` 부분을 아래와 같이 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e63a314",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class GmarketBestSpider(scrapy.Spider):\n",
    "    name = 'gmarket_best'\n",
    "    allowed_domains = ['corners.gmarket.co.kr']\n",
    "    start_urls = ['http://corners.gmarket.co.kr/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        titles = response.css('div.best-list li > a::text').getall()\n",
    "        for title in titles:\n",
    "            print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439ccb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " !scrapy crawl gmarket_best 을 통해 크롤링 수행할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20164705",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### items.py\n",
    "\n",
    "> 크롤링 데이터 다루기꞉ 저장하기\n",
    "items.py 파일 확인해보자 > items.py # project items definition file\n",
    "\n",
    "어떤 아이템들을 가져올건지 items.py에 선언을 해줘야 한다\n",
    "\n",
    "선언이 된 아이템들을 gmarket_best.py에서 가져와 전달을 해준다.\n",
    "위의 과정을 거쳐야 scrapy에서 저장을 하는 등의 작업을 수행할 수 있게 된다\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acaf339",
   "metadata": {},
   "source": [
    "- items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643727b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class Ecommerce1Item(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    # 저장할 데이터 이름 = scrapy.Field() 과 같이 작성\n",
    "    title = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c66d95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "items.py에 전달하기 위해서는 ``gmarket_best.py``를 아래와 같이 수정해야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753abe09",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### gmarket_best.py 수정\n",
    "\n",
    "```python\n",
    "\n",
    "import scrapy\n",
    "\n",
    "### items.py 의 클래스명인 Ecommerce1Item 을 import 하기\n",
    "\n",
    "from ecommerce1.items import Ecommerce1Item\n",
    "\n",
    "class GmarketSpider(scrapy.Spider):\n",
    "    name = 'gmarket_best'\n",
    "    allowed_domains = ['corners.gmarket.co.kr/Bestsellers']\n",
    "    start_urls = ['http://corners.gmarket.co.kr/Bestsellers/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        titles = response.css('div.best-list li > a::text').getall()\n",
    "\n",
    "        for title in titles:\n",
    "            item = Ecommerce1Item() #선언을 통해 item 객체 생성\n",
    "            # items.py 에서 정의한 scrapy.Field() 명을 동일하게 써줘야 함\n",
    "            item['title'] = title # parsing해서 가져온 데이터를 field 열에 계속 넣어준다.\n",
    "            yield item # yield 하는 순간 데이터가 items.py로 쌓인다\n",
    "\n",
    "```\n",
    "- items.py 필드명 선언 ->\n",
    "- crawling python file에서 items.py import를 통해 불러와 item 객체 생성 ->\n",
    "- 미리 생성해놓은 필드에 parsing해온 데이터를 yield를 통해 적재하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3cc656",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 다양한 데이터 format으로 아이템들을 저장할 수 있다.\n",
    "\n",
    "- csv, xml, json 포멧\n",
    "- 터미널 환경에서, 크롤링을 실행 시켰던 ecommerce1/ecommerce1 폴더에서 다음 명령을 수행하자\n",
    "\n",
    "```\n",
    "// scrapy crawl 크롤러명 -o 저장할 파일명 -t 저장포멧\n",
    "\n",
    "// 예\n",
    "scrapy crawl gmarket_best -o gmarket_best.csv -t csv\n",
    "scrapy crawl gmarket_best -o gmarket_best.xml -t xml\n",
    "\n",
    "scrapy crawl gmarket_best -o gmarket_best.json -t json\n",
    "> json 파일을 확인하면, 한글문자가 깨져나온다 > settings.py를 수정해줘야 한다\n",
    "```\n",
    "\n",
    "**settings.py**에 들어가서 아래의 코드를 추가해주어야 한다\n",
    "\n",
    "해당 파일 안에 utf-8 encoding 설정을 추가해준다 (위치는 상관 없다)\n",
    "\n",
    "```shell\n",
    "# FEED_EXPORT_ENCODING 추가\n",
    "FEED_EXPORT_ENCODING = 'utf-8'\n",
    "```\n",
    "\n",
    "```shell\n",
    "\n",
    "! scrapy crawl gmarket_best -o gmarket_best.csv -t csv\n",
    "\n",
    "// 아래와 같이 gmarket_best.csv 파일이 생성된 것을 확인 할 수 있다.\n",
    "! ls\n",
    "__init__.py      **gmarket_best.csv** middlewares.py   settings.py\n",
    "__pycache__      items.py         pipelines.py     spiders\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}